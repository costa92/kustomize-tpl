
==> ÂÆ°ËÆ°Êó•Âøó <==
|---------|------------------------------------------------------|----------|-----------|---------|---------------------|---------------------|
| Command |                         Args                         | Profile  |   User    | Version |     Start Time      |      End Time       |
|---------|------------------------------------------------------|----------|-----------|---------|---------------------|---------------------|
| pause   |                                                      | minikube | hellotalk | v1.33.1 | 04 Jul 24 19:25 CST |                     |
| start   | --driver docker                                      | minikube | hellotalk | v1.33.1 | 04 Jul 24 19:26 CST |                     |
| start   | --driver docker                                      | minikube | hellotalk | v1.33.1 | 05 Jul 24 09:28 CST |                     |
| delete  |                                                      | minikube | hellotalk | v1.33.1 | 05 Jul 24 09:30 CST | 05 Jul 24 09:31 CST |
| start   | --driver docker                                      | minikube | hellotalk | v1.33.1 | 05 Jul 24 09:31 CST |                     |
| start   | --driver docker                                      | minikube | hellotalk | v1.33.1 | 05 Jul 24 09:53 CST |                     |
| ssh     |                                                      | minikube | hellotalk | v1.33.1 | 05 Jul 24 09:54 CST | 05 Jul 24 09:54 CST |
| start   | --driver docker                                      | minikube | hellotalk | v1.33.1 | 05 Jul 24 09:55 CST |                     |
| delete  |                                                      | minikube | hellotalk | v1.33.1 | 05 Jul 24 09:56 CST | 05 Jul 24 09:56 CST |
| start   | --image-mirror-country=cn --driver docker            | minikube | hellotalk | v1.33.1 | 05 Jul 24 09:56 CST |                     |
|         | --image-repository=registry.cn-hangzhou.aliyuncs.com |          |           |         |                     |                     |
| delete  |                                                      | minikube | hellotalk | v1.33.1 | 05 Jul 24 10:22 CST | 05 Jul 24 10:22 CST |
| start   | --image-mirror-country=cn                            | minikube | hellotalk | v1.33.1 | 05 Jul 24 10:23 CST | 05 Jul 24 10:23 CST |
|         | --driver docker                                      |          |           |         |                     |                     |
| ip      |                                                      | minikube | hellotalk | v1.33.1 | 05 Jul 24 10:48 CST | 05 Jul 24 10:48 CST |
| ssh     |                                                      | minikube | hellotalk | v1.33.1 | 05 Jul 24 10:52 CST |                     |
| ssh     |                                                      | minikube | hellotalk | v1.33.1 | 05 Jul 24 11:01 CST |                     |
| ssh     |                                                      | minikube | hellotalk | v1.33.1 | 05 Jul 24 14:16 CST |                     |
| start   | --image-mirror-country=cn                            | minikube | hellotalk | v1.33.1 | 05 Jul 24 14:16 CST | 05 Jul 24 14:16 CST |
|         | --driver docker                                      |          |           |         |                     |                     |
| start   | --image-mirror-country=cn                            | minikube | hellotalk | v1.33.1 | 05 Jul 24 15:05 CST | 05 Jul 24 15:05 CST |
|         | --driver docker                                      |          |           |         |                     |                     |
| ssh     |                                                      | minikube | hellotalk | v1.33.1 | 05 Jul 24 15:56 CST |                     |
| ssh     |                                                      | minikube | hellotalk | v1.33.1 | 05 Jul 24 17:08 CST |                     |
| ip      |                                                      | minikube | hellotalk | v1.33.1 | 08 Jul 24 11:33 CST | 08 Jul 24 11:33 CST |
| ssh     |                                                      | minikube | hellotalk | v1.33.1 | 08 Jul 24 11:55 CST |                     |
| ssh     |                                                      | minikube | hellotalk | v1.33.1 | 08 Jul 24 11:59 CST | 08 Jul 24 14:08 CST |
| service | --all                                                | minikube | hellotalk | v1.33.1 | 08 Jul 24 12:04 CST | 08 Jul 24 12:04 CST |
| service | -n prometheus                                        | minikube | hellotalk | v1.33.1 | 08 Jul 24 12:05 CST |                     |
| service | --all prometheus                                     | minikube | hellotalk | v1.33.1 | 08 Jul 24 12:05 CST |                     |
| service | prometheus --all                                     | minikube | hellotalk | v1.33.1 | 08 Jul 24 12:05 CST |                     |
| service | --all                                                | minikube | hellotalk | v1.33.1 | 08 Jul 24 12:05 CST | 08 Jul 24 12:06 CST |
| service | prometheus                                           | minikube | hellotalk | v1.33.1 | 08 Jul 24 12:21 CST |                     |
| service | prometheus -n prometheus                             | minikube | hellotalk | v1.33.1 | 08 Jul 24 12:21 CST |                     |
| service | list                                                 | minikube | hellotalk | v1.33.1 | 08 Jul 24 12:21 CST | 08 Jul 24 12:22 CST |
| ssh     |                                                      | minikube | hellotalk | v1.33.1 | 08 Jul 24 12:34 CST | 08 Jul 24 14:08 CST |
| service | list                                                 | minikube | hellotalk | v1.33.1 | 08 Jul 24 14:34 CST | 08 Jul 24 14:34 CST |
|---------|------------------------------------------------------|----------|-----------|---------|---------------------|---------------------|


==> ‰∏äÊ¨°ÂêØÂä® <==
Log file created at: 2024/07/05 15:05:46
Running on machine: hellotalk
Binary: Built with gc go1.22.1 for linux/amd64
Log line format: [IWEF]mmdd hh:mm:ss.uuuuuu threadid file:line] msg
I0705 15:05:46.912106   39530 out.go:291] Setting OutFile to fd 1 ...
I0705 15:05:46.912244   39530 out.go:343] isatty.IsTerminal(1) = true
I0705 15:05:46.912247   39530 out.go:304] Setting ErrFile to fd 2...
I0705 15:05:46.912250   39530 out.go:343] isatty.IsTerminal(2) = true
I0705 15:05:46.912422   39530 root.go:338] Updating PATH: /home/hellotalk/.minikube/bin
I0705 15:05:46.912721   39530 out.go:298] Setting JSON to false
I0705 15:05:46.913855   39530 start.go:129] hostinfo: {"hostname":"hellotalk","uptime":3107,"bootTime":1720160040,"procs":401,"os":"linux","platform":"Kylin","platformFamily":"debian","platformVersion":"V10","kernelVersion":"6.8.0-36-generic","kernelArch":"x86_64","virtualizationSystem":"kvm","virtualizationRole":"host","hostId":"fe5c207a-b8d3-4039-ad19-89ea856d1eaf"}
I0705 15:05:46.913891   39530 start.go:139] virtualization: kvm host
I0705 15:05:46.919518   39530 out.go:177] üòÑ  Kylin V10 ‰∏äÁöÑ minikube v1.33.1
I0705 15:05:46.930141   39530 notify.go:220] Checking for updates...
I0705 15:05:46.930546   39530 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.30.0
I0705 15:05:46.930982   39530 driver.go:392] Setting default libvirt URI to qemu:///system
I0705 15:05:46.943870   39530 docker.go:122] docker version: linux-27.0.3:Docker Engine - Community
I0705 15:05:46.943950   39530 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I0705 15:05:46.971969   39530 info.go:266] docker info: {ID:52e42d06-3d35-4ae0-a5f4-ec96fa6ab5ad Containers:2 ContainersRunning:0 ContainersPaused:0 ContainersStopped:2 Images:1 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Using metacopy false] [Native Overlay Diff true] [userxattr false]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:false CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:true BridgeNfIP6Tables:true Debug:false NFd:23 OomKillDisable:false NGoroutines:39 SystemTime:2024-07-05 15:05:46.966287306 +0800 CST LoggingDriver:json-file CgroupDriver:systemd NEventsListener:0 KernelVersion:6.8.0-36-generic OperatingSystem:Ubuntu 24.04 LTS OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:12 MemTotal:33495568384 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy: HTTPSProxy: NoProxy: Name:hellotalk Labels:[] ExperimentalBuild:false ServerVersion:27.0.3 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:ae71819c4f5e67bb4d5ae76a6b735f29cc25774e Expected:ae71819c4f5e67bb4d5ae76a6b735f29cc25774e} RuncCommit:{ID:v1.1.13-0-g58aa920 Expected:v1.1.13-0-g58aa920} InitCommit:{ID:de40ad0 Expected:de40ad0} SecurityOptions:[name=apparmor name=seccomp,profile=builtin name=cgroupns] ProductLicense: Warnings:<nil> ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:buildx Path:/usr/libexec/docker/cli-plugins/docker-buildx SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.15.1] map[Name:compose Path:/usr/libexec/docker/cli-plugins/docker-compose SchemaVersion:0.1.0 ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.28.1]] Warnings:<nil>}}
I0705 15:05:46.972031   39530 docker.go:295] overlay module found
I0705 15:05:46.977410   39530 out.go:177] ‚ú®  Ê†πÊçÆÁé∞ÊúâÁöÑÈÖçÁΩÆÊñá‰ª∂‰ΩøÁî® docker È©±Âä®Á®ãÂ∫è
I0705 15:05:46.982750   39530 start.go:297] selected driver: docker
I0705 15:05:46.982754   39530 start.go:901] validating driver "docker" against &{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:docker.io/kicbase/stable:v0.0.44@sha256:eb04641328b06c5c4a14f4348470e1046bbcf9c2cbc551486e343d3a49db557e Memory:7900 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.30.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository:registry.cn-hangzhou.aliyuncs.com/google_containers LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.58.2 Port:8443 KubernetesVersion:v1.30.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/home/hellotalk:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I0705 15:05:46.982803   39530 start.go:912] status for docker: {Installed:true Healthy:true Running:false NeedsImprovement:false Error:<nil> Reason: Fix: Doc: Version:}
I0705 15:05:46.982875   39530 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I0705 15:05:47.011983   39530 info.go:266] docker info: {ID:52e42d06-3d35-4ae0-a5f4-ec96fa6ab5ad Containers:2 ContainersRunning:0 ContainersPaused:0 ContainersStopped:2 Images:1 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Using metacopy false] [Native Overlay Diff true] [userxattr false]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:false CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:true BridgeNfIP6Tables:true Debug:false NFd:23 OomKillDisable:false NGoroutines:39 SystemTime:2024-07-05 15:05:47.00658252 +0800 CST LoggingDriver:json-file CgroupDriver:systemd NEventsListener:0 KernelVersion:6.8.0-36-generic OperatingSystem:Ubuntu 24.04 LTS OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:12 MemTotal:33495568384 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy: HTTPSProxy: NoProxy: Name:hellotalk Labels:[] ExperimentalBuild:false ServerVersion:27.0.3 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:ae71819c4f5e67bb4d5ae76a6b735f29cc25774e Expected:ae71819c4f5e67bb4d5ae76a6b735f29cc25774e} RuncCommit:{ID:v1.1.13-0-g58aa920 Expected:v1.1.13-0-g58aa920} InitCommit:{ID:de40ad0 Expected:de40ad0} SecurityOptions:[name=apparmor name=seccomp,profile=builtin name=cgroupns] ProductLicense: Warnings:<nil> ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:buildx Path:/usr/libexec/docker/cli-plugins/docker-buildx SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.15.1] map[Name:compose Path:/usr/libexec/docker/cli-plugins/docker-compose SchemaVersion:0.1.0 ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.28.1]] Warnings:<nil>}}
I0705 15:05:47.012686   39530 cni.go:84] Creating CNI manager for ""
I0705 15:05:47.012694   39530 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I0705 15:05:47.012725   39530 start.go:340] cluster config:
{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:docker.io/kicbase/stable:v0.0.44@sha256:eb04641328b06c5c4a14f4348470e1046bbcf9c2cbc551486e343d3a49db557e Memory:7900 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.30.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository:registry.cn-hangzhou.aliyuncs.com/google_containers LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.58.2 Port:8443 KubernetesVersion:v1.30.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/home/hellotalk:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I0705 15:05:47.018234   39530 out.go:177] üëç  Starting "minikube" primary control-plane node in "minikube" cluster
I0705 15:05:47.023636   39530 cache.go:121] Beginning downloading kic base image for docker with docker
I0705 15:05:47.029044   39530 out.go:177] üöú  Pulling base image v0.0.44 ...
I0705 15:05:47.034450   39530 image.go:79] Checking for docker.io/kicbase/stable:v0.0.44@sha256:eb04641328b06c5c4a14f4348470e1046bbcf9c2cbc551486e343d3a49db557e in local docker daemon
I0705 15:05:47.034495   39530 profile.go:143] Saving config to /home/hellotalk/.minikube/profiles/minikube/config.json ...
I0705 15:05:47.034650   39530 cache.go:107] acquiring lock: {Name:mk3486a61b8833261f53dde7c7c967e1a0ea0353 Clock:{} Delay:500ms Timeout:10m0s Cancel:<nil>}
I0705 15:05:47.034654   39530 cache.go:107] acquiring lock: {Name:mke9c22e408c718e7d0813b8d7319e67d194f1c8 Clock:{} Delay:500ms Timeout:10m0s Cancel:<nil>}
I0705 15:05:47.034670   39530 cache.go:107] acquiring lock: {Name:mk8e7b3ba072d98ed8517e92768f2c23314593e1 Clock:{} Delay:500ms Timeout:10m0s Cancel:<nil>}
I0705 15:05:47.034699   39530 cache.go:115] /home/hellotalk/.minikube/cache/images/amd64/registry.cn-hangzhou.aliyuncs.com/google_containers/kube-proxy_v1.30.0 exists
I0705 15:05:47.034694   39530 cache.go:107] acquiring lock: {Name:mk20615de9615d94c8b64589db299690aef1ff1a Clock:{} Delay:500ms Timeout:10m0s Cancel:<nil>}
I0705 15:05:47.034700   39530 cache.go:115] /home/hellotalk/.minikube/cache/images/amd64/registry.cn-hangzhou.aliyuncs.com/google_containers/storage-provisioner_v5 exists
I0705 15:05:47.034706   39530 cache.go:96] cache image "registry.cn-hangzhou.aliyuncs.com/google_containers/kube-proxy:v1.30.0" -> "/home/hellotalk/.minikube/cache/images/amd64/registry.cn-hangzhou.aliyuncs.com/google_containers/kube-proxy_v1.30.0" took 56.849¬µs
I0705 15:05:47.034707   39530 cache.go:96] cache image "registry.cn-hangzhou.aliyuncs.com/google_containers/storage-provisioner:v5" -> "/home/hellotalk/.minikube/cache/images/amd64/registry.cn-hangzhou.aliyuncs.com/google_containers/storage-provisioner_v5" took 63.94¬µs
I0705 15:05:47.034688   39530 cache.go:107] acquiring lock: {Name:mke45c653f7509c7bf1727b48f8eed09fe2e8a04 Clock:{} Delay:500ms Timeout:10m0s Cancel:<nil>}
I0705 15:05:47.034711   39530 cache.go:80] save to tar file registry.cn-hangzhou.aliyuncs.com/google_containers/kube-proxy:v1.30.0 -> /home/hellotalk/.minikube/cache/images/amd64/registry.cn-hangzhou.aliyuncs.com/google_containers/kube-proxy_v1.30.0 succeeded
I0705 15:05:47.034712   39530 cache.go:80] save to tar file registry.cn-hangzhou.aliyuncs.com/google_containers/storage-provisioner:v5 -> /home/hellotalk/.minikube/cache/images/amd64/registry.cn-hangzhou.aliyuncs.com/google_containers/storage-provisioner_v5 succeeded
I0705 15:05:47.034722   39530 cache.go:115] /home/hellotalk/.minikube/cache/images/amd64/registry.cn-hangzhou.aliyuncs.com/google_containers/kube-apiserver_v1.30.0 exists
I0705 15:05:47.034727   39530 cache.go:96] cache image "registry.cn-hangzhou.aliyuncs.com/google_containers/kube-apiserver:v1.30.0" -> "/home/hellotalk/.minikube/cache/images/amd64/registry.cn-hangzhou.aliyuncs.com/google_containers/kube-apiserver_v1.30.0" took 34.206¬µs
I0705 15:05:47.034734   39530 cache.go:80] save to tar file registry.cn-hangzhou.aliyuncs.com/google_containers/kube-apiserver:v1.30.0 -> /home/hellotalk/.minikube/cache/images/amd64/registry.cn-hangzhou.aliyuncs.com/google_containers/kube-apiserver_v1.30.0 succeeded
I0705 15:05:47.034745   39530 cache.go:115] /home/hellotalk/.minikube/cache/images/amd64/registry.cn-hangzhou.aliyuncs.com/google_containers/pause_3.9 exists
I0705 15:05:47.034751   39530 cache.go:96] cache image "registry.cn-hangzhou.aliyuncs.com/google_containers/pause:3.9" -> "/home/hellotalk/.minikube/cache/images/amd64/registry.cn-hangzhou.aliyuncs.com/google_containers/pause_3.9" took 70.247¬µs
I0705 15:05:47.034758   39530 cache.go:80] save to tar file registry.cn-hangzhou.aliyuncs.com/google_containers/pause:3.9 -> /home/hellotalk/.minikube/cache/images/amd64/registry.cn-hangzhou.aliyuncs.com/google_containers/pause_3.9 succeeded
I0705 15:05:47.034751   39530 cache.go:115] /home/hellotalk/.minikube/cache/images/amd64/registry.cn-hangzhou.aliyuncs.com/google_containers/etcd_3.5.12-0 exists
I0705 15:05:47.034747   39530 cache.go:107] acquiring lock: {Name:mk6446cd5bdbfe589aabaa4f5e11f42337ed2e06 Clock:{} Delay:500ms Timeout:10m0s Cancel:<nil>}
I0705 15:05:47.034766   39530 cache.go:96] cache image "registry.cn-hangzhou.aliyuncs.com/google_containers/etcd:3.5.12-0" -> "/home/hellotalk/.minikube/cache/images/amd64/registry.cn-hangzhou.aliyuncs.com/google_containers/etcd_3.5.12-0" took 96.673¬µs
I0705 15:05:47.034781   39530 cache.go:80] save to tar file registry.cn-hangzhou.aliyuncs.com/google_containers/etcd:3.5.12-0 -> /home/hellotalk/.minikube/cache/images/amd64/registry.cn-hangzhou.aliyuncs.com/google_containers/etcd_3.5.12-0 succeeded
I0705 15:05:47.034769   39530 cache.go:107] acquiring lock: {Name:mk84c8ae1d1eff4d92853edf7d7808f2f572c555 Clock:{} Delay:500ms Timeout:10m0s Cancel:<nil>}
I0705 15:05:47.034764   39530 cache.go:107] acquiring lock: {Name:mkd981ff70f0bffcc2fbc7c2d5070efa2c5c631a Clock:{} Delay:500ms Timeout:10m0s Cancel:<nil>}
I0705 15:05:47.034838   39530 cache.go:115] /home/hellotalk/.minikube/cache/images/amd64/registry.cn-hangzhou.aliyuncs.com/google_containers/coredns_v1.11.1 exists
I0705 15:05:47.034844   39530 cache.go:115] /home/hellotalk/.minikube/cache/images/amd64/registry.cn-hangzhou.aliyuncs.com/google_containers/kube-controller-manager_v1.30.0 exists
I0705 15:05:47.034850   39530 cache.go:96] cache image "registry.cn-hangzhou.aliyuncs.com/google_containers/kube-controller-manager:v1.30.0" -> "/home/hellotalk/.minikube/cache/images/amd64/registry.cn-hangzhou.aliyuncs.com/google_containers/kube-controller-manager_v1.30.0" took 114.897¬µs
I0705 15:05:47.034848   39530 cache.go:96] cache image "registry.cn-hangzhou.aliyuncs.com/google_containers/coredns:v1.11.1" -> "/home/hellotalk/.minikube/cache/images/amd64/registry.cn-hangzhou.aliyuncs.com/google_containers/coredns_v1.11.1" took 129.542¬µs
I0705 15:05:47.034854   39530 cache.go:80] save to tar file registry.cn-hangzhou.aliyuncs.com/google_containers/kube-controller-manager:v1.30.0 -> /home/hellotalk/.minikube/cache/images/amd64/registry.cn-hangzhou.aliyuncs.com/google_containers/kube-controller-manager_v1.30.0 succeeded
I0705 15:05:47.034858   39530 cache.go:80] save to tar file registry.cn-hangzhou.aliyuncs.com/google_containers/coredns:v1.11.1 -> /home/hellotalk/.minikube/cache/images/amd64/registry.cn-hangzhou.aliyuncs.com/google_containers/coredns_v1.11.1 succeeded
I0705 15:05:47.034857   39530 cache.go:115] /home/hellotalk/.minikube/cache/images/amd64/registry.cn-hangzhou.aliyuncs.com/google_containers/kube-scheduler_v1.30.0 exists
I0705 15:05:47.034869   39530 cache.go:96] cache image "registry.cn-hangzhou.aliyuncs.com/google_containers/kube-scheduler:v1.30.0" -> "/home/hellotalk/.minikube/cache/images/amd64/registry.cn-hangzhou.aliyuncs.com/google_containers/kube-scheduler_v1.30.0" took 126.516¬µs
I0705 15:05:47.034876   39530 cache.go:80] save to tar file registry.cn-hangzhou.aliyuncs.com/google_containers/kube-scheduler:v1.30.0 -> /home/hellotalk/.minikube/cache/images/amd64/registry.cn-hangzhou.aliyuncs.com/google_containers/kube-scheduler_v1.30.0 succeeded
I0705 15:05:47.034882   39530 cache.go:87] Successfully saved all images to host disk.
I0705 15:05:47.043885   39530 image.go:83] Found docker.io/kicbase/stable:v0.0.44@sha256:eb04641328b06c5c4a14f4348470e1046bbcf9c2cbc551486e343d3a49db557e in local docker daemon, skipping pull
I0705 15:05:47.043894   39530 cache.go:144] docker.io/kicbase/stable:v0.0.44@sha256:eb04641328b06c5c4a14f4348470e1046bbcf9c2cbc551486e343d3a49db557e exists in daemon, skipping load
I0705 15:05:47.043908   39530 cache.go:194] Successfully downloaded all kic artifacts
I0705 15:05:47.043927   39530 start.go:360] acquireMachinesLock for minikube: {Name:mk4b960829c03ddc3010fac6bdb17168cd8bb28c Clock:{} Delay:500ms Timeout:10m0s Cancel:<nil>}
I0705 15:05:47.043963   39530 start.go:364] duration metric: took 26.304¬µs to acquireMachinesLock for "minikube"
I0705 15:05:47.043971   39530 start.go:96] Skipping create...Using existing machine configuration
I0705 15:05:47.043974   39530 fix.go:54] fixHost starting:
I0705 15:05:47.044158   39530 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0705 15:05:47.053911   39530 fix.go:112] recreateIfNeeded on minikube: state=Stopped err=<nil>
W0705 15:05:47.053929   39530 fix.go:138] unexpected machine state, will restart: <nil>
I0705 15:05:47.059458   39530 out.go:177] üîÑ  Restarting existing docker container for "minikube" ...
I0705 15:05:47.064918   39530 cli_runner.go:164] Run: docker start minikube
I0705 15:05:47.275357   39530 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0705 15:05:47.285108   39530 kic.go:430] container "minikube" state is running.
I0705 15:05:47.285427   39530 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0705 15:05:47.295681   39530 profile.go:143] Saving config to /home/hellotalk/.minikube/profiles/minikube/config.json ...
I0705 15:05:47.295832   39530 machine.go:94] provisionDockerMachine start ...
I0705 15:05:47.295881   39530 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0705 15:05:47.305704   39530 main.go:141] libmachine: Using SSH client type: native
I0705 15:05:47.305840   39530 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x82d6e0] 0x830440 <nil>  [] 0s} 127.0.0.1 32768 <nil> <nil>}
I0705 15:05:47.305844   39530 main.go:141] libmachine: About to run SSH command:
hostname
I0705 15:05:47.306220   39530 main.go:141] libmachine: Error dialing TCP: ssh: handshake failed: read tcp 127.0.0.1:36024->127.0.0.1:32768: read: connection reset by peer
I0705 15:05:50.428765   39530 main.go:141] libmachine: SSH cmd err, output: <nil>: minikube

I0705 15:05:50.428783   39530 ubuntu.go:169] provisioning hostname "minikube"
I0705 15:05:50.428884   39530 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0705 15:05:50.441079   39530 main.go:141] libmachine: Using SSH client type: native
I0705 15:05:50.441220   39530 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x82d6e0] 0x830440 <nil>  [] 0s} 127.0.0.1 32768 <nil> <nil>}
I0705 15:05:50.441225   39530 main.go:141] libmachine: About to run SSH command:
sudo hostname minikube && echo "minikube" | sudo tee /etc/hostname
I0705 15:05:50.602215   39530 main.go:141] libmachine: SSH cmd err, output: <nil>: minikube

I0705 15:05:50.602273   39530 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0705 15:05:50.612194   39530 main.go:141] libmachine: Using SSH client type: native
I0705 15:05:50.612313   39530 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x82d6e0] 0x830440 <nil>  [] 0s} 127.0.0.1 32768 <nil> <nil>}
I0705 15:05:50.612321   39530 main.go:141] libmachine: About to run SSH command:

		if ! grep -xq '.*\sminikube' /etc/hosts; then
			if grep -xq '127.0.1.1\s.*' /etc/hosts; then
				sudo sed -i 's/^127.0.1.1\s.*/127.0.1.1 minikube/g' /etc/hosts;
			else
				echo '127.0.1.1 minikube' | sudo tee -a /etc/hosts;
			fi
		fi
I0705 15:05:50.731957   39530 main.go:141] libmachine: SSH cmd err, output: <nil>:
I0705 15:05:50.731985   39530 ubuntu.go:175] set auth options {CertDir:/home/hellotalk/.minikube CaCertPath:/home/hellotalk/.minikube/certs/ca.pem CaPrivateKeyPath:/home/hellotalk/.minikube/certs/ca-key.pem CaCertRemotePath:/etc/docker/ca.pem ServerCertPath:/home/hellotalk/.minikube/machines/server.pem ServerKeyPath:/home/hellotalk/.minikube/machines/server-key.pem ClientKeyPath:/home/hellotalk/.minikube/certs/key.pem ServerCertRemotePath:/etc/docker/server.pem ServerKeyRemotePath:/etc/docker/server-key.pem ClientCertPath:/home/hellotalk/.minikube/certs/cert.pem ServerCertSANs:[] StorePath:/home/hellotalk/.minikube}
I0705 15:05:50.732037   39530 ubuntu.go:177] setting up certificates
I0705 15:05:50.732055   39530 provision.go:84] configureAuth start
I0705 15:05:50.732270   39530 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0705 15:05:50.743679   39530 provision.go:143] copyHostCerts
I0705 15:05:50.743708   39530 exec_runner.go:144] found /home/hellotalk/.minikube/ca.pem, removing ...
I0705 15:05:50.743716   39530 exec_runner.go:203] rm: /home/hellotalk/.minikube/ca.pem
I0705 15:05:50.743760   39530 exec_runner.go:151] cp: /home/hellotalk/.minikube/certs/ca.pem --> /home/hellotalk/.minikube/ca.pem (1086 bytes)
I0705 15:05:50.743830   39530 exec_runner.go:144] found /home/hellotalk/.minikube/cert.pem, removing ...
I0705 15:05:50.743833   39530 exec_runner.go:203] rm: /home/hellotalk/.minikube/cert.pem
I0705 15:05:50.743855   39530 exec_runner.go:151] cp: /home/hellotalk/.minikube/certs/cert.pem --> /home/hellotalk/.minikube/cert.pem (1131 bytes)
I0705 15:05:50.743899   39530 exec_runner.go:144] found /home/hellotalk/.minikube/key.pem, removing ...
I0705 15:05:50.743901   39530 exec_runner.go:203] rm: /home/hellotalk/.minikube/key.pem
I0705 15:05:50.743921   39530 exec_runner.go:151] cp: /home/hellotalk/.minikube/certs/key.pem --> /home/hellotalk/.minikube/key.pem (1679 bytes)
I0705 15:05:50.743963   39530 provision.go:117] generating server cert: /home/hellotalk/.minikube/machines/server.pem ca-key=/home/hellotalk/.minikube/certs/ca.pem private-key=/home/hellotalk/.minikube/certs/ca-key.pem org=hellotalk.minikube san=[127.0.0.1 192.168.58.2 localhost minikube]
I0705 15:05:50.851161   39530 provision.go:177] copyRemoteCerts
I0705 15:05:50.851202   39530 ssh_runner.go:195] Run: sudo mkdir -p /etc/docker /etc/docker /etc/docker
I0705 15:05:50.851232   39530 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0705 15:05:50.860955   39530 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32768 SSHKeyPath:/home/hellotalk/.minikube/machines/minikube/id_rsa Username:docker}
I0705 15:05:50.948555   39530 ssh_runner.go:362] scp /home/hellotalk/.minikube/certs/ca.pem --> /etc/docker/ca.pem (1086 bytes)
I0705 15:05:50.964798   39530 ssh_runner.go:362] scp /home/hellotalk/.minikube/machines/server.pem --> /etc/docker/server.pem (1188 bytes)
I0705 15:05:50.980690   39530 ssh_runner.go:362] scp /home/hellotalk/.minikube/machines/server-key.pem --> /etc/docker/server-key.pem (1675 bytes)
I0705 15:05:50.996812   39530 provision.go:87] duration metric: took 264.748284ms to configureAuth
I0705 15:05:50.996823   39530 ubuntu.go:193] setting minikube options for container-runtime
I0705 15:05:50.996922   39530 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.30.0
I0705 15:05:50.996962   39530 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0705 15:05:51.006899   39530 main.go:141] libmachine: Using SSH client type: native
I0705 15:05:51.007014   39530 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x82d6e0] 0x830440 <nil>  [] 0s} 127.0.0.1 32768 <nil> <nil>}
I0705 15:05:51.007019   39530 main.go:141] libmachine: About to run SSH command:
df --output=fstype / | tail -n 1
I0705 15:05:51.123460   39530 main.go:141] libmachine: SSH cmd err, output: <nil>: overlay

I0705 15:05:51.123471   39530 ubuntu.go:71] root file system type: overlay
I0705 15:05:51.123534   39530 provision.go:314] Updating docker unit: /lib/systemd/system/docker.service ...
I0705 15:05:51.123584   39530 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0705 15:05:51.133646   39530 main.go:141] libmachine: Using SSH client type: native
I0705 15:05:51.133765   39530 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x82d6e0] 0x830440 <nil>  [] 0s} 127.0.0.1 32768 <nil> <nil>}
I0705 15:05:51.133809   39530 main.go:141] libmachine: About to run SSH command:
sudo mkdir -p /lib/systemd/system && printf %!s(MISSING) "[Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12
ExecReload=/bin/kill -s HUP \$MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target
" | sudo tee /lib/systemd/system/docker.service.new
I0705 15:05:51.294209   39530 main.go:141] libmachine: SSH cmd err, output: <nil>: [Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12
ExecReload=/bin/kill -s HUP $MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target

I0705 15:05:51.294407   39530 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0705 15:05:51.306326   39530 main.go:141] libmachine: Using SSH client type: native
I0705 15:05:51.306443   39530 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x82d6e0] 0x830440 <nil>  [] 0s} 127.0.0.1 32768 <nil> <nil>}
I0705 15:05:51.306452   39530 main.go:141] libmachine: About to run SSH command:
sudo diff -u /lib/systemd/system/docker.service /lib/systemd/system/docker.service.new || { sudo mv /lib/systemd/system/docker.service.new /lib/systemd/system/docker.service; sudo systemctl -f daemon-reload && sudo systemctl -f enable docker && sudo systemctl -f restart docker; }
I0705 15:05:51.426332   39530 main.go:141] libmachine: SSH cmd err, output: <nil>:
I0705 15:05:51.426345   39530 machine.go:97] duration metric: took 4.130507001s to provisionDockerMachine
I0705 15:05:51.426351   39530 start.go:293] postStartSetup for "minikube" (driver="docker")
I0705 15:05:51.426358   39530 start.go:322] creating required directories: [/etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs]
I0705 15:05:51.426414   39530 ssh_runner.go:195] Run: sudo mkdir -p /etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs
I0705 15:05:51.426451   39530 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0705 15:05:51.437163   39530 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32768 SSHKeyPath:/home/hellotalk/.minikube/machines/minikube/id_rsa Username:docker}
I0705 15:05:51.531064   39530 ssh_runner.go:195] Run: cat /etc/os-release
I0705 15:05:51.537442   39530 main.go:141] libmachine: Couldn't set key VERSION_CODENAME, no corresponding struct field found
I0705 15:05:51.537485   39530 main.go:141] libmachine: Couldn't set key PRIVACY_POLICY_URL, no corresponding struct field found
I0705 15:05:51.537501   39530 main.go:141] libmachine: Couldn't set key UBUNTU_CODENAME, no corresponding struct field found
I0705 15:05:51.537510   39530 info.go:137] Remote host: Ubuntu 22.04.4 LTS
I0705 15:05:51.537524   39530 filesync.go:126] Scanning /home/hellotalk/.minikube/addons for local assets ...
I0705 15:05:51.537607   39530 filesync.go:126] Scanning /home/hellotalk/.minikube/files for local assets ...
I0705 15:05:51.537641   39530 start.go:296] duration metric: took 111.283267ms for postStartSetup
I0705 15:05:51.537741   39530 ssh_runner.go:195] Run: sh -c "df -h /var | awk 'NR==2{print $5}'"
I0705 15:05:51.537822   39530 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0705 15:05:51.548585   39530 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32768 SSHKeyPath:/home/hellotalk/.minikube/machines/minikube/id_rsa Username:docker}
I0705 15:05:51.630635   39530 ssh_runner.go:195] Run: sh -c "df -BG /var | awk 'NR==2{print $4}'"
I0705 15:05:51.633211   39530 fix.go:56] duration metric: took 4.589233477s for fixHost
I0705 15:05:51.633219   39530 start.go:83] releasing machines lock for "minikube", held for 4.589251399s
I0705 15:05:51.633263   39530 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0705 15:05:51.643033   39530 ssh_runner.go:195] Run: cat /version.json
I0705 15:05:51.643067   39530 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0705 15:05:51.643089   39530 ssh_runner.go:195] Run: curl -sS -m 2 https://registry.cn-hangzhou.aliyuncs.com/google_containers/
I0705 15:05:51.643131   39530 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0705 15:05:51.653390   39530 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32768 SSHKeyPath:/home/hellotalk/.minikube/machines/minikube/id_rsa Username:docker}
I0705 15:05:51.653466   39530 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32768 SSHKeyPath:/home/hellotalk/.minikube/machines/minikube/id_rsa Username:docker}
I0705 15:05:51.890288   39530 ssh_runner.go:195] Run: systemctl --version
I0705 15:05:51.893379   39530 ssh_runner.go:195] Run: sh -c "stat /etc/cni/net.d/*loopback.conf*"
I0705 15:05:51.896601   39530 ssh_runner.go:195] Run: sudo find /etc/cni/net.d -maxdepth 1 -type f -name *loopback.conf* -not -name *.mk_disabled -exec sh -c "grep -q loopback {} && ( grep -q name {} || sudo sed -i '/"type": "loopback"/i \ \ \ \ "name": "loopback",' {} ) && sudo sed -i 's|"cniVersion": ".*"|"cniVersion": "1.0.0"|g' {}" ;
I0705 15:05:51.909486   39530 cni.go:230] loopback cni configuration patched: "/etc/cni/net.d/*loopback.conf*" found
I0705 15:05:51.909535   39530 ssh_runner.go:195] Run: sudo find /etc/cni/net.d -maxdepth 1 -type f ( ( -name *bridge* -or -name *podman* ) -and -not -name *.mk_disabled ) -printf "%!p(MISSING), " -exec sh -c "sudo mv {} {}.mk_disabled" ;
I0705 15:05:51.915909   39530 cni.go:259] no active bridge cni configs found in "/etc/cni/net.d" - nothing to disable
I0705 15:05:51.921212   39530 start.go:494] detecting cgroup driver to use...
I0705 15:05:51.921236   39530 detect.go:199] detected "systemd" cgroup driver on host os
I0705 15:05:51.921300   39530 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %!s(MISSING) "runtime-endpoint: unix:///run/containerd/containerd.sock
" | sudo tee /etc/crictl.yaml"
I0705 15:05:51.933550   39530 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)sandbox_image = .*$|\1sandbox_image = "registry.cn-hangzhou.aliyuncs.com/google_containers/pause:3.9"|' /etc/containerd/config.toml"
I0705 15:05:51.943524   39530 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)restrict_oom_score_adj = .*$|\1restrict_oom_score_adj = false|' /etc/containerd/config.toml"
I0705 15:05:51.953626   39530 containerd.go:146] configuring containerd to use "systemd" as cgroup driver...
I0705 15:05:51.953686   39530 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)SystemdCgroup = .*$|\1SystemdCgroup = true|g' /etc/containerd/config.toml"
I0705 15:05:51.963967   39530 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runtime.v1.linux"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I0705 15:05:51.972513   39530 ssh_runner.go:195] Run: sh -c "sudo sed -i '/systemd_cgroup/d' /etc/containerd/config.toml"
I0705 15:05:51.981302   39530 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runc.v1"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I0705 15:05:51.988377   39530 ssh_runner.go:195] Run: sh -c "sudo rm -rf /etc/cni/net.mk"
I0705 15:05:51.994489   39530 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)conf_dir = .*$|\1conf_dir = "/etc/cni/net.d"|g' /etc/containerd/config.toml"
I0705 15:05:52.000812   39530 ssh_runner.go:195] Run: sh -c "sudo sed -i '/^ *enable_unprivileged_ports = .*/d' /etc/containerd/config.toml"
I0705 15:05:52.006985   39530 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)\[plugins."io.containerd.grpc.v1.cri"\]|&\n\1  enable_unprivileged_ports = true|' /etc/containerd/config.toml"
I0705 15:05:52.013485   39530 ssh_runner.go:195] Run: sudo sysctl net.bridge.bridge-nf-call-iptables
I0705 15:05:52.018908   39530 ssh_runner.go:195] Run: sudo sh -c "echo 1 > /proc/sys/net/ipv4/ip_forward"
I0705 15:05:52.024255   39530 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0705 15:05:52.070521   39530 ssh_runner.go:195] Run: sudo systemctl restart containerd
I0705 15:05:52.124422   39530 start.go:494] detecting cgroup driver to use...
I0705 15:05:52.124444   39530 detect.go:199] detected "systemd" cgroup driver on host os
I0705 15:05:52.124494   39530 ssh_runner.go:195] Run: sudo systemctl cat docker.service
I0705 15:05:52.135526   39530 cruntime.go:279] skipping containerd shutdown because we are bound to it
I0705 15:05:52.135585   39530 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service crio
I0705 15:05:52.143752   39530 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %!s(MISSING) "runtime-endpoint: unix:///var/run/cri-dockerd.sock
" | sudo tee /etc/crictl.yaml"
I0705 15:05:52.155847   39530 ssh_runner.go:195] Run: which cri-dockerd
I0705 15:05:52.158144   39530 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/cri-docker.service.d
I0705 15:05:52.164155   39530 ssh_runner.go:362] scp memory --> /etc/systemd/system/cri-docker.service.d/10-cni.conf (225 bytes)
I0705 15:05:52.177441   39530 ssh_runner.go:195] Run: sudo systemctl unmask docker.service
I0705 15:05:52.225990   39530 ssh_runner.go:195] Run: sudo systemctl enable docker.socket
I0705 15:05:52.273173   39530 docker.go:574] configuring docker to use "systemd" as cgroup driver...
I0705 15:05:52.273239   39530 ssh_runner.go:362] scp memory --> /etc/docker/daemon.json (129 bytes)
I0705 15:05:52.285749   39530 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0705 15:05:52.332354   39530 ssh_runner.go:195] Run: sudo systemctl restart docker
I0705 15:05:53.159250   39530 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.socket
I0705 15:05:53.166868   39530 ssh_runner.go:195] Run: sudo systemctl stop cri-docker.socket
I0705 15:05:53.174507   39530 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.service
I0705 15:05:53.181717   39530 ssh_runner.go:195] Run: sudo systemctl unmask cri-docker.socket
I0705 15:05:53.226925   39530 ssh_runner.go:195] Run: sudo systemctl enable cri-docker.socket
I0705 15:05:53.272606   39530 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0705 15:05:53.316958   39530 ssh_runner.go:195] Run: sudo systemctl restart cri-docker.socket
I0705 15:05:53.332546   39530 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.service
I0705 15:05:53.339981   39530 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0705 15:05:53.384925   39530 ssh_runner.go:195] Run: sudo systemctl restart cri-docker.service
I0705 15:05:53.445055   39530 start.go:541] Will wait 60s for socket path /var/run/cri-dockerd.sock
I0705 15:05:53.445128   39530 ssh_runner.go:195] Run: stat /var/run/cri-dockerd.sock
I0705 15:05:53.447445   39530 start.go:562] Will wait 60s for crictl version
I0705 15:05:53.447489   39530 ssh_runner.go:195] Run: which crictl
I0705 15:05:53.449545   39530 ssh_runner.go:195] Run: sudo /usr/bin/crictl version
I0705 15:05:53.470316   39530 start.go:578] Version:  0.1.0
RuntimeName:  docker
RuntimeVersion:  26.1.1
RuntimeApiVersion:  v1
I0705 15:05:53.470368   39530 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I0705 15:05:53.483500   39530 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I0705 15:05:53.502430   39530 out.go:204] üê≥  Ê≠£Âú® Docker 26.1.1 ‰∏≠ÂáÜÂ§á Kubernetes v1.30.0‚Ä¶
I0705 15:05:53.502504   39530 cli_runner.go:164] Run: docker network inspect minikube --format "{"Name": "{{.Name}}","Driver": "{{.Driver}}","Subnet": "{{range .IPAM.Config}}{{.Subnet}}{{end}}","Gateway": "{{range .IPAM.Config}}{{.Gateway}}{{end}}","MTU": {{if (index .Options "com.docker.network.driver.mtu")}}{{(index .Options "com.docker.network.driver.mtu")}}{{else}}0{{end}}, "ContainerIPs": [{{range $k,$v := .Containers }}"{{$v.IPv4Address}}",{{end}}]}"
I0705 15:05:53.512299   39530 ssh_runner.go:195] Run: grep 192.168.58.1	host.minikube.internal$ /etc/hosts
I0705 15:05:53.514751   39530 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\thost.minikube.internal$' "/etc/hosts"; echo "192.168.58.1	host.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I0705 15:05:53.521793   39530 kubeadm.go:877] updating cluster {Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:docker.io/kicbase/stable:v0.0.44@sha256:eb04641328b06c5c4a14f4348470e1046bbcf9c2cbc551486e343d3a49db557e Memory:7900 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.30.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository:registry.cn-hangzhou.aliyuncs.com/google_containers LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.58.2 Port:8443 KubernetesVersion:v1.30.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/home/hellotalk:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s} ...
I0705 15:05:53.521855   39530 preload.go:132] Checking if preload exists for k8s version v1.30.0 and runtime docker
I0705 15:05:53.521873   39530 preload.go:147] Found local preload: /home/hellotalk/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.30.0-docker-overlay2-amd64.tar.lz4
I0705 15:05:53.521916   39530 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I0705 15:05:53.533456   39530 docker.go:685] Got preloaded images: -- stdout --
registry.cn-hangzhou.aliyuncs.com/google_containers/kube-apiserver:v1.30.0
registry.cn-hangzhou.aliyuncs.com/google_containers/kube-controller-manager:v1.30.0
registry.cn-hangzhou.aliyuncs.com/google_containers/kube-scheduler:v1.30.0
registry.cn-hangzhou.aliyuncs.com/google_containers/kube-proxy:v1.30.0
registry.cn-hangzhou.aliyuncs.com/google_containers/etcd:3.5.12-0
costa92/traefik-plugin:2.9.10
costa92/my-blog:local-20231111194616
registry.cn-hangzhou.aliyuncs.com/google_containers/coredns:v1.11.1
istio/proxyv2:1.15.6
istio/pilot:1.15.6
registry.cn-hangzhou.aliyuncs.com/google_containers/pause:3.9
registry.cn-hangzhou.aliyuncs.com/google_containers/storage-provisioner:v5

-- /stdout --
I0705 15:05:53.533465   39530 docker.go:615] Images already preloaded, skipping extraction
I0705 15:05:53.533510   39530 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I0705 15:05:53.545139   39530 docker.go:685] Got preloaded images: -- stdout --
registry.cn-hangzhou.aliyuncs.com/google_containers/kube-apiserver:v1.30.0
registry.cn-hangzhou.aliyuncs.com/google_containers/kube-scheduler:v1.30.0
registry.cn-hangzhou.aliyuncs.com/google_containers/kube-controller-manager:v1.30.0
registry.cn-hangzhou.aliyuncs.com/google_containers/kube-proxy:v1.30.0
registry.cn-hangzhou.aliyuncs.com/google_containers/etcd:3.5.12-0
costa92/traefik-plugin:2.9.10
costa92/my-blog:local-20231111194616
registry.cn-hangzhou.aliyuncs.com/google_containers/coredns:v1.11.1
istio/proxyv2:1.15.6
istio/pilot:1.15.6
registry.cn-hangzhou.aliyuncs.com/google_containers/pause:3.9
registry.cn-hangzhou.aliyuncs.com/google_containers/storage-provisioner:v5

-- /stdout --
I0705 15:05:53.545147   39530 cache_images.go:84] Images are preloaded, skipping loading
I0705 15:05:53.545153   39530 kubeadm.go:928] updating node { 192.168.58.2 8443 v1.30.0 docker true true} ...
I0705 15:05:53.545221   39530 kubeadm.go:940] kubelet [Unit]
Wants=docker.socket

[Service]
ExecStart=
ExecStart=/var/lib/minikube/binaries/v1.30.0/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --config=/var/lib/kubelet/config.yaml --hostname-override=minikube --kubeconfig=/etc/kubernetes/kubelet.conf --node-ip=192.168.58.2

[Install]
 config:
{KubernetesVersion:v1.30.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository:registry.cn-hangzhou.aliyuncs.com/google_containers LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:}
I0705 15:05:53.545296   39530 ssh_runner.go:195] Run: docker info --format {{.CgroupDriver}}
I0705 15:05:53.573046   39530 cni.go:84] Creating CNI manager for ""
I0705 15:05:53.573057   39530 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I0705 15:05:53.573062   39530 kubeadm.go:84] Using pod CIDR: 10.244.0.0/16
I0705 15:05:53.573076   39530 kubeadm.go:181] kubeadm options: {CertDir:/var/lib/minikube/certs ServiceCIDR:10.96.0.0/12 PodSubnet:10.244.0.0/16 AdvertiseAddress:192.168.58.2 APIServerPort:8443 KubernetesVersion:v1.30.0 EtcdDataDir:/var/lib/minikube/etcd EtcdExtraArgs:map[] ClusterName:minikube NodeName:minikube DNSDomain:cluster.local CRISocket:/var/run/cri-dockerd.sock ImageRepository:registry.cn-hangzhou.aliyuncs.com/google_containers ComponentOptions:[{Component:apiServer ExtraArgs:map[enable-admission-plugins:NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota] Pairs:map[certSANs:["127.0.0.1", "localhost", "192.168.58.2"]]} {Component:controllerManager ExtraArgs:map[allocate-node-cidrs:true leader-elect:false] Pairs:map[]} {Component:scheduler ExtraArgs:map[leader-elect:false] Pairs:map[]}] FeatureArgs:map[] NodeIP:192.168.58.2 CgroupDriver:systemd ClientCAFile:/var/lib/minikube/certs/ca.crt StaticPodPath:/etc/kubernetes/manifests ControlPlaneAddress:control-plane.minikube.internal KubeProxyOptions:map[] ResolvConfSearchRegression:false KubeletConfigOpts:map[containerRuntimeEndpoint:unix:///var/run/cri-dockerd.sock hairpinMode:hairpin-veth runtimeRequestTimeout:15m] PrependCriSocketUnix:true}
I0705 15:05:53.573182   39530 kubeadm.go:187] kubeadm config:
apiVersion: kubeadm.k8s.io/v1beta3
kind: InitConfiguration
localAPIEndpoint:
  advertiseAddress: 192.168.58.2
  bindPort: 8443
bootstrapTokens:
  - groups:
      - system:bootstrappers:kubeadm:default-node-token
    ttl: 24h0m0s
    usages:
      - signing
      - authentication
nodeRegistration:
  criSocket: unix:///var/run/cri-dockerd.sock
  name: "minikube"
  kubeletExtraArgs:
    node-ip: 192.168.58.2
  taints: []
---
apiVersion: kubeadm.k8s.io/v1beta3
kind: ClusterConfiguration
imageRepository: registry.cn-hangzhou.aliyuncs.com/google_containers
apiServer:
  certSANs: ["127.0.0.1", "localhost", "192.168.58.2"]
  extraArgs:
    enable-admission-plugins: "NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota"
controllerManager:
  extraArgs:
    allocate-node-cidrs: "true"
    leader-elect: "false"
scheduler:
  extraArgs:
    leader-elect: "false"
certificatesDir: /var/lib/minikube/certs
clusterName: mk
controlPlaneEndpoint: control-plane.minikube.internal:8443
etcd:
  local:
    dataDir: /var/lib/minikube/etcd
    extraArgs:
      proxy-refresh-interval: "70000"
kubernetesVersion: v1.30.0
networking:
  dnsDomain: cluster.local
  podSubnet: "10.244.0.0/16"
  serviceSubnet: 10.96.0.0/12
---
apiVersion: kubelet.config.k8s.io/v1beta1
kind: KubeletConfiguration
authentication:
  x509:
    clientCAFile: /var/lib/minikube/certs/ca.crt
cgroupDriver: systemd
containerRuntimeEndpoint: unix:///var/run/cri-dockerd.sock
hairpinMode: hairpin-veth
runtimeRequestTimeout: 15m
clusterDomain: "cluster.local"
# disable disk resource management by default
imageGCHighThresholdPercent: 100
evictionHard:
  nodefs.available: "0%!"(MISSING)
  nodefs.inodesFree: "0%!"(MISSING)
  imagefs.available: "0%!"(MISSING)
failSwapOn: false
staticPodPath: /etc/kubernetes/manifests
---
apiVersion: kubeproxy.config.k8s.io/v1alpha1
kind: KubeProxyConfiguration
clusterCIDR: "10.244.0.0/16"
metricsBindAddress: 0.0.0.0:10249
conntrack:
  maxPerCore: 0
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_established"
  tcpEstablishedTimeout: 0s
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_close"
  tcpCloseWaitTimeout: 0s

I0705 15:05:53.573228   39530 ssh_runner.go:195] Run: sudo ls /var/lib/minikube/binaries/v1.30.0
I0705 15:05:53.579256   39530 binaries.go:44] Found k8s binaries, skipping transfer
I0705 15:05:53.579302   39530 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/kubelet.service.d /lib/systemd/system /var/tmp/minikube
I0705 15:05:53.584849   39530 ssh_runner.go:362] scp memory --> /etc/systemd/system/kubelet.service.d/10-kubeadm.conf (307 bytes)
I0705 15:05:53.596547   39530 ssh_runner.go:362] scp memory --> /lib/systemd/system/kubelet.service (352 bytes)
I0705 15:05:53.608196   39530 ssh_runner.go:362] scp memory --> /var/tmp/minikube/kubeadm.yaml.new (2218 bytes)
I0705 15:05:53.619875   39530 ssh_runner.go:195] Run: grep 192.168.58.2	control-plane.minikube.internal$ /etc/hosts
I0705 15:05:53.621980   39530 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\tcontrol-plane.minikube.internal$' "/etc/hosts"; echo "192.168.58.2	control-plane.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I0705 15:05:53.628837   39530 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0705 15:05:53.675806   39530 ssh_runner.go:195] Run: sudo systemctl start kubelet
I0705 15:05:53.696622   39530 certs.go:68] Setting up /home/hellotalk/.minikube/profiles/minikube for IP: 192.168.58.2
I0705 15:05:53.696630   39530 certs.go:194] generating shared ca certs ...
I0705 15:05:53.696640   39530 certs.go:226] acquiring lock for ca certs: {Name:mk13fbde79f9491c95dda7e6b6ac4e7c3fe829b3 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0705 15:05:53.696723   39530 certs.go:235] skipping valid "minikubeCA" ca cert: /home/hellotalk/.minikube/ca.key
I0705 15:05:53.696747   39530 certs.go:235] skipping valid "proxyClientCA" ca cert: /home/hellotalk/.minikube/proxy-client-ca.key
I0705 15:05:53.696752   39530 certs.go:256] generating profile certs ...
I0705 15:05:53.696804   39530 certs.go:359] skipping valid signed profile cert regeneration for "minikube-user": /home/hellotalk/.minikube/profiles/minikube/client.key
I0705 15:05:53.696829   39530 certs.go:359] skipping valid signed profile cert regeneration for "minikube": /home/hellotalk/.minikube/profiles/minikube/apiserver.key.502bbb95
I0705 15:05:53.696856   39530 certs.go:359] skipping valid signed profile cert regeneration for "aggregator": /home/hellotalk/.minikube/profiles/minikube/proxy-client.key
I0705 15:05:53.696924   39530 certs.go:484] found cert: /home/hellotalk/.minikube/certs/ca-key.pem (1679 bytes)
I0705 15:05:53.696944   39530 certs.go:484] found cert: /home/hellotalk/.minikube/certs/ca.pem (1086 bytes)
I0705 15:05:53.696959   39530 certs.go:484] found cert: /home/hellotalk/.minikube/certs/cert.pem (1131 bytes)
I0705 15:05:53.696972   39530 certs.go:484] found cert: /home/hellotalk/.minikube/certs/key.pem (1679 bytes)
I0705 15:05:53.697376   39530 ssh_runner.go:362] scp /home/hellotalk/.minikube/ca.crt --> /var/lib/minikube/certs/ca.crt (1111 bytes)
I0705 15:05:53.715478   39530 ssh_runner.go:362] scp /home/hellotalk/.minikube/ca.key --> /var/lib/minikube/certs/ca.key (1675 bytes)
I0705 15:05:53.733161   39530 ssh_runner.go:362] scp /home/hellotalk/.minikube/proxy-client-ca.crt --> /var/lib/minikube/certs/proxy-client-ca.crt (1119 bytes)
I0705 15:05:53.754060   39530 ssh_runner.go:362] scp /home/hellotalk/.minikube/proxy-client-ca.key --> /var/lib/minikube/certs/proxy-client-ca.key (1675 bytes)
I0705 15:05:53.773287   39530 ssh_runner.go:362] scp /home/hellotalk/.minikube/profiles/minikube/apiserver.crt --> /var/lib/minikube/certs/apiserver.crt (1411 bytes)
I0705 15:05:53.793406   39530 ssh_runner.go:362] scp /home/hellotalk/.minikube/profiles/minikube/apiserver.key --> /var/lib/minikube/certs/apiserver.key (1675 bytes)
I0705 15:05:53.811803   39530 ssh_runner.go:362] scp /home/hellotalk/.minikube/profiles/minikube/proxy-client.crt --> /var/lib/minikube/certs/proxy-client.crt (1147 bytes)
I0705 15:05:53.829222   39530 ssh_runner.go:362] scp /home/hellotalk/.minikube/profiles/minikube/proxy-client.key --> /var/lib/minikube/certs/proxy-client.key (1679 bytes)
I0705 15:05:53.845638   39530 ssh_runner.go:362] scp /home/hellotalk/.minikube/ca.crt --> /usr/share/ca-certificates/minikubeCA.pem (1111 bytes)
I0705 15:05:53.863845   39530 ssh_runner.go:362] scp memory --> /var/lib/minikube/kubeconfig (738 bytes)
I0705 15:05:53.876211   39530 ssh_runner.go:195] Run: openssl version
I0705 15:05:53.880044   39530 ssh_runner.go:195] Run: sudo /bin/bash -c "test -s /usr/share/ca-certificates/minikubeCA.pem && ln -fs /usr/share/ca-certificates/minikubeCA.pem /etc/ssl/certs/minikubeCA.pem"
I0705 15:05:53.887654   39530 ssh_runner.go:195] Run: ls -la /usr/share/ca-certificates/minikubeCA.pem
I0705 15:05:53.889869   39530 certs.go:528] hashing: -rw-r--r-- 1 root root 1111 Jul  5 01:28 /usr/share/ca-certificates/minikubeCA.pem
I0705 15:05:53.889898   39530 ssh_runner.go:195] Run: openssl x509 -hash -noout -in /usr/share/ca-certificates/minikubeCA.pem
I0705 15:05:53.894232   39530 ssh_runner.go:195] Run: sudo /bin/bash -c "test -L /etc/ssl/certs/b5213941.0 || ln -fs /etc/ssl/certs/minikubeCA.pem /etc/ssl/certs/b5213941.0"
I0705 15:05:53.900981   39530 ssh_runner.go:195] Run: stat /var/lib/minikube/certs/apiserver-kubelet-client.crt
I0705 15:05:53.903690   39530 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/apiserver-etcd-client.crt -checkend 86400
I0705 15:05:53.908353   39530 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/apiserver-kubelet-client.crt -checkend 86400
I0705 15:05:53.912831   39530 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/server.crt -checkend 86400
I0705 15:05:53.917206   39530 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/healthcheck-client.crt -checkend 86400
I0705 15:05:53.922021   39530 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/peer.crt -checkend 86400
I0705 15:05:53.926564   39530 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/front-proxy-client.crt -checkend 86400
I0705 15:05:53.930740   39530 kubeadm.go:391] StartCluster: {Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:docker.io/kicbase/stable:v0.0.44@sha256:eb04641328b06c5c4a14f4348470e1046bbcf9c2cbc551486e343d3a49db557e Memory:7900 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.30.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository:registry.cn-hangzhou.aliyuncs.com/google_containers LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.58.2 Port:8443 KubernetesVersion:v1.30.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/home/hellotalk:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I0705 15:05:53.930814   39530 ssh_runner.go:195] Run: docker ps --filter status=paused --filter=name=k8s_.*_(kube-system)_ --format={{.ID}}
I0705 15:05:53.942378   39530 ssh_runner.go:195] Run: sudo ls /var/lib/kubelet/kubeadm-flags.env /var/lib/kubelet/config.yaml /var/lib/minikube/etcd
W0705 15:05:53.948308   39530 kubeadm.go:404] apiserver tunnel failed: apiserver port not set
I0705 15:05:53.948314   39530 kubeadm.go:407] found existing configuration files, will attempt cluster restart
I0705 15:05:53.948322   39530 kubeadm.go:587] restartPrimaryControlPlane start ...
I0705 15:05:53.948355   39530 ssh_runner.go:195] Run: sudo test -d /data/minikube
I0705 15:05:53.954722   39530 kubeadm.go:129] /data/minikube skipping compat symlinks: sudo test -d /data/minikube: Process exited with status 1
stdout:

stderr:
I0705 15:05:53.955443   39530 kubeconfig.go:125] found "minikube" server: "https://192.168.58.2:8443"
I0705 15:05:53.957231   39530 ssh_runner.go:195] Run: sudo diff -u /var/tmp/minikube/kubeadm.yaml /var/tmp/minikube/kubeadm.yaml.new
I0705 15:05:53.963604   39530 kubeadm.go:624] The running cluster does not require reconfiguration: 192.168.58.2
I0705 15:05:53.963620   39530 kubeadm.go:591] duration metric: took 15.294444ms to restartPrimaryControlPlane
I0705 15:05:53.963625   39530 kubeadm.go:393] duration metric: took 32.889803ms to StartCluster
I0705 15:05:53.963634   39530 settings.go:142] acquiring lock: {Name:mk9f31ba78da5a548d61d66cdfeef138d1422ea5 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0705 15:05:53.963666   39530 settings.go:150] Updating kubeconfig:  /home/hellotalk/.kube/config
I0705 15:05:53.964682   39530 lock.go:35] WriteFile acquiring /home/hellotalk/.kube/config: {Name:mkef2c36c5d89913cd310795ec667b46dc1413d0 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0705 15:05:53.964840   39530 start.go:234] Will wait 6m0s for node &{Name: IP:192.168.58.2 Port:8443 KubernetesVersion:v1.30.0 ContainerRuntime:docker ControlPlane:true Worker:true}
I0705 15:05:53.970338   39530 out.go:177] üîé  Ê≠£Âú®È™åËØÅ Kubernetes ÁªÑ‰ª∂...
I0705 15:05:53.964874   39530 addons.go:502] enable addons start: toEnable=map[ambassador:false auto-pause:false cloud-spanner:false csi-hostpath-driver:false dashboard:false default-storageclass:true efk:false freshpod:false gcp-auth:false gvisor:false headlamp:false helm-tiller:false inaccel:false ingress:false ingress-dns:false inspektor-gadget:false istio:false istio-provisioner:false kong:false kubeflow:false kubevirt:false logviewer:false metallb:false metrics-server:false nvidia-device-plugin:false nvidia-driver-installer:false nvidia-gpu-device-plugin:false olm:false pod-security-policy:false portainer:false registry:false registry-aliases:false registry-creds:false storage-provisioner:true storage-provisioner-gluster:false storage-provisioner-rancher:false volumesnapshots:false yakd:false]
I0705 15:05:53.964979   39530 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.30.0
I0705 15:05:53.981169   39530 addons.go:69] Setting storage-provisioner=true in profile "minikube"
I0705 15:05:53.981172   39530 addons.go:69] Setting default-storageclass=true in profile "minikube"
I0705 15:05:53.981192   39530 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0705 15:05:53.981192   39530 addons.go:234] Setting addon storage-provisioner=true in "minikube"
I0705 15:05:53.981196   39530 addons_storage_classes.go:33] enableOrDisableStorageClasses default-storageclass=true on "minikube"
W0705 15:05:53.981199   39530 addons.go:243] addon storage-provisioner should already be in state true
I0705 15:05:53.981225   39530 host.go:66] Checking if "minikube" exists ...
I0705 15:05:53.981391   39530 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0705 15:05:53.981458   39530 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0705 15:05:53.993436   39530 addons.go:234] Setting addon default-storageclass=true in "minikube"
I0705 15:05:53.997968   39530 out.go:177]     ‚ñ™ Ê≠£Âú®‰ΩøÁî®ÈïúÂÉè registry.cn-hangzhou.aliyuncs.com/google_containers/storage-provisioner:v5
W0705 15:05:53.997973   39530 addons.go:243] addon default-storageclass should already be in state true
I0705 15:05:53.997999   39530 host.go:66] Checking if "minikube" exists ...
I0705 15:05:54.003644   39530 addons.go:426] installing /etc/kubernetes/addons/storage-provisioner.yaml
I0705 15:05:54.003654   39530 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/storage-provisioner.yaml (2708 bytes)
I0705 15:05:54.003716   39530 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0705 15:05:54.004031   39530 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0705 15:05:54.014775   39530 addons.go:426] installing /etc/kubernetes/addons/storageclass.yaml
I0705 15:05:54.014782   39530 ssh_runner.go:362] scp storageclass/storageclass.yaml --> /etc/kubernetes/addons/storageclass.yaml (271 bytes)
I0705 15:05:54.014838   39530 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0705 15:05:54.014851   39530 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32768 SSHKeyPath:/home/hellotalk/.minikube/machines/minikube/id_rsa Username:docker}
I0705 15:05:54.025631   39530 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32768 SSHKeyPath:/home/hellotalk/.minikube/machines/minikube/id_rsa Username:docker}
I0705 15:05:54.057425   39530 ssh_runner.go:195] Run: sudo systemctl start kubelet
I0705 15:05:54.066722   39530 api_server.go:52] waiting for apiserver process to appear ...
I0705 15:05:54.066781   39530 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0705 15:05:54.106530   39530 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml
I0705 15:05:54.116015   39530 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml
W0705 15:05:54.144640   39530 addons.go:452] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0705 15:05:54.144654   39530 retry.go:31] will retry after 301.355159ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
W0705 15:05:54.154247   39530 addons.go:452] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0705 15:05:54.154263   39530 retry.go:31] will retry after 261.844435ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0705 15:05:54.416673   39530 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml
I0705 15:05:54.446499   39530 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml
W0705 15:05:54.458433   39530 addons.go:452] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0705 15:05:54.458446   39530 retry.go:31] will retry after 245.39495ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
W0705 15:05:54.487361   39530 addons.go:452] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0705 15:05:54.487374   39530 retry.go:31] will retry after 450.225233ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0705 15:05:54.567559   39530 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0705 15:05:54.704359   39530 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml
I0705 15:05:54.938544   39530 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml
I0705 15:05:55.067545   39530 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0705 15:05:55.985692   39530 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: (1.281317328s)
I0705 15:05:56.286991   39530 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: (1.348427852s)
I0705 15:05:56.287026   39530 ssh_runner.go:235] Completed: sudo pgrep -xnf kube-apiserver.*minikube.*: (1.219468541s)
I0705 15:05:56.287036   39530 api_server.go:72] duration metric: took 2.32218398s to wait for apiserver process to appear ...
I0705 15:05:56.287040   39530 api_server.go:88] waiting for apiserver healthz status ...
I0705 15:05:56.287054   39530 api_server.go:253] Checking apiserver healthz at https://192.168.58.2:8443/healthz ...
I0705 15:05:56.289534   39530 api_server.go:279] https://192.168.58.2:8443/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I0705 15:05:56.305313   39530 out.go:177] üåü  ÂêØÁî®Êèí‰ª∂Ôºö default-storageclass, storage-provisioner
I0705 15:05:56.310635   39530 addons.go:505] duration metric: took 2.345761936s for enable addons: enabled=[default-storageclass storage-provisioner]
W0705 15:05:56.305329   39530 api_server.go:103] status: https://192.168.58.2:8443/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I0705 15:05:56.787689   39530 api_server.go:253] Checking apiserver healthz at https://192.168.58.2:8443/healthz ...
I0705 15:05:56.790327   39530 api_server.go:279] https://192.168.58.2:8443/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W0705 15:05:56.790338   39530 api_server.go:103] status: https://192.168.58.2:8443/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I0705 15:05:57.288080   39530 api_server.go:253] Checking apiserver healthz at https://192.168.58.2:8443/healthz ...
I0705 15:05:57.290550   39530 api_server.go:279] https://192.168.58.2:8443/healthz returned 200:
ok
I0705 15:05:57.291040   39530 api_server.go:141] control plane version: v1.30.0
I0705 15:05:57.291047   39530 api_server.go:131] duration metric: took 1.004003808s to wait for apiserver health ...
I0705 15:05:57.291052   39530 system_pods.go:43] waiting for kube-system pods to appear ...
I0705 15:05:57.295395   39530 system_pods.go:59] 7 kube-system pods found
I0705 15:05:57.295408   39530 system_pods.go:61] "coredns-7c445c467-g79kg" [aa59004b-3cbd-4ff5-bde6-d4f26aea71bc] Running / Ready:ContainersNotReady (containers with unready status: [coredns]) / ContainersReady:ContainersNotReady (containers with unready status: [coredns])
I0705 15:05:57.295413   39530 system_pods.go:61] "etcd-minikube" [bbddfc00-09a2-4f17-8339-13d9757d182d] Running / Ready:ContainersNotReady (containers with unready status: [etcd]) / ContainersReady:ContainersNotReady (containers with unready status: [etcd])
I0705 15:05:57.295417   39530 system_pods.go:61] "kube-apiserver-minikube" [4f8ce07b-3f00-4415-ba4f-43386508ebd1] Running / Ready:ContainersNotReady (containers with unready status: [kube-apiserver]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-apiserver])
I0705 15:05:57.295421   39530 system_pods.go:61] "kube-controller-manager-minikube" [c590ecb6-3885-48b4-8c6d-2f61f4cc8184] Running / Ready:ContainersNotReady (containers with unready status: [kube-controller-manager]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-controller-manager])
I0705 15:05:57.295424   39530 system_pods.go:61] "kube-proxy-nx76r" [84bf6f89-6056-433e-8d73-a7b1bc0179d7] Running / Ready:ContainersNotReady (containers with unready status: [kube-proxy]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-proxy])
I0705 15:05:57.295427   39530 system_pods.go:61] "kube-scheduler-minikube" [d8952b3e-a5a0-495c-b79a-f3e5b897fcc5] Running / Ready:ContainersNotReady (containers with unready status: [kube-scheduler]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-scheduler])
I0705 15:05:57.295429   39530 system_pods.go:61] "storage-provisioner" [ae37d88d-9905-4974-8e4d-04f44e84dcba] Running / Ready:ContainersNotReady (containers with unready status: [storage-provisioner]) / ContainersReady:ContainersNotReady (containers with unready status: [storage-provisioner])
I0705 15:05:57.295434   39530 system_pods.go:74] duration metric: took 4.378238ms to wait for pod list to return data ...
I0705 15:05:57.295441   39530 kubeadm.go:576] duration metric: took 3.330587988s to wait for: map[apiserver:true system_pods:true]
I0705 15:05:57.295448   39530 node_conditions.go:102] verifying NodePressure condition ...
I0705 15:05:57.297209   39530 node_conditions.go:122] node storage ephemeral capacity is 490616760Ki
I0705 15:05:57.297219   39530 node_conditions.go:123] node cpu capacity is 12
I0705 15:05:57.297225   39530 node_conditions.go:105] duration metric: took 1.774525ms to run NodePressure ...
I0705 15:05:57.297231   39530 start.go:240] waiting for startup goroutines ...
I0705 15:05:57.297235   39530 start.go:245] waiting for cluster config update ...
I0705 15:05:57.297242   39530 start.go:254] writing updated cluster config ...
I0705 15:05:57.297395   39530 ssh_runner.go:195] Run: rm -f paused
I0705 15:05:57.332755   39530 start.go:600] kubectl: 1.30.0, cluster: 1.30.0 (minor skew: 0)
I0705 15:05:57.337559   39530 out.go:177] üèÑ  ÂÆåÊàêÔºÅkubectl Áé∞Âú®Â∑≤ÈÖçÁΩÆÔºåÈªòËÆ§‰ΩøÁî®"minikube"ÈõÜÁæ§Âíå"default"ÂëΩÂêçÁ©∫Èó¥


==> Docker <==
Jul 08 04:18:41 minikube dockerd[941]: 2024/07/08 04:18:41 http: superfluous response.WriteHeader call from go.opentelemetry.io/contrib/instrumentation/net/http/otelhttp.(*respWriterWrapper).WriteHeader (wrap.go:98)
Jul 08 04:18:41 minikube dockerd[941]: 2024/07/08 04:18:41 http: superfluous response.WriteHeader call from go.opentelemetry.io/contrib/instrumentation/net/http/otelhttp.(*respWriterWrapper).WriteHeader (wrap.go:98)
Jul 08 04:18:41 minikube dockerd[941]: 2024/07/08 04:18:41 http: superfluous response.WriteHeader call from go.opentelemetry.io/contrib/instrumentation/net/http/otelhttp.(*respWriterWrapper).WriteHeader (wrap.go:98)
Jul 08 04:18:41 minikube dockerd[941]: 2024/07/08 04:18:41 http: superfluous response.WriteHeader call from go.opentelemetry.io/contrib/instrumentation/net/http/otelhttp.(*respWriterWrapper).WriteHeader (wrap.go:98)
Jul 08 04:18:41 minikube dockerd[941]: 2024/07/08 04:18:41 http: superfluous response.WriteHeader call from go.opentelemetry.io/contrib/instrumentation/net/http/otelhttp.(*respWriterWrapper).WriteHeader (wrap.go:98)
Jul 08 04:18:41 minikube dockerd[941]: 2024/07/08 04:18:41 http: superfluous response.WriteHeader call from go.opentelemetry.io/contrib/instrumentation/net/http/otelhttp.(*respWriterWrapper).WriteHeader (wrap.go:98)
Jul 08 04:18:41 minikube dockerd[941]: 2024/07/08 04:18:41 http: superfluous response.WriteHeader call from go.opentelemetry.io/contrib/instrumentation/net/http/otelhttp.(*respWriterWrapper).WriteHeader (wrap.go:98)
Jul 08 04:18:41 minikube dockerd[941]: 2024/07/08 04:18:41 http: superfluous response.WriteHeader call from go.opentelemetry.io/contrib/instrumentation/net/http/otelhttp.(*respWriterWrapper).WriteHeader (wrap.go:98)
Jul 08 04:18:41 minikube dockerd[941]: 2024/07/08 04:18:41 http: superfluous response.WriteHeader call from go.opentelemetry.io/contrib/instrumentation/net/http/otelhttp.(*respWriterWrapper).WriteHeader (wrap.go:98)
Jul 08 04:18:41 minikube dockerd[941]: 2024/07/08 04:18:41 http: superfluous response.WriteHeader call from go.opentelemetry.io/contrib/instrumentation/net/http/otelhttp.(*respWriterWrapper).WriteHeader (wrap.go:98)
Jul 08 04:18:41 minikube dockerd[941]: 2024/07/08 04:18:41 http: superfluous response.WriteHeader call from go.opentelemetry.io/contrib/instrumentation/net/http/otelhttp.(*respWriterWrapper).WriteHeader (wrap.go:98)
Jul 08 04:34:12 minikube dockerd[941]: 2024/07/08 04:34:12 http: superfluous response.WriteHeader call from go.opentelemetry.io/contrib/instrumentation/net/http/otelhttp.(*respWriterWrapper).WriteHeader (wrap.go:98)
Jul 08 04:34:12 minikube dockerd[941]: 2024/07/08 04:34:12 http: superfluous response.WriteHeader call from go.opentelemetry.io/contrib/instrumentation/net/http/otelhttp.(*respWriterWrapper).WriteHeader (wrap.go:98)
Jul 08 04:34:12 minikube dockerd[941]: 2024/07/08 04:34:12 http: superfluous response.WriteHeader call from go.opentelemetry.io/contrib/instrumentation/net/http/otelhttp.(*respWriterWrapper).WriteHeader (wrap.go:98)
Jul 08 04:34:12 minikube dockerd[941]: 2024/07/08 04:34:12 http: superfluous response.WriteHeader call from go.opentelemetry.io/contrib/instrumentation/net/http/otelhttp.(*respWriterWrapper).WriteHeader (wrap.go:98)
Jul 08 04:34:13 minikube dockerd[941]: 2024/07/08 04:34:13 http: superfluous response.WriteHeader call from go.opentelemetry.io/contrib/instrumentation/net/http/otelhttp.(*respWriterWrapper).WriteHeader (wrap.go:98)
Jul 08 04:34:13 minikube dockerd[941]: 2024/07/08 04:34:13 http: superfluous response.WriteHeader call from go.opentelemetry.io/contrib/instrumentation/net/http/otelhttp.(*respWriterWrapper).WriteHeader (wrap.go:98)
Jul 08 04:34:13 minikube dockerd[941]: 2024/07/08 04:34:13 http: superfluous response.WriteHeader call from go.opentelemetry.io/contrib/instrumentation/net/http/otelhttp.(*respWriterWrapper).WriteHeader (wrap.go:98)
Jul 08 04:34:13 minikube dockerd[941]: 2024/07/08 04:34:13 http: superfluous response.WriteHeader call from go.opentelemetry.io/contrib/instrumentation/net/http/otelhttp.(*respWriterWrapper).WriteHeader (wrap.go:98)
Jul 08 04:34:13 minikube dockerd[941]: 2024/07/08 04:34:13 http: superfluous response.WriteHeader call from go.opentelemetry.io/contrib/instrumentation/net/http/otelhttp.(*respWriterWrapper).WriteHeader (wrap.go:98)
Jul 08 04:34:13 minikube dockerd[941]: 2024/07/08 04:34:13 http: superfluous response.WriteHeader call from go.opentelemetry.io/contrib/instrumentation/net/http/otelhttp.(*respWriterWrapper).WriteHeader (wrap.go:98)
Jul 08 04:34:13 minikube dockerd[941]: 2024/07/08 04:34:13 http: superfluous response.WriteHeader call from go.opentelemetry.io/contrib/instrumentation/net/http/otelhttp.(*respWriterWrapper).WriteHeader (wrap.go:98)
Jul 08 04:34:13 minikube dockerd[941]: 2024/07/08 04:34:13 http: superfluous response.WriteHeader call from go.opentelemetry.io/contrib/instrumentation/net/http/otelhttp.(*respWriterWrapper).WriteHeader (wrap.go:98)
Jul 08 04:34:13 minikube dockerd[941]: 2024/07/08 04:34:13 http: superfluous response.WriteHeader call from go.opentelemetry.io/contrib/instrumentation/net/http/otelhttp.(*respWriterWrapper).WriteHeader (wrap.go:98)
Jul 08 04:34:13 minikube dockerd[941]: 2024/07/08 04:34:13 http: superfluous response.WriteHeader call from go.opentelemetry.io/contrib/instrumentation/net/http/otelhttp.(*respWriterWrapper).WriteHeader (wrap.go:98)
Jul 08 04:34:28 minikube dockerd[941]: 2024/07/08 04:34:28 http: superfluous response.WriteHeader call from go.opentelemetry.io/contrib/instrumentation/net/http/otelhttp.(*respWriterWrapper).WriteHeader (wrap.go:98)
Jul 08 04:34:28 minikube dockerd[941]: 2024/07/08 04:34:28 http: superfluous response.WriteHeader call from go.opentelemetry.io/contrib/instrumentation/net/http/otelhttp.(*respWriterWrapper).WriteHeader (wrap.go:98)
Jul 08 04:34:29 minikube dockerd[941]: 2024/07/08 04:34:29 http: superfluous response.WriteHeader call from go.opentelemetry.io/contrib/instrumentation/net/http/otelhttp.(*respWriterWrapper).WriteHeader (wrap.go:98)
Jul 08 04:34:29 minikube dockerd[941]: 2024/07/08 04:34:29 http: superfluous response.WriteHeader call from go.opentelemetry.io/contrib/instrumentation/net/http/otelhttp.(*respWriterWrapper).WriteHeader (wrap.go:98)
Jul 08 04:34:29 minikube dockerd[941]: 2024/07/08 04:34:29 http: superfluous response.WriteHeader call from go.opentelemetry.io/contrib/instrumentation/net/http/otelhttp.(*respWriterWrapper).WriteHeader (wrap.go:98)
Jul 08 04:34:29 minikube dockerd[941]: 2024/07/08 04:34:29 http: superfluous response.WriteHeader call from go.opentelemetry.io/contrib/instrumentation/net/http/otelhttp.(*respWriterWrapper).WriteHeader (wrap.go:98)
Jul 08 04:34:29 minikube dockerd[941]: 2024/07/08 04:34:29 http: superfluous response.WriteHeader call from go.opentelemetry.io/contrib/instrumentation/net/http/otelhttp.(*respWriterWrapper).WriteHeader (wrap.go:98)
Jul 08 04:34:29 minikube dockerd[941]: 2024/07/08 04:34:29 http: superfluous response.WriteHeader call from go.opentelemetry.io/contrib/instrumentation/net/http/otelhttp.(*respWriterWrapper).WriteHeader (wrap.go:98)
Jul 08 04:34:29 minikube dockerd[941]: 2024/07/08 04:34:29 http: superfluous response.WriteHeader call from go.opentelemetry.io/contrib/instrumentation/net/http/otelhttp.(*respWriterWrapper).WriteHeader (wrap.go:98)
Jul 08 04:34:29 minikube dockerd[941]: 2024/07/08 04:34:29 http: superfluous response.WriteHeader call from go.opentelemetry.io/contrib/instrumentation/net/http/otelhttp.(*respWriterWrapper).WriteHeader (wrap.go:98)
Jul 08 04:34:29 minikube dockerd[941]: 2024/07/08 04:34:29 http: superfluous response.WriteHeader call from go.opentelemetry.io/contrib/instrumentation/net/http/otelhttp.(*respWriterWrapper).WriteHeader (wrap.go:98)
Jul 08 04:34:29 minikube dockerd[941]: 2024/07/08 04:34:29 http: superfluous response.WriteHeader call from go.opentelemetry.io/contrib/instrumentation/net/http/otelhttp.(*respWriterWrapper).WriteHeader (wrap.go:98)
Jul 08 04:34:29 minikube dockerd[941]: 2024/07/08 04:34:29 http: superfluous response.WriteHeader call from go.opentelemetry.io/contrib/instrumentation/net/http/otelhttp.(*respWriterWrapper).WriteHeader (wrap.go:98)
Jul 08 04:34:29 minikube dockerd[941]: 2024/07/08 04:34:29 http: superfluous response.WriteHeader call from go.opentelemetry.io/contrib/instrumentation/net/http/otelhttp.(*respWriterWrapper).WriteHeader (wrap.go:98)
Jul 08 06:10:45 minikube dockerd[941]: 2024/07/08 06:10:45 http: superfluous response.WriteHeader call from go.opentelemetry.io/contrib/instrumentation/net/http/otelhttp.(*respWriterWrapper).WriteHeader (wrap.go:98)
Jul 08 06:10:45 minikube dockerd[941]: 2024/07/08 06:10:45 http: superfluous response.WriteHeader call from go.opentelemetry.io/contrib/instrumentation/net/http/otelhttp.(*respWriterWrapper).WriteHeader (wrap.go:98)
Jul 08 06:10:46 minikube dockerd[941]: 2024/07/08 06:10:46 http: superfluous response.WriteHeader call from go.opentelemetry.io/contrib/instrumentation/net/http/otelhttp.(*respWriterWrapper).WriteHeader (wrap.go:98)
Jul 08 06:10:46 minikube dockerd[941]: 2024/07/08 06:10:46 http: superfluous response.WriteHeader call from go.opentelemetry.io/contrib/instrumentation/net/http/otelhttp.(*respWriterWrapper).WriteHeader (wrap.go:98)
Jul 08 06:10:46 minikube dockerd[941]: 2024/07/08 06:10:46 http: superfluous response.WriteHeader call from go.opentelemetry.io/contrib/instrumentation/net/http/otelhttp.(*respWriterWrapper).WriteHeader (wrap.go:98)
Jul 08 06:10:46 minikube dockerd[941]: 2024/07/08 06:10:46 http: superfluous response.WriteHeader call from go.opentelemetry.io/contrib/instrumentation/net/http/otelhttp.(*respWriterWrapper).WriteHeader (wrap.go:98)
Jul 08 06:10:46 minikube dockerd[941]: 2024/07/08 06:10:46 http: superfluous response.WriteHeader call from go.opentelemetry.io/contrib/instrumentation/net/http/otelhttp.(*respWriterWrapper).WriteHeader (wrap.go:98)
Jul 08 06:10:46 minikube dockerd[941]: 2024/07/08 06:10:46 http: superfluous response.WriteHeader call from go.opentelemetry.io/contrib/instrumentation/net/http/otelhttp.(*respWriterWrapper).WriteHeader (wrap.go:98)
Jul 08 06:10:46 minikube dockerd[941]: 2024/07/08 06:10:46 http: superfluous response.WriteHeader call from go.opentelemetry.io/contrib/instrumentation/net/http/otelhttp.(*respWriterWrapper).WriteHeader (wrap.go:98)
Jul 08 06:10:46 minikube dockerd[941]: 2024/07/08 06:10:46 http: superfluous response.WriteHeader call from go.opentelemetry.io/contrib/instrumentation/net/http/otelhttp.(*respWriterWrapper).WriteHeader (wrap.go:98)
Jul 08 06:10:46 minikube dockerd[941]: 2024/07/08 06:10:46 http: superfluous response.WriteHeader call from go.opentelemetry.io/contrib/instrumentation/net/http/otelhttp.(*respWriterWrapper).WriteHeader (wrap.go:98)
Jul 08 06:10:46 minikube dockerd[941]: 2024/07/08 06:10:46 http: superfluous response.WriteHeader call from go.opentelemetry.io/contrib/instrumentation/net/http/otelhttp.(*respWriterWrapper).WriteHeader (wrap.go:98)
Jul 08 06:10:46 minikube dockerd[941]: 2024/07/08 06:10:46 http: superfluous response.WriteHeader call from go.opentelemetry.io/contrib/instrumentation/net/http/otelhttp.(*respWriterWrapper).WriteHeader (wrap.go:98)
Jul 08 06:10:46 minikube dockerd[941]: 2024/07/08 06:10:46 http: superfluous response.WriteHeader call from go.opentelemetry.io/contrib/instrumentation/net/http/otelhttp.(*respWriterWrapper).WriteHeader (wrap.go:98)
Jul 08 06:49:08 minikube dockerd[941]: time="2024-07-08T06:49:08.874718622Z" level=info msg="ignoring event" container=5f701fef7925ad6c139f45592c637ff2b98df468660bd17c3408d7b21f9b0f08 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Jul 08 06:49:08 minikube dockerd[941]: time="2024-07-08T06:49:08.931623194Z" level=info msg="ignoring event" container=639984f80550e76dc879d3dcc966b44863d2c288ef3ac6ca67a1764b367b1c6c module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Jul 08 06:49:09 minikube cri-dockerd[1202]: time="2024-07-08T06:49:09Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/d9feeff1448d69aae9947636ed90a1dc21eb8fdb0f585902ec78957c52f59b9f/resolv.conf as [nameserver 192.168.58.1 options edns0 trust-ad ndots:0]"
Jul 08 06:49:12 minikube cri-dockerd[1202]: time="2024-07-08T06:49:12Z" level=error msg="error getting RW layer size for container ID 'b7615fc7a1b9495c88508f12983858fa9ae9d16113c602daad95ad5dea12839e': Error response from daemon: No such container: b7615fc7a1b9495c88508f12983858fa9ae9d16113c602daad95ad5dea12839e"
Jul 08 06:49:12 minikube cri-dockerd[1202]: time="2024-07-08T06:49:12Z" level=error msg="Set backoffDuration to : 1m0s for container ID 'b7615fc7a1b9495c88508f12983858fa9ae9d16113c602daad95ad5dea12839e'"
Jul 08 06:49:12 minikube cri-dockerd[1202]: time="2024-07-08T06:49:12Z" level=error msg="error getting RW layer size for container ID '5f701fef7925ad6c139f45592c637ff2b98df468660bd17c3408d7b21f9b0f08': Error response from daemon: No such container: 5f701fef7925ad6c139f45592c637ff2b98df468660bd17c3408d7b21f9b0f08"
Jul 08 06:49:12 minikube cri-dockerd[1202]: time="2024-07-08T06:49:12Z" level=error msg="Set backoffDuration to : 1m0s for container ID '5f701fef7925ad6c139f45592c637ff2b98df468660bd17c3408d7b21f9b0f08'"


==> container status <==
CONTAINER           IMAGE                                                                                     CREATED             STATE               NAME                      ATTEMPT             POD ID              POD
e474f8be7678f       a0bf559e280cf                                                                             3 minutes ago       Running             kube-proxy                0                   d9feeff1448d6       kube-proxy-r82ck
e28ef531d87ef       prom/prometheus@sha256:15ccbb1cec5fad2cd9f20f574ba5a4dd4160e8472213c76faac17f6481cb6a75   3 hours ago         Running             prometheus                0                   72b881692fbc6       prometheus-cdbc74675-qk5x9
14bd235c81f53       9beeba249f3ee                                                                             2 days ago          Running             server                    0                   90ebee0138f26       sammy-app-dev-79d5c7f686-drvhg
7946fadd3dfe5       6e38f40d628db                                                                             3 days ago          Running             storage-provisioner       5                   6806aa93c174c       storage-provisioner
ff442c0870982       6de25f09e41b3                                                                             3 days ago          Running             server                    2                   70ae7abd343bc       blog-web-dev-5db6d8554b-7dn4g
c982c09b63bd3       89a3c9d82f7aa                                                                             3 days ago          Running             traefik                   2                   e38001b3966a5       traefik-deployment-9f49c9d87-5z4kx
3fe399b562c80       2abb8b4241bb1                                                                             3 days ago          Running             istio-proxy               2                   9959607d49f04       istio-egressgateway-77d7544cf7-jch6b
4571ae42f4ac9       2abb8b4241bb1                                                                             3 days ago          Running             istio-proxy               2                   493aa5cc036f7       istio-ingressgateway-ff47dcc89-mjj92
7fe4a0ae6607a       cf42fa92eee0d                                                                             3 days ago          Running             discovery                 2                   f4cac25f56930       istiod-7c948c8756-4mcdl
5c627330e574e       6e38f40d628db                                                                             3 days ago          Exited              storage-provisioner       4                   6806aa93c174c       storage-provisioner
41975226fa7f2       cbb01a7bd410d                                                                             3 days ago          Running             coredns                   2                   de7b0a32c65cc       coredns-7c445c467-g79kg
cf4d19cdbdb9b       3861cfcd7c04c                                                                             3 days ago          Running             etcd                      2                   94fe84a2c0173       etcd-minikube
6765934f60a26       c7aad43836fa5                                                                             3 days ago          Running             kube-controller-manager   2                   b14e623e11835       kube-controller-manager-minikube
88f69a23f2def       c42f13656d0b2                                                                             3 days ago          Running             kube-apiserver            2                   069994915ee2c       kube-apiserver-minikube
416e9dc66185d       259c8277fcbbc                                                                             3 days ago          Running             kube-scheduler            2                   49f50121ca708       kube-scheduler-minikube
80f8a9d8e59dc       89a3c9d82f7aa                                                                             3 days ago          Exited              traefik                   1                   8dc37659bfd7f       traefik-deployment-9f49c9d87-5z4kx
3becec6102d61       6de25f09e41b3                                                                             3 days ago          Exited              server                    1                   abfaff73e820b       blog-web-dev-5db6d8554b-7dn4g
84928e1aee7c2       cbb01a7bd410d                                                                             3 days ago          Exited              coredns                   1                   d0582644f9fe9       coredns-7c445c467-g79kg
d1623cd7dba43       2abb8b4241bb1                                                                             3 days ago          Exited              istio-proxy               1                   78cee6a02f19f       istio-egressgateway-77d7544cf7-jch6b
7c56b45b6282f       cf42fa92eee0d                                                                             3 days ago          Exited              discovery                 1                   4861de0d028bb       istiod-7c948c8756-4mcdl
9ffe2023ed242       2abb8b4241bb1                                                                             3 days ago          Exited              istio-proxy               1                   358dd1bea6fe6       istio-ingressgateway-ff47dcc89-mjj92
b24dd1a5dfbcd       259c8277fcbbc                                                                             3 days ago          Exited              kube-scheduler            1                   28ef00e5f1ad6       kube-scheduler-minikube
5902080ba6469       c7aad43836fa5                                                                             3 days ago          Exited              kube-controller-manager   1                   08bf6a82fa8da       kube-controller-manager-minikube
146fa6240acde       3861cfcd7c04c                                                                             3 days ago          Exited              etcd                      1                   917f765a96e69       etcd-minikube
bed85e4092426       c42f13656d0b2                                                                             3 days ago          Exited              kube-apiserver            1                   009aa3cc23c03       kube-apiserver-minikube


==> coredns [41975226fa7f] <==
[INFO] 10.244.0.19:35221 - 39162 "A IN zipkin.istio-system.svc.cluster.local. udp 55 false 512" NXDOMAIN qr,aa,rd 148 0.000084262s
[INFO] 10.244.0.17:34997 - 28814 "A IN zipkin.istio-system.svc.cluster.local. udp 55 false 512" NXDOMAIN qr,aa,rd 148 0.000058412s
[INFO] 10.244.0.19:35221 - 28606 "A IN zipkin.istio-system.cluster.local. udp 51 false 512" NXDOMAIN qr,aa,rd 144 0.000068771s
[INFO] 10.244.0.17:34997 - 43730 "A IN zipkin.istio-system.cluster.local. udp 51 false 512" NXDOMAIN qr,aa,rd 144 0.000055231s
[INFO] 10.244.0.17:34997 - 29685 "A IN zipkin.istio-system. udp 37 false 512" NXDOMAIN qr,rd,ra 37 0.000840776s
[INFO] 10.244.0.19:35221 - 23715 "A IN zipkin.istio-system. udp 37 false 512" NXDOMAIN qr,rd,ra 37 0.00091375s
[INFO] 10.244.0.17:49246 - 19382 "A IN zipkin.istio-system.istio-system.svc.cluster.local. udp 68 false 512" NXDOMAIN qr,aa,rd 161 0.000103147s
[INFO] 10.244.0.17:49246 - 39744 "A IN zipkin.istio-system.svc.cluster.local. udp 55 false 512" NXDOMAIN qr,aa,rd 148 0.000068044s
[INFO] 10.244.0.17:49246 - 32501 "A IN zipkin.istio-system.cluster.local. udp 51 false 512" NXDOMAIN qr,aa,rd 144 0.000050478s
[INFO] 10.244.0.19:44904 - 40745 "A IN zipkin.istio-system.istio-system.svc.cluster.local. udp 68 false 512" NXDOMAIN qr,aa,rd 161 0.000028871s
[INFO] 10.244.0.19:44904 - 26518 "A IN zipkin.istio-system.svc.cluster.local. udp 55 false 512" NXDOMAIN qr,aa,rd 148 0.00003401s
[INFO] 10.244.0.19:44904 - 63123 "A IN zipkin.istio-system.cluster.local. udp 51 false 512" NXDOMAIN qr,aa,rd 144 0.000024184s
[INFO] 10.244.0.19:44904 - 22951 "A IN zipkin.istio-system. udp 37 false 512" NXDOMAIN qr,rd,ra 37 0.00038701s
[INFO] 10.244.0.17:49246 - 55124 "A IN zipkin.istio-system. udp 37 false 512" NXDOMAIN qr,rd,ra 37 0.000865971s
[INFO] 10.244.0.17:55707 - 39257 "A IN zipkin.istio-system.istio-system.svc.cluster.local. udp 68 false 512" NXDOMAIN qr,aa,rd 161 0.000134722s
[INFO] 10.244.0.19:41615 - 56317 "A IN zipkin.istio-system.istio-system.svc.cluster.local. udp 68 false 512" NXDOMAIN qr,aa,rd 161 0.000088463s
[INFO] 10.244.0.17:55707 - 15760 "A IN zipkin.istio-system.svc.cluster.local. udp 55 false 512" NXDOMAIN qr,aa,rd 148 0.00008855s
[INFO] 10.244.0.19:41615 - 28405 "A IN zipkin.istio-system.svc.cluster.local. udp 55 false 512" NXDOMAIN qr,aa,rd 148 0.000072897s
[INFO] 10.244.0.17:55707 - 51244 "A IN zipkin.istio-system.cluster.local. udp 51 false 512" NXDOMAIN qr,aa,rd 144 0.000054784s
[INFO] 10.244.0.19:41615 - 57960 "A IN zipkin.istio-system.cluster.local. udp 51 false 512" NXDOMAIN qr,aa,rd 144 0.000068401s
[INFO] 10.244.0.17:55707 - 9771 "A IN zipkin.istio-system. udp 37 false 512" NXDOMAIN qr,rd,ra 37 0.001151275s
[INFO] 10.244.0.19:41615 - 24526 "A IN zipkin.istio-system. udp 37 false 512" NXDOMAIN qr,rd,ra 37 0.001177489s
[INFO] 10.244.0.17:54490 - 56360 "A IN zipkin.istio-system.istio-system.svc.cluster.local. udp 68 false 512" NXDOMAIN qr,aa,rd 161 0.000125813s
[INFO] 10.244.0.17:54490 - 28503 "A IN zipkin.istio-system.svc.cluster.local. udp 55 false 512" NXDOMAIN qr,aa,rd 148 0.000095302s
[INFO] 10.244.0.19:43241 - 29269 "A IN zipkin.istio-system.istio-system.svc.cluster.local. udp 68 false 512" NXDOMAIN qr,aa,rd 161 0.000063672s
[INFO] 10.244.0.19:43241 - 52811 "A IN zipkin.istio-system.svc.cluster.local. udp 55 false 512" NXDOMAIN qr,aa,rd 148 0.000028603s
[INFO] 10.244.0.17:54490 - 44447 "A IN zipkin.istio-system.cluster.local. udp 51 false 512" NXDOMAIN qr,aa,rd 144 0.000081085s
[INFO] 10.244.0.19:43241 - 39094 "A IN zipkin.istio-system.cluster.local. udp 51 false 512" NXDOMAIN qr,aa,rd 144 0.000036961s
[INFO] 10.244.0.17:54490 - 20184 "A IN zipkin.istio-system. udp 37 false 512" NXDOMAIN qr,rd,ra 37 0.000936228s
[INFO] 10.244.0.19:43241 - 28309 "A IN zipkin.istio-system. udp 37 false 512" NXDOMAIN qr,rd,ra 37 0.000932826s
[INFO] 10.244.0.17:57881 - 64469 "AAAA IN istiod.istio-system.svc.istio-system.svc.cluster.local. udp 83 false 1232" NXDOMAIN qr,aa,rd 165 0.000127343s
[INFO] 10.244.0.17:33039 - 64386 "A IN istiod.istio-system.svc.istio-system.svc.cluster.local. udp 83 false 1232" NXDOMAIN qr,aa,rd 165 0.00016931s
[INFO] 10.244.0.17:55622 - 16449 "A IN istiod.istio-system.svc.svc.cluster.local. udp 70 false 1232" NXDOMAIN qr,aa,rd 152 0.000060227s
[INFO] 10.244.0.17:36112 - 52639 "AAAA IN istiod.istio-system.svc.svc.cluster.local. udp 70 false 1232" NXDOMAIN qr,aa,rd 152 0.000085346s
[INFO] 10.244.0.17:38650 - 23687 "A IN istiod.istio-system.svc.cluster.local. udp 66 false 1232" NOERROR qr,aa,rd 108 0.000066686s
[INFO] 10.244.0.17:46028 - 42926 "AAAA IN istiod.istio-system.svc.cluster.local. udp 66 false 1232" NOERROR qr,aa,rd 148 0.000099131s
[INFO] 10.244.0.19:57939 - 57262 "A IN zipkin.istio-system.istio-system.svc.cluster.local. udp 68 false 512" NXDOMAIN qr,aa,rd 161 0.000114688s
[INFO] 10.244.0.17:60672 - 51160 "A IN zipkin.istio-system.istio-system.svc.cluster.local. udp 68 false 512" NXDOMAIN qr,aa,rd 161 0.000048799s
[INFO] 10.244.0.19:57939 - 51783 "A IN zipkin.istio-system.svc.cluster.local. udp 55 false 512" NXDOMAIN qr,aa,rd 148 0.000074505s
[INFO] 10.244.0.17:60672 - 15308 "A IN zipkin.istio-system.svc.cluster.local. udp 55 false 512" NXDOMAIN qr,aa,rd 148 0.000031958s
[INFO] 10.244.0.19:57939 - 35065 "A IN zipkin.istio-system.cluster.local. udp 51 false 512" NXDOMAIN qr,aa,rd 144 0.000056401s
[INFO] 10.244.0.17:60672 - 21835 "A IN zipkin.istio-system.cluster.local. udp 51 false 512" NXDOMAIN qr,aa,rd 144 0.000030654s
[INFO] 10.244.0.17:60672 - 63936 "A IN zipkin.istio-system. udp 37 false 512" NXDOMAIN qr,rd,ra 37 0.000840904s
[INFO] 10.244.0.19:57939 - 27760 "A IN zipkin.istio-system. udp 37 false 512" NXDOMAIN qr,rd,ra 37 0.000917413s
[INFO] 10.244.0.19:57886 - 2001 "A IN zipkin.istio-system.istio-system.svc.cluster.local. udp 68 false 512" NXDOMAIN qr,aa,rd 161 0.000102184s
[INFO] 10.244.0.19:57886 - 25280 "A IN zipkin.istio-system.svc.cluster.local. udp 55 false 512" NXDOMAIN qr,aa,rd 148 0.000059515s
[INFO] 10.244.0.17:50330 - 59468 "A IN zipkin.istio-system.istio-system.svc.cluster.local. udp 68 false 512" NXDOMAIN qr,aa,rd 161 0.000043243s
[INFO] 10.244.0.17:50330 - 46131 "A IN zipkin.istio-system.svc.cluster.local. udp 55 false 512" NXDOMAIN qr,aa,rd 148 0.000031159s
[INFO] 10.244.0.19:57886 - 63935 "A IN zipkin.istio-system.cluster.local. udp 51 false 512" NXDOMAIN qr,aa,rd 144 0.000073851s
[INFO] 10.244.0.17:50330 - 23747 "A IN zipkin.istio-system.cluster.local. udp 51 false 512" NXDOMAIN qr,aa,rd 144 0.000030324s
[INFO] 10.244.0.17:50330 - 13503 "A IN zipkin.istio-system. udp 37 false 512" NXDOMAIN qr,rd,ra 37 0.000783496s
[INFO] 10.244.0.19:57886 - 28831 "A IN zipkin.istio-system. udp 37 false 512" NXDOMAIN qr,rd,ra 37 0.000878934s
[INFO] 10.244.0.17:46753 - 60251 "A IN zipkin.istio-system.istio-system.svc.cluster.local. udp 68 false 512" NXDOMAIN qr,aa,rd 161 0.00009785s
[INFO] 10.244.0.19:48336 - 36306 "A IN zipkin.istio-system.istio-system.svc.cluster.local. udp 68 false 512" NXDOMAIN qr,aa,rd 161 0.000088139s
[INFO] 10.244.0.17:46753 - 53306 "A IN zipkin.istio-system.svc.cluster.local. udp 55 false 512" NXDOMAIN qr,aa,rd 148 0.000075589s
[INFO] 10.244.0.19:48336 - 21876 "A IN zipkin.istio-system.svc.cluster.local. udp 55 false 512" NXDOMAIN qr,aa,rd 148 0.000048319s
[INFO] 10.244.0.17:46753 - 16671 "A IN zipkin.istio-system.cluster.local. udp 51 false 512" NXDOMAIN qr,aa,rd 144 0.000050557s
[INFO] 10.244.0.19:48336 - 56758 "A IN zipkin.istio-system.cluster.local. udp 51 false 512" NXDOMAIN qr,aa,rd 144 0.000067075s
[INFO] 10.244.0.17:46753 - 18678 "A IN zipkin.istio-system. udp 37 false 512" NXDOMAIN qr,rd,ra 37 0.001153341s
[INFO] 10.244.0.19:48336 - 43753 "A IN zipkin.istio-system. udp 37 false 512" NXDOMAIN qr,rd,ra 37 0.001175752s


==> coredns [84928e1aee7c] <==
[INFO] 10.244.0.9:32852 - 25042 "A IN zipkin.istio-system.cluster.local. udp 51 false 512" NXDOMAIN qr,aa,rd 144 0.000035747s
[INFO] 10.244.0.9:32852 - 49228 "A IN zipkin.istio-system. udp 37 false 512" NXDOMAIN qr,aa,rd,ra 112 0.000024346s
[INFO] 10.244.0.11:58562 - 40114 "A IN zipkin.istio-system.istio-system.svc.cluster.local. udp 68 false 512" NXDOMAIN qr,aa,rd 161 0.00010194s
[INFO] 10.244.0.11:58562 - 49242 "A IN zipkin.istio-system.svc.cluster.local. udp 55 false 512" NXDOMAIN qr,aa,rd 148 0.000073121s
[INFO] 10.244.0.11:58562 - 64654 "A IN zipkin.istio-system.cluster.local. udp 51 false 512" NXDOMAIN qr,aa,rd 144 0.000054712s
[INFO] 10.244.0.11:58562 - 31965 "A IN zipkin.istio-system. udp 37 false 512" NXDOMAIN qr,aa,rd,ra 112 0.000026179s
[INFO] 10.244.0.9:58780 - 33667 "A IN zipkin.istio-system.istio-system.svc.cluster.local. udp 68 false 512" NXDOMAIN qr,aa,rd 161 0.000061862s
[INFO] 10.244.0.9:58780 - 19339 "A IN zipkin.istio-system.svc.cluster.local. udp 55 false 512" NXDOMAIN qr,aa,rd 148 0.000041725s
[INFO] 10.244.0.9:58780 - 26502 "A IN zipkin.istio-system.cluster.local. udp 51 false 512" NXDOMAIN qr,aa,rd 144 0.000033484s
[INFO] 10.244.0.9:58780 - 22990 "A IN zipkin.istio-system. udp 37 false 512" NXDOMAIN qr,rd,ra 37 0.000854185s
[INFO] 10.244.0.11:50330 - 41357 "A IN zipkin.istio-system.istio-system.svc.cluster.local. udp 68 false 512" NXDOMAIN qr,aa,rd 161 0.000097883s
[INFO] 10.244.0.11:50330 - 49186 "A IN zipkin.istio-system.svc.cluster.local. udp 55 false 512" NXDOMAIN qr,aa,rd 148 0.000082216s
[INFO] 10.244.0.11:50330 - 29416 "A IN zipkin.istio-system.cluster.local. udp 51 false 512" NXDOMAIN qr,aa,rd 144 0.000049941s
[INFO] 10.244.0.11:50330 - 49362 "A IN zipkin.istio-system. udp 37 false 512" NXDOMAIN qr,rd,ra 37 0.000837181s
[INFO] 10.244.0.9:50069 - 56415 "A IN zipkin.istio-system.istio-system.svc.cluster.local. udp 68 false 512" NXDOMAIN qr,aa,rd 161 0.000070123s
[INFO] 10.244.0.9:50069 - 37862 "A IN zipkin.istio-system.svc.cluster.local. udp 55 false 512" NXDOMAIN qr,aa,rd 148 0.000056924s
[INFO] 10.244.0.9:50069 - 36787 "A IN zipkin.istio-system.cluster.local. udp 51 false 512" NXDOMAIN qr,aa,rd 144 0.000045496s
[INFO] 10.244.0.9:50069 - 32804 "A IN zipkin.istio-system. udp 37 false 512" NXDOMAIN qr,rd,ra 37 0.000958221s
[INFO] 10.244.0.11:52071 - 3372 "A IN zipkin.istio-system.istio-system.svc.cluster.local. udp 68 false 512" NXDOMAIN qr,aa,rd 161 0.000165789s
[INFO] 10.244.0.11:52071 - 19891 "A IN zipkin.istio-system.svc.cluster.local. udp 55 false 512" NXDOMAIN qr,aa,rd 148 0.000180259s
[INFO] 10.244.0.11:52071 - 37555 "A IN zipkin.istio-system.cluster.local. udp 51 false 512" NXDOMAIN qr,aa,rd 144 0.000095094s
[INFO] 10.244.0.11:52071 - 8484 "A IN zipkin.istio-system. udp 37 false 512" NXDOMAIN qr,rd,ra 37 0.001370141s
[INFO] 10.244.0.9:45542 - 45979 "A IN zipkin.istio-system.istio-system.svc.cluster.local. udp 68 false 512" NXDOMAIN qr,aa,rd 161 0.000060477s
[INFO] 10.244.0.9:45542 - 20474 "A IN zipkin.istio-system.svc.cluster.local. udp 55 false 512" NXDOMAIN qr,aa,rd 148 0.000039118s
[INFO] 10.244.0.9:45542 - 44706 "A IN zipkin.istio-system.cluster.local. udp 51 false 512" NXDOMAIN qr,aa,rd 144 0.000030725s
[INFO] 10.244.0.9:45542 - 29951 "A IN zipkin.istio-system. udp 37 false 512" NXDOMAIN qr,rd,ra 37 0.000850284s
[INFO] 10.244.0.11:39684 - 64460 "A IN zipkin.istio-system.istio-system.svc.cluster.local. udp 68 false 512" NXDOMAIN qr,aa,rd 161 0.000162352s
[INFO] 10.244.0.11:39684 - 60193 "A IN zipkin.istio-system.svc.cluster.local. udp 55 false 512" NXDOMAIN qr,aa,rd 148 0.00008476s
[INFO] 10.244.0.11:39684 - 40329 "A IN zipkin.istio-system.cluster.local. udp 51 false 512" NXDOMAIN qr,aa,rd 144 0.000096346s
[INFO] 10.244.0.11:39684 - 65387 "A IN zipkin.istio-system. udp 37 false 512" NXDOMAIN qr,rd,ra 37 0.001236139s
[INFO] 10.244.0.9:43469 - 15970 "A IN zipkin.istio-system.istio-system.svc.cluster.local. udp 68 false 512" NXDOMAIN qr,aa,rd 161 0.000065751s
[INFO] 10.244.0.9:43469 - 59541 "A IN zipkin.istio-system.svc.cluster.local. udp 55 false 512" NXDOMAIN qr,aa,rd 148 0.000031841s
[INFO] 10.244.0.9:43469 - 21802 "A IN zipkin.istio-system.cluster.local. udp 51 false 512" NXDOMAIN qr,aa,rd 144 0.000036883s
[INFO] 10.244.0.9:43469 - 20054 "A IN zipkin.istio-system. udp 37 false 512" NXDOMAIN qr,rd,ra 37 0.000910085s
[INFO] 10.244.0.11:46248 - 14979 "A IN zipkin.istio-system.istio-system.svc.cluster.local. udp 68 false 512" NXDOMAIN qr,aa,rd 161 0.000114438s
[INFO] 10.244.0.11:46248 - 42183 "A IN zipkin.istio-system.svc.cluster.local. udp 55 false 512" NXDOMAIN qr,aa,rd 148 0.00009091s
[INFO] 10.244.0.11:46248 - 32807 "A IN zipkin.istio-system.cluster.local. udp 51 false 512" NXDOMAIN qr,aa,rd 144 0.000061887s
[INFO] 10.244.0.11:46248 - 63469 "A IN zipkin.istio-system. udp 37 false 512" NXDOMAIN qr,rd,ra 37 0.00099782s
[INFO] 10.244.0.9:51743 - 53887 "A IN zipkin.istio-system.istio-system.svc.cluster.local. udp 68 false 512" NXDOMAIN qr,aa,rd 161 0.00006703s
[INFO] 10.244.0.9:51743 - 35212 "A IN zipkin.istio-system.svc.cluster.local. udp 55 false 512" NXDOMAIN qr,aa,rd 148 0.000043145s
[INFO] 10.244.0.9:51743 - 19660 "A IN zipkin.istio-system.cluster.local. udp 51 false 512" NXDOMAIN qr,aa,rd 144 0.000048189s
[INFO] 10.244.0.9:51743 - 7039 "A IN zipkin.istio-system. udp 37 false 512" NXDOMAIN qr,rd,ra 37 0.000957353s
[INFO] 10.244.0.11:55893 - 16429 "A IN zipkin.istio-system.istio-system.svc.cluster.local. udp 68 false 512" NXDOMAIN qr,aa,rd 161 0.00010254s
[INFO] 10.244.0.11:55893 - 43867 "A IN zipkin.istio-system.svc.cluster.local. udp 55 false 512" NXDOMAIN qr,aa,rd 148 0.000065445s
[INFO] 10.244.0.11:55893 - 52392 "A IN zipkin.istio-system.cluster.local. udp 51 false 512" NXDOMAIN qr,aa,rd 144 0.000055862s
[INFO] 10.244.0.11:55893 - 55402 "A IN zipkin.istio-system. udp 37 false 512" NXDOMAIN qr,rd,ra 37 0.00079605s
[INFO] 10.244.0.9:45898 - 36498 "A IN zipkin.istio-system.istio-system.svc.cluster.local. udp 68 false 512" NXDOMAIN qr,aa,rd 161 0.00006534s
[INFO] 10.244.0.9:45898 - 42152 "A IN zipkin.istio-system.svc.cluster.local. udp 55 false 512" NXDOMAIN qr,aa,rd 148 0.000036178s
[INFO] 10.244.0.9:45898 - 12949 "A IN zipkin.istio-system.cluster.local. udp 51 false 512" NXDOMAIN qr,aa,rd 144 0.000031899s
[INFO] 10.244.0.9:45898 - 23933 "A IN zipkin.istio-system. udp 37 false 512" NXDOMAIN qr,rd,ra 37 0.000931477s
[INFO] 10.244.0.11:34785 - 35789 "A IN zipkin.istio-system.istio-system.svc.cluster.local. udp 68 false 512" NXDOMAIN qr,aa,rd 161 0.0001083s
[INFO] 10.244.0.11:34785 - 28710 "A IN zipkin.istio-system.svc.cluster.local. udp 55 false 512" NXDOMAIN qr,aa,rd 148 0.000056103s
[INFO] 10.244.0.11:34785 - 19810 "A IN zipkin.istio-system.cluster.local. udp 51 false 512" NXDOMAIN qr,aa,rd 144 0.000054279s
[INFO] 10.244.0.11:34785 - 63417 "A IN zipkin.istio-system. udp 37 false 512" NXDOMAIN qr,rd,ra 37 0.000944239s
[INFO] 10.244.0.9:59336 - 34445 "A IN zipkin.istio-system.istio-system.svc.cluster.local. udp 68 false 512" NXDOMAIN qr,aa,rd 161 0.000073915s
[INFO] 10.244.0.9:59336 - 36860 "A IN zipkin.istio-system.svc.cluster.local. udp 55 false 512" NXDOMAIN qr,aa,rd 148 0.000048894s
[INFO] 10.244.0.9:59336 - 30131 "A IN zipkin.istio-system.cluster.local. udp 51 false 512" NXDOMAIN qr,aa,rd 144 0.000030328s
[INFO] 10.244.0.9:59336 - 16097 "A IN zipkin.istio-system. udp 37 false 512" NXDOMAIN qr,rd,ra 37 0.000833286s
[INFO] SIGTERM: Shutting down servers then terminating
[INFO] plugin/health: Going into lameduck mode for 5s


==> describe nodes <==
Name:               minikube
Roles:              control-plane
Labels:             beta.kubernetes.io/arch=amd64
                    beta.kubernetes.io/os=linux
                    kubernetes.io/arch=amd64
                    kubernetes.io/hostname=minikube
                    kubernetes.io/os=linux
                    minikube.k8s.io/commit=5883c09216182566a63dff4c326a6fc9ed2982ff
                    minikube.k8s.io/name=minikube
                    minikube.k8s.io/primary=true
                    minikube.k8s.io/updated_at=2024_07_05T10_23_30_0700
                    minikube.k8s.io/version=v1.33.1
                    node-role.kubernetes.io/control-plane=
                    node.kubernetes.io/exclude-from-external-load-balancers=
Annotations:        kubeadm.alpha.kubernetes.io/cri-socket: unix:///var/run/cri-dockerd.sock
                    node.alpha.kubernetes.io/ttl: 0
                    volumes.kubernetes.io/controller-managed-attach-detach: true
CreationTimestamp:  Fri, 05 Jul 2024 02:23:27 +0000
Taints:             <none>
Unschedulable:      false
Lease:
  HolderIdentity:  minikube
  AcquireTime:     <unset>
  RenewTime:       Mon, 08 Jul 2024 06:52:27 +0000
Conditions:
  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message
  ----             ------  -----------------                 ------------------                ------                       -------
  MemoryPressure   False   Mon, 08 Jul 2024 06:49:11 +0000   Fri, 05 Jul 2024 02:23:25 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available
  DiskPressure     False   Mon, 08 Jul 2024 06:49:11 +0000   Fri, 05 Jul 2024 02:23:25 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure
  PIDPressure      False   Mon, 08 Jul 2024 06:49:11 +0000   Fri, 05 Jul 2024 02:23:25 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available
  Ready            True    Mon, 08 Jul 2024 06:49:11 +0000   Fri, 05 Jul 2024 02:23:28 +0000   KubeletReady                 kubelet is posting ready status
Addresses:
  InternalIP:  192.168.58.2
  Hostname:    minikube
Capacity:
  cpu:                12
  ephemeral-storage:  490616760Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             32710516Ki
  pods:               110
Allocatable:
  cpu:                12
  ephemeral-storage:  490616760Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             32710516Ki
  pods:               110
System Info:
  Machine ID:                 3e04f234589849b0afefb47266bb4010
  System UUID:                9c668f92-a207-43c1-80a7-f94f34217154
  Boot ID:                    cb467a49-0be3-4290-a538-11de2d550326
  Kernel Version:             6.8.0-36-generic
  OS Image:                   Ubuntu 22.04.4 LTS
  Operating System:           linux
  Architecture:               amd64
  Container Runtime Version:  docker://26.1.1
  Kubelet Version:            v1.30.0
  Kube-Proxy Version:         v1.30.0
PodCIDR:                      10.244.0.0/24
PodCIDRs:                     10.244.0.0/24
Non-terminated Pods:          (14 in total)
  Namespace                   Name                                    CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age
  ---------                   ----                                    ------------  ----------  ---------------  -------------  ---
  dev                         blog-web-dev-5db6d8554b-7dn4g           100m (0%!)(MISSING)     100m (0%!)(MISSING)   128M (0%!)(MISSING)        256M (0%!)(MISSING)      3d4h
  dev                         sammy-app-dev-79d5c7f686-drvhg          100m (0%!)(MISSING)     100m (0%!)(MISSING)   128Mi (0%!)(MISSING)       256Mi (0%!)(MISSING)     2d22h
  istio-system                istio-egressgateway-77d7544cf7-jch6b    10m (0%!)(MISSING)      2 (16%!)(MISSING)     40Mi (0%!)(MISSING)        1Gi (3%!)(MISSING)       3d4h
  istio-system                istio-ingressgateway-ff47dcc89-mjj92    10m (0%!)(MISSING)      2 (16%!)(MISSING)     40Mi (0%!)(MISSING)        1Gi (3%!)(MISSING)       3d4h
  istio-system                istiod-7c948c8756-4mcdl                 10m (0%!)(MISSING)      0 (0%!)(MISSING)      100Mi (0%!)(MISSING)       0 (0%!)(MISSING)         3d4h
  kube-system                 coredns-7c445c467-g79kg                 100m (0%!)(MISSING)     0 (0%!)(MISSING)      70Mi (0%!)(MISSING)        170Mi (0%!)(MISSING)     3d4h
  kube-system                 etcd-minikube                           100m (0%!)(MISSING)     0 (0%!)(MISSING)      100Mi (0%!)(MISSING)       0 (0%!)(MISSING)         3d4h
  kube-system                 kube-apiserver-minikube                 250m (2%!)(MISSING)     0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         3d4h
  kube-system                 kube-controller-manager-minikube        200m (1%!)(MISSING)     0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         3d4h
  kube-system                 kube-proxy-r82ck                        0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         3m26s
  kube-system                 kube-scheduler-minikube                 100m (0%!)(MISSING)     0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         3d4h
  kube-system                 storage-provisioner                     0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         3d4h
  prometheus                  prometheus-cdbc74675-qk5x9              0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         3h24m
  traefik                     traefik-deployment-9f49c9d87-5z4kx      0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         3d3h
Allocated resources:
  (Total limits may be over 100 percent, i.e., overcommitted.)
  Resource           Requests        Limits
  --------           --------        ------
  cpu                980m (8%!)(MISSING)       4200m (35%!)(MISSING)
  memory             629219328 (1%!)(MISSING)  2850177024 (8%!)(MISSING)
  ephemeral-storage  0 (0%!)(MISSING)          0 (0%!)(MISSING)
  hugepages-1Gi      0 (0%!)(MISSING)          0 (0%!)(MISSING)
  hugepages-2Mi      0 (0%!)(MISSING)          0 (0%!)(MISSING)
Events:
  Type    Reason    Age    From        Message
  ----    ------    ----   ----        -------
  Normal  Starting  3m24s  kube-proxy


==> dmesg <==
[  +0.000001] RIP: 0010:drm_gem_shmem_vmap+0x1a5/0x1e0
[  +0.000002] Code: 4c 8b 6f 50 4d 85 ed 75 03 4c 8b 2f e8 b4 a1 ec ff 48 c7 c1 b2 64 82 86 4c 89 ea 48 c7 c7 f1 3f 82 86 48 89 c6 e8 0b fe 43 ff <0f> 0b 48 8b 83 f0 00 00 00 4c 89 e6 48 8b 38 e8 67 46 f5 ff b8 fb
[  +0.000002] RSP: 0018:ffffae3145c93a88 EFLAGS: 00010246
[  +0.000001] RAX: 0000000000000000 RBX: ffff8a378bd28600 RCX: 0000000000000000
[  +0.000002] RDX: 0000000000000000 RSI: 0000000000000000 RDI: 0000000000000000
[  +0.000001] RBP: ffffae3145c93aa8 R08: 0000000000000000 R09: 0000000000000000
[  +0.000001] R10: 0000000000000000 R11: 0000000000000000 R12: ffff8a314c91ccc8
[  +0.000000] R13: ffff8a3140307f40 R14: ffff8a314c91ccc8 R15: ffff8a314c91ccc8
[  +0.000001] FS:  0000729a84bbfac0(0000) GS:ffff8a387b980000(0000) knlGS:0000000000000000
[  +0.000002] CS:  0010 DS: 0000 ES: 0000 CR0: 0000000080050033
[  +0.000001] CR2: 0000729a84102b63 CR3: 000000018a3e0004 CR4: 00000000003706f0
[  +0.000001] Call Trace:
[  +0.000001]  <TASK>
[  +0.000004]  ? show_regs+0x6d/0x80
[  +0.000003]  ? __warn+0x89/0x160
[  +0.000003]  ? drm_gem_shmem_vmap+0x1a5/0x1e0
[  +0.000001]  ? report_bug+0x17e/0x1b0
[  +0.000004]  ? handle_bug+0x51/0xa0
[  +0.000002]  ? exc_invalid_op+0x18/0x80
[  +0.000002]  ? asm_exc_invalid_op+0x1b/0x20
[  +0.000004]  ? drm_gem_shmem_vmap+0x1a5/0x1e0
[  +0.000001]  ? drm_gem_shmem_vmap+0x1a5/0x1e0
[  +0.000001]  drm_gem_shmem_object_vmap+0x9/0x20
[  +0.000002]  drm_gem_vmap+0x23/0x80
[  +0.000003]  drm_gem_vmap_unlocked+0x2b/0x50
[  +0.000002]  drm_gem_fb_vmap+0x40/0x150
[  +0.000003]  drm_gem_begin_shadow_fb_access+0x25/0x40
[  +0.000001]  drm_atomic_helper_prepare_planes.part.0+0x13f/0x1e0
[  +0.000003]  drm_atomic_helper_prepare_planes+0x5d/0x70
[  +0.000001]  drm_atomic_helper_commit+0x84/0x160
[  +0.000002]  drm_atomic_commit+0x96/0xd0
[  +0.000003]  ? __pfx___drm_printfn_info+0x10/0x10
[  +0.000002]  drm_atomic_helper_set_config+0x82/0xd0
[  +0.000002]  drm_mode_setcrtc+0x532/0x8b0
[  +0.000003]  ? __pfx_drm_mode_setcrtc+0x10/0x10
[  +0.000002]  drm_ioctl_kernel+0xb9/0x120
[  +0.000002]  drm_ioctl+0x2d4/0x550
[  +0.000001]  ? __pfx_drm_mode_setcrtc+0x10/0x10
[  +0.000002]  __x64_sys_ioctl+0xa0/0xf0
[  +0.000003]  x64_sys_call+0x143b/0x25c0
[  +0.000002]  do_syscall_64+0x7f/0x180
[  +0.000002]  ? do_syscall_64+0x8c/0x180
[  +0.000002]  ? do_syscall_64+0x8c/0x180
[  +0.000002]  ? irqentry_exit+0x43/0x50
[  +0.000003]  entry_SYSCALL_64_after_hwframe+0x78/0x80
[  +0.000001] RIP: 0033:0x729a84f24ded
[  +0.000015] Code: 04 25 28 00 00 00 48 89 45 c8 31 c0 48 8d 45 10 c7 45 b0 10 00 00 00 48 89 45 b8 48 8d 45 d0 48 89 45 c0 b8 10 00 00 00 0f 05 <89> c2 3d 00 f0 ff ff 77 1a 48 8b 45 c8 64 48 2b 04 25 28 00 00 00
[  +0.000001] RSP: 002b:00007ffefa60f1a0 EFLAGS: 00000246 ORIG_RAX: 0000000000000010
[  +0.000002] RAX: ffffffffffffffda RBX: 0000610f3b0c89b0 RCX: 0000729a84f24ded
[  +0.000001] RDX: 00007ffefa60f230 RSI: 00000000c06864a2 RDI: 0000000000000015
[  +0.000001] RBP: 00007ffefa60f1f0 R08: 0000000000000000 R09: 0000610f3b589160
[  +0.000001] R10: 0000000000000000 R11: 0000000000000246 R12: 00007ffefa60f230
[  +0.000001] R13: 00000000c06864a2 R14: 0000000000000015 R15: 0000610f3af6b988
[  +0.000001]  </TASK>
[  +0.000001] ---[ end trace 0000000000000000 ]---
[Jul 8 06:00] xhci_hcd 0000:01:00.2: xHC error in resume, USBSTS 0x401, Reinit
[ +22.002823] [drm:nv_drm_master_set [nvidia_drm]] *ERROR* [nvidia-drm] [GPU ID 0x00000100] Failed to grab modeset ownership
[  +0.000089] [drm:nv_drm_master_set [nvidia_drm]] *ERROR* [nvidia-drm] [GPU ID 0x00000100] Failed to grab modeset ownership
[  +0.000034] [drm:nv_drm_master_set [nvidia_drm]] *ERROR* [nvidia-drm] [GPU ID 0x00000100] Failed to grab modeset ownership
[  +0.000030] [drm:nv_drm_master_set [nvidia_drm]] *ERROR* [nvidia-drm] [GPU ID 0x00000100] Failed to grab modeset ownership


==> etcd [146fa6240acd] <==
{"level":"info","ts":"2024-07-05T06:16:13.990302Z","caller":"embed/etcd.go:127","msg":"configuring peer listeners","listen-peer-urls":["https://192.168.58.2:2380"]}
{"level":"info","ts":"2024-07-05T06:16:13.990317Z","caller":"embed/etcd.go:494","msg":"starting with peer TLS","tls-info":"cert = /var/lib/minikube/certs/etcd/peer.crt, key = /var/lib/minikube/certs/etcd/peer.key, client-cert=, client-key=, trusted-ca = /var/lib/minikube/certs/etcd/ca.crt, client-cert-auth = true, crl-file = ","cipher-suites":[]}
{"level":"info","ts":"2024-07-05T06:16:13.992465Z","caller":"embed/etcd.go:135","msg":"configuring client listeners","listen-client-urls":["https://127.0.0.1:2379","https://192.168.58.2:2379"]}
{"level":"info","ts":"2024-07-05T06:16:13.992714Z","caller":"embed/etcd.go:308","msg":"starting an etcd server","etcd-version":"3.5.12","git-sha":"e7b3bb6cc","go-version":"go1.20.13","go-os":"linux","go-arch":"amd64","max-cpu-set":12,"max-cpu-available":12,"member-initialized":true,"name":"minikube","data-dir":"/var/lib/minikube/etcd","wal-dir":"","wal-dir-dedicated":"","member-dir":"/var/lib/minikube/etcd/member","force-new-cluster":false,"heartbeat-interval":"100ms","election-timeout":"1s","initial-election-tick-advance":true,"snapshot-count":10000,"max-wals":5,"max-snapshots":5,"snapshot-catchup-entries":5000,"initial-advertise-peer-urls":["https://192.168.58.2:2380"],"listen-peer-urls":["https://192.168.58.2:2380"],"advertise-client-urls":["https://192.168.58.2:2379"],"listen-client-urls":["https://127.0.0.1:2379","https://192.168.58.2:2379"],"listen-metrics-urls":["http://127.0.0.1:2381"],"cors":["*"],"host-whitelist":["*"],"initial-cluster":"","initial-cluster-state":"new","initial-cluster-token":"","quota-backend-bytes":2147483648,"max-request-bytes":1572864,"max-concurrent-streams":4294967295,"pre-vote":true,"initial-corrupt-check":true,"corrupt-check-time-interval":"0s","compact-check-time-enabled":false,"compact-check-time-interval":"1m0s","auto-compaction-mode":"periodic","auto-compaction-retention":"0s","auto-compaction-interval":"0s","discovery-url":"","discovery-proxy":"","downgrade-check-interval":"5s"}
{"level":"info","ts":"2024-07-05T06:16:14.002513Z","caller":"etcdserver/backend.go:81","msg":"opened backend db","path":"/var/lib/minikube/etcd/member/snap/db","took":"9.513672ms"}
{"level":"info","ts":"2024-07-05T06:16:14.04052Z","caller":"etcdserver/server.go:511","msg":"recovered v2 store from snapshot","snapshot-index":10001,"snapshot-size":"7.1 kB"}
{"level":"info","ts":"2024-07-05T06:16:14.040564Z","caller":"etcdserver/server.go:524","msg":"recovered v3 backend from snapshot","backend-size-bytes":4599808,"backend-size":"4.6 MB","backend-size-in-use-bytes":2121728,"backend-size-in-use":"2.1 MB"}
{"level":"info","ts":"2024-07-05T06:16:14.075162Z","caller":"etcdserver/raft.go:530","msg":"restarting local member","cluster-id":"3a56e4ca95e2355c","local-member-id":"b2c6679ac05f2cf1","commit-index":17955}
{"level":"info","ts":"2024-07-05T06:16:14.075682Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"b2c6679ac05f2cf1 switched to configuration voters=(12882097698489969905)"}
{"level":"info","ts":"2024-07-05T06:16:14.075702Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"b2c6679ac05f2cf1 became follower at term 2"}
{"level":"info","ts":"2024-07-05T06:16:14.07571Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"newRaft b2c6679ac05f2cf1 [peers: [b2c6679ac05f2cf1], term: 2, commit: 17955, applied: 10001, lastindex: 17955, lastterm: 2]"}
{"level":"info","ts":"2024-07-05T06:16:14.075782Z","caller":"api/capability.go:75","msg":"enabled capabilities for version","cluster-version":"3.5"}
{"level":"info","ts":"2024-07-05T06:16:14.075927Z","caller":"membership/cluster.go:278","msg":"recovered/added member from store","cluster-id":"3a56e4ca95e2355c","local-member-id":"b2c6679ac05f2cf1","recovered-remote-peer-id":"b2c6679ac05f2cf1","recovered-remote-peer-urls":["https://192.168.58.2:2380"]}
{"level":"info","ts":"2024-07-05T06:16:14.075936Z","caller":"membership/cluster.go:287","msg":"set cluster version from store","cluster-version":"3.5"}
{"level":"warn","ts":"2024-07-05T06:16:14.083065Z","caller":"auth/store.go:1241","msg":"simple token is not cryptographically signed"}
{"level":"info","ts":"2024-07-05T06:16:14.084979Z","caller":"mvcc/kvstore.go:341","msg":"restored last compact revision","meta-bucket-name":"meta","meta-bucket-name-key":"finishedCompactRev","restored-compact-revision":15199}
{"level":"info","ts":"2024-07-05T06:16:14.08649Z","caller":"mvcc/kvstore.go:407","msg":"kvstore restored","current-rev":15613}
{"level":"info","ts":"2024-07-05T06:16:14.089389Z","caller":"etcdserver/quota.go:94","msg":"enabled backend quota with default value","quota-name":"v3-applier","quota-size-bytes":2147483648,"quota-size":"2.1 GB"}
{"level":"info","ts":"2024-07-05T06:16:14.092839Z","caller":"etcdserver/corrupt.go:96","msg":"starting initial corruption check","local-member-id":"b2c6679ac05f2cf1","timeout":"7s"}
{"level":"info","ts":"2024-07-05T06:16:14.093251Z","caller":"etcdserver/corrupt.go:177","msg":"initial corruption checking passed; no corruption","local-member-id":"b2c6679ac05f2cf1"}
{"level":"info","ts":"2024-07-05T06:16:14.093284Z","caller":"etcdserver/server.go:851","msg":"starting etcd server","local-member-id":"b2c6679ac05f2cf1","local-server-version":"3.5.12","cluster-id":"3a56e4ca95e2355c","cluster-version":"3.5"}
{"level":"info","ts":"2024-07-05T06:16:14.093334Z","caller":"etcdserver/server.go:744","msg":"started as single-node; fast-forwarding election ticks","local-member-id":"b2c6679ac05f2cf1","forward-ticks":9,"forward-duration":"900ms","election-ticks":10,"election-timeout":"1s"}
{"level":"info","ts":"2024-07-05T06:16:14.093377Z","caller":"fileutil/purge.go:50","msg":"started to purge file","dir":"/var/lib/minikube/etcd/member/snap","suffix":"snap.db","max":5,"interval":"30s"}
{"level":"info","ts":"2024-07-05T06:16:14.093432Z","caller":"fileutil/purge.go:50","msg":"started to purge file","dir":"/var/lib/minikube/etcd/member/snap","suffix":"snap","max":5,"interval":"30s"}
{"level":"info","ts":"2024-07-05T06:16:14.093455Z","caller":"fileutil/purge.go:50","msg":"started to purge file","dir":"/var/lib/minikube/etcd/member/wal","suffix":"wal","max":5,"interval":"30s"}
{"level":"info","ts":"2024-07-05T06:16:14.0954Z","caller":"embed/etcd.go:726","msg":"starting with client TLS","tls-info":"cert = /var/lib/minikube/certs/etcd/server.crt, key = /var/lib/minikube/certs/etcd/server.key, client-cert=, client-key=, trusted-ca = /var/lib/minikube/certs/etcd/ca.crt, client-cert-auth = true, crl-file = ","cipher-suites":[]}
{"level":"info","ts":"2024-07-05T06:16:14.095468Z","caller":"embed/etcd.go:597","msg":"serving peer traffic","address":"192.168.58.2:2380"}
{"level":"info","ts":"2024-07-05T06:16:14.095499Z","caller":"embed/etcd.go:569","msg":"cmux::serve","address":"192.168.58.2:2380"}
{"level":"info","ts":"2024-07-05T06:16:14.09555Z","caller":"embed/etcd.go:277","msg":"now serving peer/client/metrics","local-member-id":"b2c6679ac05f2cf1","initial-advertise-peer-urls":["https://192.168.58.2:2380"],"listen-peer-urls":["https://192.168.58.2:2380"],"advertise-client-urls":["https://192.168.58.2:2379"],"listen-client-urls":["https://127.0.0.1:2379","https://192.168.58.2:2379"],"listen-metrics-urls":["http://127.0.0.1:2381"]}
{"level":"info","ts":"2024-07-05T06:16:14.095573Z","caller":"embed/etcd.go:857","msg":"serving metrics","address":"http://127.0.0.1:2381"}
{"level":"info","ts":"2024-07-05T06:16:15.076992Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"b2c6679ac05f2cf1 is starting a new election at term 2"}
{"level":"info","ts":"2024-07-05T06:16:15.07715Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"b2c6679ac05f2cf1 became pre-candidate at term 2"}
{"level":"info","ts":"2024-07-05T06:16:15.077235Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"b2c6679ac05f2cf1 received MsgPreVoteResp from b2c6679ac05f2cf1 at term 2"}
{"level":"info","ts":"2024-07-05T06:16:15.077267Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"b2c6679ac05f2cf1 became candidate at term 3"}
{"level":"info","ts":"2024-07-05T06:16:15.077288Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"b2c6679ac05f2cf1 received MsgVoteResp from b2c6679ac05f2cf1 at term 3"}
{"level":"info","ts":"2024-07-05T06:16:15.077315Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"b2c6679ac05f2cf1 became leader at term 3"}
{"level":"info","ts":"2024-07-05T06:16:15.077339Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"raft.node: b2c6679ac05f2cf1 elected leader b2c6679ac05f2cf1 at term 3"}
{"level":"info","ts":"2024-07-05T06:16:15.086811Z","caller":"etcdserver/server.go:2068","msg":"published local member to cluster through raft","local-member-id":"b2c6679ac05f2cf1","local-member-attributes":"{Name:minikube ClientURLs:[https://192.168.58.2:2379]}","request-path":"/0/members/b2c6679ac05f2cf1/attributes","cluster-id":"3a56e4ca95e2355c","publish-timeout":"7s"}
{"level":"info","ts":"2024-07-05T06:16:15.086815Z","caller":"embed/serve.go:103","msg":"ready to serve client requests"}
{"level":"info","ts":"2024-07-05T06:16:15.086846Z","caller":"embed/serve.go:103","msg":"ready to serve client requests"}
{"level":"info","ts":"2024-07-05T06:16:15.087403Z","caller":"etcdmain/main.go:44","msg":"notifying init daemon"}
{"level":"info","ts":"2024-07-05T06:16:15.087482Z","caller":"etcdmain/main.go:50","msg":"successfully notified init daemon"}
{"level":"info","ts":"2024-07-05T06:16:15.092586Z","caller":"embed/serve.go:250","msg":"serving client traffic securely","traffic":"grpc+http","address":"192.168.58.2:2379"}
{"level":"info","ts":"2024-07-05T06:16:15.09264Z","caller":"embed/serve.go:250","msg":"serving client traffic securely","traffic":"grpc+http","address":"127.0.0.1:2379"}
{"level":"info","ts":"2024-07-05T06:26:15.122096Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":16187}
{"level":"info","ts":"2024-07-05T06:26:15.142701Z","caller":"mvcc/kvstore_compaction.go:68","msg":"finished scheduled compaction","compact-revision":16187,"took":"20.432655ms","hash":921192264,"current-db-size-bytes":4599808,"current-db-size":"4.6 MB","current-db-size-in-use-bytes":3047424,"current-db-size-in-use":"3.0 MB"}
{"level":"info","ts":"2024-07-05T06:26:15.14273Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":921192264,"revision":16187,"compact-revision":15199}
{"level":"info","ts":"2024-07-05T06:31:15.129093Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":16586}
{"level":"info","ts":"2024-07-05T06:31:15.132968Z","caller":"mvcc/kvstore_compaction.go:68","msg":"finished scheduled compaction","compact-revision":16586,"took":"3.685398ms","hash":1847463180,"current-db-size-bytes":4599808,"current-db-size":"4.6 MB","current-db-size-in-use-bytes":3047424,"current-db-size-in-use":"3.0 MB"}
{"level":"info","ts":"2024-07-05T06:31:15.132993Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":1847463180,"revision":16586,"compact-revision":16187}
{"level":"info","ts":"2024-07-05T06:32:57.83214Z","caller":"osutil/interrupt_unix.go:64","msg":"received signal; shutting down","signal":"terminated"}
{"level":"info","ts":"2024-07-05T06:32:57.832187Z","caller":"embed/etcd.go:375","msg":"closing etcd server","name":"minikube","data-dir":"/var/lib/minikube/etcd","advertise-peer-urls":["https://192.168.58.2:2380"],"advertise-client-urls":["https://192.168.58.2:2379"]}
{"level":"warn","ts":"2024-07-05T06:32:57.832261Z","caller":"embed/serve.go:212","msg":"stopping secure grpc server due to error","error":"accept tcp 127.0.0.1:2379: use of closed network connection"}
{"level":"warn","ts":"2024-07-05T06:32:57.832362Z","caller":"embed/serve.go:214","msg":"stopped secure grpc server due to error","error":"accept tcp 127.0.0.1:2379: use of closed network connection"}
{"level":"warn","ts":"2024-07-05T06:32:57.849706Z","caller":"embed/serve.go:212","msg":"stopping secure grpc server due to error","error":"accept tcp 192.168.58.2:2379: use of closed network connection"}
{"level":"warn","ts":"2024-07-05T06:32:57.849763Z","caller":"embed/serve.go:214","msg":"stopped secure grpc server due to error","error":"accept tcp 192.168.58.2:2379: use of closed network connection"}
{"level":"info","ts":"2024-07-05T06:32:57.851156Z","caller":"etcdserver/server.go:1471","msg":"skipped leadership transfer for single voting member cluster","local-member-id":"b2c6679ac05f2cf1","current-leader-member-id":"b2c6679ac05f2cf1"}
{"level":"info","ts":"2024-07-05T06:32:57.869137Z","caller":"embed/etcd.go:579","msg":"stopping serving peer traffic","address":"192.168.58.2:2380"}
{"level":"info","ts":"2024-07-05T06:32:57.869215Z","caller":"embed/etcd.go:584","msg":"stopped serving peer traffic","address":"192.168.58.2:2380"}
{"level":"info","ts":"2024-07-05T06:32:57.869231Z","caller":"embed/etcd.go:377","msg":"closed etcd server","name":"minikube","data-dir":"/var/lib/minikube/etcd","advertise-peer-urls":["https://192.168.58.2:2380"],"advertise-client-urls":["https://192.168.58.2:2379"]}


==> etcd [cf4d19cdbdb9] <==
{"level":"info","ts":"2024-07-08T04:17:52.147046Z","caller":"mvcc/kvstore_compaction.go:68","msg":"finished scheduled compaction","compact-revision":53450,"took":"5.391794ms","hash":2476356531,"current-db-size-bytes":11251712,"current-db-size":"11 MB","current-db-size-in-use-bytes":3031040,"current-db-size-in-use":"3.0 MB"}
{"level":"info","ts":"2024-07-08T04:17:52.147071Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":2476356531,"revision":53450,"compact-revision":53050}
{"level":"info","ts":"2024-07-08T04:22:52.160476Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":53854}
{"level":"info","ts":"2024-07-08T04:22:52.166632Z","caller":"mvcc/kvstore_compaction.go:68","msg":"finished scheduled compaction","compact-revision":53854,"took":"6.000153ms","hash":2652447890,"current-db-size-bytes":11251712,"current-db-size":"11 MB","current-db-size-in-use-bytes":3162112,"current-db-size-in-use":"3.2 MB"}
{"level":"info","ts":"2024-07-08T04:22:52.166649Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":2652447890,"revision":53854,"compact-revision":53450}
{"level":"info","ts":"2024-07-08T04:27:52.179587Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":54254}
{"level":"info","ts":"2024-07-08T04:27:52.185741Z","caller":"mvcc/kvstore_compaction.go:68","msg":"finished scheduled compaction","compact-revision":54254,"took":"5.981709ms","hash":3425957767,"current-db-size-bytes":11251712,"current-db-size":"11 MB","current-db-size-in-use-bytes":3031040,"current-db-size-in-use":"3.0 MB"}
{"level":"info","ts":"2024-07-08T04:27:52.185759Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":3425957767,"revision":54254,"compact-revision":53854}
{"level":"info","ts":"2024-07-08T04:32:52.199254Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":54650}
{"level":"info","ts":"2024-07-08T04:32:52.205546Z","caller":"mvcc/kvstore_compaction.go:68","msg":"finished scheduled compaction","compact-revision":54650,"took":"6.123327ms","hash":1485849819,"current-db-size-bytes":11251712,"current-db-size":"11 MB","current-db-size-in-use-bytes":3022848,"current-db-size-in-use":"3.0 MB"}
{"level":"info","ts":"2024-07-08T04:32:52.205569Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":1485849819,"revision":54650,"compact-revision":54254}
{"level":"info","ts":"2024-07-08T04:37:52.210429Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":55051}
{"level":"info","ts":"2024-07-08T04:37:52.216209Z","caller":"mvcc/kvstore_compaction.go:68","msg":"finished scheduled compaction","compact-revision":55051,"took":"5.622454ms","hash":1467162490,"current-db-size-bytes":11251712,"current-db-size":"11 MB","current-db-size-in-use-bytes":3018752,"current-db-size-in-use":"3.0 MB"}
{"level":"info","ts":"2024-07-08T04:37:52.216229Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":1467162490,"revision":55051,"compact-revision":54650}
{"level":"info","ts":"2024-07-08T04:42:52.229817Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":55452}
{"level":"info","ts":"2024-07-08T04:42:52.23603Z","caller":"mvcc/kvstore_compaction.go:68","msg":"finished scheduled compaction","compact-revision":55452,"took":"6.046837ms","hash":1096998283,"current-db-size-bytes":11251712,"current-db-size":"11 MB","current-db-size-in-use-bytes":3026944,"current-db-size-in-use":"3.0 MB"}
{"level":"info","ts":"2024-07-08T04:42:52.236046Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":1096998283,"revision":55452,"compact-revision":55051}
{"level":"info","ts":"2024-07-08T04:47:52.249573Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":55850}
{"level":"info","ts":"2024-07-08T04:47:52.255939Z","caller":"mvcc/kvstore_compaction.go:68","msg":"finished scheduled compaction","compact-revision":55850,"took":"6.199623ms","hash":3377548507,"current-db-size-bytes":11251712,"current-db-size":"11 MB","current-db-size-in-use-bytes":2998272,"current-db-size-in-use":"3.0 MB"}
{"level":"info","ts":"2024-07-08T04:47:52.255956Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":3377548507,"revision":55850,"compact-revision":55452}
{"level":"info","ts":"2024-07-08T04:52:52.269026Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":56250}
{"level":"info","ts":"2024-07-08T04:52:52.275131Z","caller":"mvcc/kvstore_compaction.go:68","msg":"finished scheduled compaction","compact-revision":56250,"took":"5.955525ms","hash":1085891901,"current-db-size-bytes":11251712,"current-db-size":"11 MB","current-db-size-in-use-bytes":2994176,"current-db-size-in-use":"3.0 MB"}
{"level":"info","ts":"2024-07-08T04:52:52.275151Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":1085891901,"revision":56250,"compact-revision":55850}
{"level":"info","ts":"2024-07-08T06:01:05.183858Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":56650}
{"level":"info","ts":"2024-07-08T06:01:05.187513Z","caller":"mvcc/kvstore_compaction.go:68","msg":"finished scheduled compaction","compact-revision":56650,"took":"3.495648ms","hash":3829138436,"current-db-size-bytes":11251712,"current-db-size":"11 MB","current-db-size-in-use-bytes":3006464,"current-db-size-in-use":"3.0 MB"}
{"level":"info","ts":"2024-07-08T06:01:05.187538Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":3829138436,"revision":56650,"compact-revision":56250}
{"level":"info","ts":"2024-07-08T06:06:05.198286Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":57050}
{"level":"info","ts":"2024-07-08T06:06:05.201849Z","caller":"mvcc/kvstore_compaction.go:68","msg":"finished scheduled compaction","compact-revision":57050,"took":"3.415257ms","hash":4202372263,"current-db-size-bytes":11251712,"current-db-size":"11 MB","current-db-size-in-use-bytes":3002368,"current-db-size-in-use":"3.0 MB"}
{"level":"info","ts":"2024-07-08T06:06:05.201865Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":4202372263,"revision":57050,"compact-revision":56650}
{"level":"info","ts":"2024-07-08T06:11:05.208047Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":57450}
{"level":"info","ts":"2024-07-08T06:11:05.211674Z","caller":"mvcc/kvstore_compaction.go:68","msg":"finished scheduled compaction","compact-revision":57450,"took":"3.475562ms","hash":731163589,"current-db-size-bytes":11251712,"current-db-size":"11 MB","current-db-size-in-use-bytes":3002368,"current-db-size-in-use":"3.0 MB"}
{"level":"info","ts":"2024-07-08T06:11:05.211694Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":731163589,"revision":57450,"compact-revision":57050}
{"level":"info","ts":"2024-07-08T06:16:05.220403Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":57850}
{"level":"info","ts":"2024-07-08T06:16:05.22821Z","caller":"mvcc/kvstore_compaction.go:68","msg":"finished scheduled compaction","compact-revision":57850,"took":"7.646755ms","hash":787974333,"current-db-size-bytes":11251712,"current-db-size":"11 MB","current-db-size-in-use-bytes":2998272,"current-db-size-in-use":"3.0 MB"}
{"level":"info","ts":"2024-07-08T06:16:05.228237Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":787974333,"revision":57850,"compact-revision":57450}
{"level":"info","ts":"2024-07-08T06:21:05.238724Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":58251}
{"level":"info","ts":"2024-07-08T06:21:05.242401Z","caller":"mvcc/kvstore_compaction.go:68","msg":"finished scheduled compaction","compact-revision":58251,"took":"3.536344ms","hash":426746858,"current-db-size-bytes":11251712,"current-db-size":"11 MB","current-db-size-in-use-bytes":2969600,"current-db-size-in-use":"3.0 MB"}
{"level":"info","ts":"2024-07-08T06:21:05.24242Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":426746858,"revision":58251,"compact-revision":57850}
{"level":"info","ts":"2024-07-08T06:26:05.245314Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":58650}
{"level":"info","ts":"2024-07-08T06:26:05.248974Z","caller":"mvcc/kvstore_compaction.go:68","msg":"finished scheduled compaction","compact-revision":58650,"took":"3.499163ms","hash":1518422767,"current-db-size-bytes":11251712,"current-db-size":"11 MB","current-db-size-in-use-bytes":2994176,"current-db-size-in-use":"3.0 MB"}
{"level":"info","ts":"2024-07-08T06:26:05.248991Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":1518422767,"revision":58650,"compact-revision":58251}
{"level":"info","ts":"2024-07-08T06:31:05.259857Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":59050}
{"level":"info","ts":"2024-07-08T06:31:05.26354Z","caller":"mvcc/kvstore_compaction.go:68","msg":"finished scheduled compaction","compact-revision":59050,"took":"3.535615ms","hash":690304945,"current-db-size-bytes":11251712,"current-db-size":"11 MB","current-db-size-in-use-bytes":2998272,"current-db-size-in-use":"3.0 MB"}
{"level":"info","ts":"2024-07-08T06:31:05.263558Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":690304945,"revision":59050,"compact-revision":58650}
{"level":"info","ts":"2024-07-08T06:36:05.267227Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":59450}
{"level":"info","ts":"2024-07-08T06:36:05.27096Z","caller":"mvcc/kvstore_compaction.go:68","msg":"finished scheduled compaction","compact-revision":59450,"took":"3.604033ms","hash":28908615,"current-db-size-bytes":11251712,"current-db-size":"11 MB","current-db-size-in-use-bytes":3006464,"current-db-size-in-use":"3.0 MB"}
{"level":"info","ts":"2024-07-08T06:36:05.270981Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":28908615,"revision":59450,"compact-revision":59050}
{"level":"info","ts":"2024-07-08T06:41:05.281819Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":59850}
{"level":"info","ts":"2024-07-08T06:41:05.285505Z","caller":"mvcc/kvstore_compaction.go:68","msg":"finished scheduled compaction","compact-revision":59850,"took":"3.518801ms","hash":3596555683,"current-db-size-bytes":11251712,"current-db-size":"11 MB","current-db-size-in-use-bytes":3022848,"current-db-size-in-use":"3.0 MB"}
{"level":"info","ts":"2024-07-08T06:41:05.285522Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":3596555683,"revision":59850,"compact-revision":59450}
{"level":"info","ts":"2024-07-08T06:46:05.296715Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":60250}
{"level":"info","ts":"2024-07-08T06:46:05.300713Z","caller":"mvcc/kvstore_compaction.go:68","msg":"finished scheduled compaction","compact-revision":60250,"took":"3.82068ms","hash":34994440,"current-db-size-bytes":11251712,"current-db-size":"11 MB","current-db-size-in-use-bytes":3002368,"current-db-size-in-use":"3.0 MB"}
{"level":"info","ts":"2024-07-08T06:46:05.300744Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":34994440,"revision":60250,"compact-revision":59850}
{"level":"info","ts":"2024-07-08T06:51:05.307996Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":60650}
{"level":"info","ts":"2024-07-08T06:51:05.314145Z","caller":"mvcc/kvstore_compaction.go:68","msg":"finished scheduled compaction","compact-revision":60650,"took":"5.993942ms","hash":3323955756,"current-db-size-bytes":11251712,"current-db-size":"11 MB","current-db-size-in-use-bytes":3059712,"current-db-size-in-use":"3.1 MB"}
{"level":"info","ts":"2024-07-08T06:51:05.314171Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":3323955756,"revision":60650,"compact-revision":60250}
{"level":"info","ts":"2024-07-08T06:51:28.184126Z","caller":"etcdserver/server.go:1401","msg":"triggering snapshot","local-member-id":"b2c6679ac05f2cf1","local-member-applied-index":70008,"local-member-snapshot-index":60007,"local-member-snapshot-count":10000}
{"level":"info","ts":"2024-07-08T06:51:28.199807Z","caller":"etcdserver/server.go:2420","msg":"saved snapshot","snapshot-index":70008}
{"level":"info","ts":"2024-07-08T06:51:28.199871Z","caller":"etcdserver/server.go:2450","msg":"compacted Raft logs","compact-index":65008}
{"level":"info","ts":"2024-07-08T06:51:33.509413Z","caller":"fileutil/purge.go:96","msg":"purged","path":"/var/lib/minikube/etcd/member/snap/0000000000000004-0000000000004e22.snap"}


==> kernel <==
 06:52:34 up 3 days, 38 min,  0 users,  load average: 0.43, 0.64, 0.67
Linux minikube 6.8.0-36-generic #36-Ubuntu SMP PREEMPT_DYNAMIC Mon Jun 10 10:49:14 UTC 2024 x86_64 x86_64 x86_64 GNU/Linux
PRETTY_NAME="Ubuntu 22.04.4 LTS"


==> kube-apiserver [88f69a23f2de] <==
I0705 10:03:53.405343       1 alloc.go:330] "allocated clusterIPs" service="cert-manager/cert-manager-webhook" clusterIPs={"IPv4":"10.109.200.63"}
W0705 10:03:53.453784       1 dispatcher.go:210] Failed calling webhook, failing open mpod.kb.io: failed calling webhook "mpod.kb.io": failed to call webhook: Post "https://opentelemetry-operator-webhook-service.opentelemetry-operator-system.svc:443/mutate-v1-pod?timeout=10s": dial tcp 10.106.65.252:443: connect: connection refused
E0705 10:03:53.453812       1 dispatcher.go:214] failed calling webhook "mpod.kb.io": failed to call webhook: Post "https://opentelemetry-operator-webhook-service.opentelemetry-operator-system.svc:443/mutate-v1-pod?timeout=10s": dial tcp 10.106.65.252:443: connect: connection refused
W0705 10:03:53.465377       1 dispatcher.go:210] Failed calling webhook, failing open mpod.kb.io: failed calling webhook "mpod.kb.io": failed to call webhook: Post "https://opentelemetry-operator-webhook-service.opentelemetry-operator-system.svc:443/mutate-v1-pod?timeout=10s": dial tcp 10.106.65.252:443: connect: connection refused
E0705 10:03:53.465398       1 dispatcher.go:214] failed calling webhook "mpod.kb.io": failed to call webhook: Post "https://opentelemetry-operator-webhook-service.opentelemetry-operator-system.svc:443/mutate-v1-pod?timeout=10s": dial tcp 10.106.65.252:443: connect: connection refused
W0705 10:03:53.496148       1 dispatcher.go:210] Failed calling webhook, failing open mpod.kb.io: failed calling webhook "mpod.kb.io": failed to call webhook: Post "https://opentelemetry-operator-webhook-service.opentelemetry-operator-system.svc:443/mutate-v1-pod?timeout=10s": dial tcp 10.106.65.252:443: connect: connection refused
E0705 10:03:53.496202       1 dispatcher.go:214] failed calling webhook "mpod.kb.io": failed to call webhook: Post "https://opentelemetry-operator-webhook-service.opentelemetry-operator-system.svc:443/mutate-v1-pod?timeout=10s": dial tcp 10.106.65.252:443: connect: connection refused
I0705 10:09:50.423076       1 handler.go:286] Adding GroupVersion opentelemetry.io v1alpha1 to ResourceManager
I0705 10:09:50.470525       1 handler.go:286] Adding GroupVersion opentelemetry.io v1alpha1 to ResourceManager
I0705 10:09:50.489936       1 handler.go:286] Adding GroupVersion opentelemetry.io v1alpha1 to ResourceManager
I0705 10:09:50.536674       1 handler.go:286] Adding GroupVersion opentelemetry.io v1alpha1 to ResourceManager
I0705 10:09:50.595799       1 handler.go:286] Adding GroupVersion opentelemetry.io v1alpha1 to ResourceManager
I0705 10:09:50.595836       1 handler.go:286] Adding GroupVersion opentelemetry.io v1beta1 to ResourceManager
W0705 10:09:51.490880       1 cacher.go:168] Terminating all watchers from cacher instrumentations.opentelemetry.io
W0705 10:09:51.536848       1 cacher.go:168] Terminating all watchers from cacher opampbridges.opentelemetry.io
W0705 10:09:51.891275       1 cacher.go:168] Terminating all watchers from cacher opentelemetrycollectors.opentelemetry.io
I0705 10:09:56.791138       1 handler.go:286] Adding GroupVersion opentelemetry.io v1alpha1 to ResourceManager
I0705 10:09:56.850150       1 handler.go:286] Adding GroupVersion opentelemetry.io v1alpha1 to ResourceManager
I0705 10:09:57.010183       1 alloc.go:330] "allocated clusterIPs" service="opentelemetry-operator-system/opentelemetry-operator-controller-manager-metrics-service" clusterIPs={"IPv4":"10.110.252.236"}
I0705 10:09:57.057682       1 alloc.go:330] "allocated clusterIPs" service="opentelemetry-operator-system/opentelemetry-operator-webhook-service" clusterIPs={"IPv4":"10.105.109.174"}
I0705 10:09:57.098816       1 controller.go:615] quota admission added evaluator for: certificates.cert-manager.io
I0705 10:09:57.140749       1 controller.go:615] quota admission added evaluator for: issuers.cert-manager.io
I0705 10:09:57.155142       1 handler.go:286] Adding GroupVersion opentelemetry.io v1alpha1 to ResourceManager
I0705 10:09:57.155178       1 handler.go:286] Adding GroupVersion opentelemetry.io v1beta1 to ResourceManager
I0705 10:09:57.327575       1 controller.go:615] quota admission added evaluator for: certificaterequests.cert-manager.io
E0708 01:34:39.463599       1 authentication.go:73] "Unable to authenticate the request" err="[invalid bearer token, service account token has expired]"
E0708 01:34:39.765235       1 authentication.go:73] "Unable to authenticate the request" err="[invalid bearer token, service account token has expired]"
I0708 03:02:34.831708       1 handler.go:286] Adding GroupVersion opentelemetry.io v1alpha1 to ResourceManager
I0708 03:02:34.865465       1 handler.go:286] Adding GroupVersion opentelemetry.io v1alpha1 to ResourceManager
I0708 03:02:34.888626       1 handler.go:286] Adding GroupVersion opentelemetry.io v1alpha1 to ResourceManager
I0708 03:02:34.949952       1 handler.go:286] Adding GroupVersion opentelemetry.io v1alpha1 to ResourceManager
I0708 03:02:34.986503       1 handler.go:286] Adding GroupVersion opentelemetry.io v1alpha1 to ResourceManager
I0708 03:02:34.986545       1 handler.go:286] Adding GroupVersion opentelemetry.io v1beta1 to ResourceManager
W0708 03:02:35.889424       1 cacher.go:168] Terminating all watchers from cacher instrumentations.opentelemetry.io
W0708 03:02:35.950706       1 cacher.go:168] Terminating all watchers from cacher opampbridges.opentelemetry.io
W0708 03:02:36.354759       1 cacher.go:168] Terminating all watchers from cacher opentelemetrycollectors.opentelemetry.io
I0708 03:02:57.746875       1 handler.go:286] Adding GroupVersion cert-manager.io v1 to ResourceManager
I0708 03:02:57.770533       1 handler.go:286] Adding GroupVersion cert-manager.io v1 to ResourceManager
I0708 03:02:57.773599       1 handler.go:286] Adding GroupVersion cert-manager.io v1 to ResourceManager
I0708 03:02:57.819718       1 handler.go:286] Adding GroupVersion acme.cert-manager.io v1 to ResourceManager
I0708 03:02:57.820249       1 handler.go:286] Adding GroupVersion cert-manager.io v1 to ResourceManager
I0708 03:02:57.865300       1 handler.go:286] Adding GroupVersion cert-manager.io v1 to ResourceManager
I0708 03:02:57.877658       1 handler.go:286] Adding GroupVersion acme.cert-manager.io v1 to ResourceManager
I0708 03:02:57.928920       1 handler.go:286] Adding GroupVersion cert-manager.io v1 to ResourceManager
I0708 03:02:57.937930       1 handler.go:286] Adding GroupVersion acme.cert-manager.io v1 to ResourceManager
I0708 03:02:57.961990       1 handler.go:286] Adding GroupVersion cert-manager.io v1 to ResourceManager
W0708 03:02:58.774077       1 cacher.go:168] Terminating all watchers from cacher certificaterequests.cert-manager.io
W0708 03:02:58.823477       1 cacher.go:168] Terminating all watchers from cacher certificates.cert-manager.io
W0708 03:02:58.878567       1 cacher.go:168] Terminating all watchers from cacher challenges.acme.cert-manager.io
W0708 03:02:58.962289       1 cacher.go:168] Terminating all watchers from cacher clusterissuers.cert-manager.io
W0708 03:02:58.978395       1 cacher.go:168] Terminating all watchers from cacher orders.acme.cert-manager.io
W0708 03:02:59.025980       1 cacher.go:168] Terminating all watchers from cacher issuers.cert-manager.io
I0708 03:22:18.786958       1 alloc.go:330] "allocated clusterIPs" service="prometheus/prometheus" clusterIPs={"IPv4":"10.96.140.131"}
E0708 03:59:29.199365       1 conn.go:339] Error on socket receive: read tcp 192.168.58.2:8443->192.168.58.1:48180: use of closed network connection
E0708 04:08:39.048506       1 conn.go:339] Error on socket receive: read tcp 192.168.58.2:8443->192.168.58.1:45194: use of closed network connection
E0708 04:09:40.884859       1 conn.go:339] Error on socket receive: read tcp 192.168.58.2:8443->192.168.58.1:53458: use of closed network connection
E0708 06:00:19.995980       1 authentication.go:73] "Unable to authenticate the request" err="[invalid bearer token, service account token has expired]"
E0708 06:00:20.275160       1 authentication.go:73] "Unable to authenticate the request" err="[invalid bearer token, service account token has expired]"
E0708 06:10:42.557780       1 conn.go:339] Error on socket receive: read tcp 192.168.58.2:8443->192.168.58.1:50886: use of closed network connection
E0708 06:50:29.659621       1 conn.go:339] Error on socket receive: read tcp 192.168.58.2:8443->192.168.58.1:50232: use of closed network connection


==> kube-apiserver [bed85e409242] <==
W0705 06:32:58.837362       1 logging.go:59] [core] [Channel #277 SubChannel #278] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0705 06:32:58.837362       1 logging.go:59] [core] [Channel #163 SubChannel #164] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0705 06:32:58.837374       1 logging.go:59] [core] [Channel #286 SubChannel #287] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0705 06:32:58.837405       1 logging.go:59] [core] [Channel #46 SubChannel #47] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0705 06:32:58.837433       1 logging.go:59] [core] [Channel #208 SubChannel #209] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0705 06:32:58.837464       1 logging.go:59] [core] [Channel #178 SubChannel #179] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0705 06:32:58.837483       1 logging.go:59] [core] [Channel #136 SubChannel #137] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0705 06:32:58.837490       1 logging.go:59] [core] [Channel #262 SubChannel #263] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0705 06:32:58.837509       1 logging.go:59] [core] [Channel #220 SubChannel #221] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0705 06:32:58.837511       1 logging.go:59] [core] [Channel #15 SubChannel #16] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0705 06:32:58.837541       1 logging.go:59] [core] [Channel #259 SubChannel #260] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0705 06:32:58.837550       1 logging.go:59] [core] [Channel #37 SubChannel #38] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0705 06:32:58.837583       1 logging.go:59] [core] [Channel #94 SubChannel #95] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0705 06:32:58.837616       1 logging.go:59] [core] [Channel #82 SubChannel #83] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0705 06:32:58.837624       1 logging.go:59] [core] [Channel #253 SubChannel #254] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0705 06:32:58.837682       1 logging.go:59] [core] [Channel #190 SubChannel #191] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0705 06:32:58.837594       1 logging.go:59] [core] [Channel #187 SubChannel #188] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0705 06:32:58.837651       1 logging.go:59] [core] [Channel #106 SubChannel #107] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0705 06:32:58.837718       1 logging.go:59] [core] [Channel #280 SubChannel #281] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0705 06:32:58.837652       1 logging.go:59] [core] [Channel #271 SubChannel #272] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0705 06:32:58.837753       1 logging.go:59] [core] [Channel #73 SubChannel #74] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0705 06:32:58.838955       1 logging.go:59] [core] [Channel #31 SubChannel #32] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0705 06:32:58.838992       1 logging.go:59] [core] [Channel #289 SubChannel #290] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0705 06:32:58.838962       1 logging.go:59] [core] [Channel #211 SubChannel #212] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0705 06:32:58.838962       1 logging.go:59] [core] [Channel #22 SubChannel #23] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0705 06:32:58.838969       1 logging.go:59] [core] [Channel #232 SubChannel #233] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0705 06:32:58.838974       1 logging.go:59] [core] [Channel #91 SubChannel #92] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0705 06:32:58.839054       1 logging.go:59] [core] [Channel #64 SubChannel #65] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0705 06:32:58.838995       1 logging.go:59] [core] [Channel #238 SubChannel #239] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0705 06:32:58.839072       1 logging.go:59] [core] [Channel #139 SubChannel #140] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0705 06:32:58.839079       1 logging.go:59] [core] [Channel #76 SubChannel #77] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0705 06:32:58.839096       1 logging.go:59] [core] [Channel #193 SubChannel #194] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0705 06:32:58.839099       1 logging.go:59] [core] [Channel #229 SubChannel #230] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0705 06:32:58.839174       1 logging.go:59] [core] [Channel #217 SubChannel #218] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0705 06:32:58.839182       1 logging.go:59] [core] [Channel #58 SubChannel #59] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0705 06:32:58.839185       1 logging.go:59] [core] [Channel #175 SubChannel #176] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0705 06:32:58.840417       1 logging.go:59] [core] [Channel #88 SubChannel #89] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0705 06:32:58.840418       1 logging.go:59] [core] [Channel #25 SubChannel #26] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0705 06:32:58.840445       1 logging.go:59] [core] [Channel #274 SubChannel #275] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0705 06:32:58.840451       1 logging.go:59] [core] [Channel #118 SubChannel #119] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0705 06:32:58.840471       1 logging.go:59] [core] [Channel #79 SubChannel #80] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0705 06:32:58.840496       1 logging.go:59] [core] [Channel #265 SubChannel #266] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0705 06:32:58.840497       1 logging.go:59] [core] [Channel #268 SubChannel #269] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0705 06:32:58.840508       1 logging.go:59] [core] [Channel #34 SubChannel #35] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0705 06:32:58.840516       1 logging.go:59] [core] [Channel #103 SubChannel #104] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0705 06:32:58.840527       1 logging.go:59] [core] [Channel #133 SubChannel #134] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0705 06:32:58.840531       1 logging.go:59] [core] [Channel #148 SubChannel #149] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0705 06:32:58.840542       1 logging.go:59] [core] [Channel #49 SubChannel #50] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0705 06:32:58.840547       1 logging.go:59] [core] [Channel #97 SubChannel #98] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0705 06:32:58.840562       1 logging.go:59] [core] [Channel #142 SubChannel #143] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0705 06:32:58.840565       1 logging.go:59] [core] [Channel #250 SubChannel #251] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0705 06:32:58.840574       1 logging.go:59] [core] [Channel #5 SubChannel #6] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0705 06:32:58.840605       1 logging.go:59] [core] [Channel #166 SubChannel #167] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0705 06:32:58.840606       1 logging.go:59] [core] [Channel #112 SubChannel #113] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0705 06:32:58.840622       1 logging.go:59] [core] [Channel #28 SubChannel #29] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0705 06:32:58.840764       1 logging.go:59] [core] [Channel #160 SubChannel #161] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0705 06:32:58.840837       1 logging.go:59] [core] [Channel #196 SubChannel #197] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0705 06:32:58.840842       1 logging.go:59] [core] [Channel #61 SubChannel #62] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0705 06:32:58.840936       1 logging.go:59] [core] [Channel #154 SubChannel #155] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0705 06:32:58.841067       1 logging.go:59] [core] [Channel #67 SubChannel #68] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"


==> kube-controller-manager [5902080ba646] <==
I0705 06:16:27.781017       1 shared_informer.go:320] Caches are synced for endpoint_slice_mirroring
I0705 06:16:27.785855       1 shared_informer.go:320] Caches are synced for legacy-service-account-token-cleaner
I0705 06:16:27.795048       1 shared_informer.go:320] Caches are synced for validatingadmissionpolicy-status
I0705 06:16:27.796794       1 shared_informer.go:320] Caches are synced for endpoint
I0705 06:16:27.799981       1 shared_informer.go:320] Caches are synced for certificate-csrapproving
I0705 06:16:27.814305       1 shared_informer.go:320] Caches are synced for node
I0705 06:16:27.814338       1 range_allocator.go:175] "Sending events to api server" logger="node-ipam-controller"
I0705 06:16:27.814355       1 range_allocator.go:179] "Starting range CIDR allocator" logger="node-ipam-controller"
I0705 06:16:27.814360       1 shared_informer.go:313] Waiting for caches to sync for cidrallocator
I0705 06:16:27.814366       1 shared_informer.go:320] Caches are synced for cidrallocator
I0705 06:16:27.815472       1 shared_informer.go:320] Caches are synced for expand
I0705 06:16:27.816682       1 shared_informer.go:320] Caches are synced for crt configmap
I0705 06:16:27.817356       1 shared_informer.go:320] Caches are synced for taint-eviction-controller
I0705 06:16:27.819069       1 shared_informer.go:320] Caches are synced for ReplicaSet
I0705 06:16:27.819195       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="dev/blog-web-dev-5db6d8554b" duration="42.389¬µs"
I0705 06:16:27.819349       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="traefik/traefik-deployment-9f49c9d87" duration="56.5¬µs"
I0705 06:16:27.821226       1 shared_informer.go:320] Caches are synced for TTL
I0705 06:16:27.822367       1 shared_informer.go:320] Caches are synced for endpoint_slice
I0705 06:16:27.828235       1 shared_informer.go:320] Caches are synced for namespace
I0705 06:16:27.829358       1 shared_informer.go:320] Caches are synced for deployment
I0705 06:16:27.831470       1 shared_informer.go:320] Caches are synced for stateful set
I0705 06:16:27.833063       1 shared_informer.go:320] Caches are synced for certificate-csrsigning-kubelet-client
I0705 06:16:27.833075       1 shared_informer.go:320] Caches are synced for certificate-csrsigning-kubelet-serving
I0705 06:16:27.833081       1 shared_informer.go:320] Caches are synced for certificate-csrsigning-legacy-unknown
I0705 06:16:27.833066       1 shared_informer.go:320] Caches are synced for certificate-csrsigning-kube-apiserver-client
I0705 06:16:27.834195       1 shared_informer.go:320] Caches are synced for taint
I0705 06:16:27.834259       1 node_lifecycle_controller.go:1227] "Initializing eviction metric for zone" logger="node-lifecycle-controller" zone=""
I0705 06:16:27.834312       1 node_lifecycle_controller.go:879] "Missing timestamp for Node. Assuming now as a timestamp" logger="node-lifecycle-controller" node="minikube"
I0705 06:16:27.834352       1 node_lifecycle_controller.go:1073] "Controller detected that zone is now in new state" logger="node-lifecycle-controller" zone="" newState="Normal"
I0705 06:16:27.835457       1 shared_informer.go:320] Caches are synced for GC
I0705 06:16:27.848699       1 shared_informer.go:320] Caches are synced for HPA
I0705 06:16:27.877551       1 shared_informer.go:320] Caches are synced for PV protection
I0705 06:16:27.920168       1 shared_informer.go:320] Caches are synced for persistent volume
I0705 06:16:27.960105       1 shared_informer.go:320] Caches are synced for attach detach
I0705 06:16:28.022966       1 shared_informer.go:320] Caches are synced for ReplicationController
I0705 06:16:28.031156       1 shared_informer.go:320] Caches are synced for disruption
I0705 06:16:28.043389       1 shared_informer.go:320] Caches are synced for resource quota
I0705 06:16:28.062923       1 shared_informer.go:320] Caches are synced for resource quota
I0705 06:16:28.082433       1 shared_informer.go:320] Caches are synced for job
I0705 06:16:28.084615       1 shared_informer.go:320] Caches are synced for TTL after finished
I0705 06:16:28.084683       1 shared_informer.go:320] Caches are synced for cronjob
I0705 06:16:28.120321       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-7c445c467" duration="301.067033ms"
I0705 06:16:28.120557       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-7c445c467" duration="43.868¬µs"
I0705 06:16:28.126015       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="istio-system/istiod-7c948c8756" duration="306.776914ms"
I0705 06:16:28.126024       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="istio-system/istio-ingressgateway-ff47dcc89" duration="306.831696ms"
I0705 06:16:28.126039       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="istio-system/istio-egressgateway-77d7544cf7" duration="306.921982ms"
I0705 06:16:28.126122       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="istio-system/istiod-7c948c8756" duration="52.684¬µs"
I0705 06:16:28.126199       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="istio-system/istio-egressgateway-77d7544cf7" duration="52.418¬µs"
I0705 06:16:28.126317       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="istio-system/istio-ingressgateway-ff47dcc89" duration="80.065¬µs"
I0705 06:16:28.475948       1 shared_informer.go:320] Caches are synced for garbage collector
I0705 06:16:28.498328       1 shared_informer.go:320] Caches are synced for garbage collector
I0705 06:16:28.498341       1 garbagecollector.go:157] "All resource monitors have synced. Proceeding to collect garbage" logger="garbage-collector-controller"
I0705 06:16:49.123579       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="istio-system/istiod-7c948c8756" duration="9.271043ms"
I0705 06:16:49.123650       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="istio-system/istiod-7c948c8756" duration="38.59¬µs"
I0705 06:16:56.828571       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-7c445c467" duration="20.634215ms"
I0705 06:16:56.828621       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-7c445c467" duration="29.302¬µs"
I0705 06:17:02.144914       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="istio-system/istio-egressgateway-77d7544cf7" duration="13.305289ms"
I0705 06:17:02.147721       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="istio-system/istio-egressgateway-77d7544cf7" duration="60.156¬µs"
I0705 06:17:14.135140       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="istio-system/istio-ingressgateway-ff47dcc89" duration="13.311066ms"
I0705 06:17:14.135204       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="istio-system/istio-ingressgateway-ff47dcc89" duration="39.226¬µs"


==> kube-controller-manager [6765934f60a2] <==
W0708 06:50:05.907560       1 reflector.go:547] k8s.io/client-go/metadata/metadatainformer/informer.go:138: failed to list *v1.PartialObjectMetadata: the server could not find the requested resource
E0708 06:50:05.907580       1 reflector.go:150] k8s.io/client-go/metadata/metadatainformer/informer.go:138: Failed to watch *v1.PartialObjectMetadata: failed to list *v1.PartialObjectMetadata: the server could not find the requested resource
W0708 06:50:14.598188       1 reflector.go:547] k8s.io/client-go/metadata/metadatainformer/informer.go:138: failed to list *v1.PartialObjectMetadata: the server could not find the requested resource
E0708 06:50:14.598207       1 reflector.go:150] k8s.io/client-go/metadata/metadatainformer/informer.go:138: Failed to watch *v1.PartialObjectMetadata: failed to list *v1.PartialObjectMetadata: the server could not find the requested resource
W0708 06:50:18.848351       1 reflector.go:547] k8s.io/client-go/metadata/metadatainformer/informer.go:138: failed to list *v1.PartialObjectMetadata: the server could not find the requested resource
E0708 06:50:18.848371       1 reflector.go:150] k8s.io/client-go/metadata/metadatainformer/informer.go:138: Failed to watch *v1.PartialObjectMetadata: failed to list *v1.PartialObjectMetadata: the server could not find the requested resource
W0708 06:50:19.257625       1 reflector.go:547] k8s.io/client-go/metadata/metadatainformer/informer.go:138: failed to list *v1.PartialObjectMetadata: the server could not find the requested resource
E0708 06:50:19.257646       1 reflector.go:150] k8s.io/client-go/metadata/metadatainformer/informer.go:138: Failed to watch *v1.PartialObjectMetadata: failed to list *v1.PartialObjectMetadata: the server could not find the requested resource
W0708 06:50:26.071179       1 reflector.go:547] k8s.io/client-go/metadata/metadatainformer/informer.go:138: failed to list *v1.PartialObjectMetadata: the server could not find the requested resource
E0708 06:50:26.071199       1 reflector.go:150] k8s.io/client-go/metadata/metadatainformer/informer.go:138: Failed to watch *v1.PartialObjectMetadata: failed to list *v1.PartialObjectMetadata: the server could not find the requested resource
W0708 06:50:28.371322       1 reflector.go:547] k8s.io/client-go/metadata/metadatainformer/informer.go:138: failed to list *v1.PartialObjectMetadata: the server could not find the requested resource
E0708 06:50:28.371342       1 reflector.go:150] k8s.io/client-go/metadata/metadatainformer/informer.go:138: Failed to watch *v1.PartialObjectMetadata: failed to list *v1.PartialObjectMetadata: the server could not find the requested resource
W0708 06:50:30.295954       1 reflector.go:547] k8s.io/client-go/metadata/metadatainformer/informer.go:138: failed to list *v1.PartialObjectMetadata: the server could not find the requested resource
E0708 06:50:30.295977       1 reflector.go:150] k8s.io/client-go/metadata/metadatainformer/informer.go:138: Failed to watch *v1.PartialObjectMetadata: failed to list *v1.PartialObjectMetadata: the server could not find the requested resource
W0708 06:50:33.190368       1 reflector.go:547] k8s.io/client-go/metadata/metadatainformer/informer.go:138: failed to list *v1.PartialObjectMetadata: the server could not find the requested resource
E0708 06:50:33.190387       1 reflector.go:150] k8s.io/client-go/metadata/metadatainformer/informer.go:138: Failed to watch *v1.PartialObjectMetadata: failed to list *v1.PartialObjectMetadata: the server could not find the requested resource
W0708 06:50:55.796349       1 reflector.go:547] k8s.io/client-go/metadata/metadatainformer/informer.go:138: failed to list *v1.PartialObjectMetadata: the server could not find the requested resource
E0708 06:50:55.796370       1 reflector.go:150] k8s.io/client-go/metadata/metadatainformer/informer.go:138: Failed to watch *v1.PartialObjectMetadata: failed to list *v1.PartialObjectMetadata: the server could not find the requested resource
W0708 06:50:58.085598       1 reflector.go:547] k8s.io/client-go/metadata/metadatainformer/informer.go:138: failed to list *v1.PartialObjectMetadata: the server could not find the requested resource
E0708 06:50:58.085618       1 reflector.go:150] k8s.io/client-go/metadata/metadatainformer/informer.go:138: Failed to watch *v1.PartialObjectMetadata: failed to list *v1.PartialObjectMetadata: the server could not find the requested resource
W0708 06:50:58.497668       1 reflector.go:547] k8s.io/client-go/metadata/metadatainformer/informer.go:138: failed to list *v1.PartialObjectMetadata: the server could not find the requested resource
E0708 06:50:58.497686       1 reflector.go:150] k8s.io/client-go/metadata/metadatainformer/informer.go:138: Failed to watch *v1.PartialObjectMetadata: failed to list *v1.PartialObjectMetadata: the server could not find the requested resource
W0708 06:51:00.650116       1 reflector.go:547] k8s.io/client-go/metadata/metadatainformer/informer.go:138: failed to list *v1.PartialObjectMetadata: the server could not find the requested resource
E0708 06:51:00.650135       1 reflector.go:150] k8s.io/client-go/metadata/metadatainformer/informer.go:138: Failed to watch *v1.PartialObjectMetadata: failed to list *v1.PartialObjectMetadata: the server could not find the requested resource
W0708 06:51:04.002107       1 reflector.go:547] k8s.io/client-go/metadata/metadatainformer/informer.go:138: failed to list *v1.PartialObjectMetadata: the server could not find the requested resource
E0708 06:51:04.002127       1 reflector.go:150] k8s.io/client-go/metadata/metadatainformer/informer.go:138: Failed to watch *v1.PartialObjectMetadata: failed to list *v1.PartialObjectMetadata: the server could not find the requested resource
W0708 06:51:05.002335       1 reflector.go:547] k8s.io/client-go/metadata/metadatainformer/informer.go:138: failed to list *v1.PartialObjectMetadata: the server could not find the requested resource
E0708 06:51:05.002354       1 reflector.go:150] k8s.io/client-go/metadata/metadatainformer/informer.go:138: Failed to watch *v1.PartialObjectMetadata: failed to list *v1.PartialObjectMetadata: the server could not find the requested resource
W0708 06:51:05.615516       1 reflector.go:547] k8s.io/client-go/metadata/metadatainformer/informer.go:138: failed to list *v1.PartialObjectMetadata: the server could not find the requested resource
E0708 06:51:05.615535       1 reflector.go:150] k8s.io/client-go/metadata/metadatainformer/informer.go:138: Failed to watch *v1.PartialObjectMetadata: failed to list *v1.PartialObjectMetadata: the server could not find the requested resource
W0708 06:51:08.317558       1 reflector.go:547] k8s.io/client-go/metadata/metadatainformer/informer.go:138: failed to list *v1.PartialObjectMetadata: the server could not find the requested resource
E0708 06:51:08.317580       1 reflector.go:150] k8s.io/client-go/metadata/metadatainformer/informer.go:138: Failed to watch *v1.PartialObjectMetadata: failed to list *v1.PartialObjectMetadata: the server could not find the requested resource
W0708 06:51:18.155107       1 reflector.go:547] k8s.io/client-go/metadata/metadatainformer/informer.go:138: failed to list *v1.PartialObjectMetadata: the server could not find the requested resource
E0708 06:51:18.155128       1 reflector.go:150] k8s.io/client-go/metadata/metadatainformer/informer.go:138: Failed to watch *v1.PartialObjectMetadata: failed to list *v1.PartialObjectMetadata: the server could not find the requested resource
W0708 06:51:35.960301       1 reflector.go:547] k8s.io/client-go/metadata/metadatainformer/informer.go:138: failed to list *v1.PartialObjectMetadata: the server could not find the requested resource
E0708 06:51:35.960327       1 reflector.go:150] k8s.io/client-go/metadata/metadatainformer/informer.go:138: Failed to watch *v1.PartialObjectMetadata: failed to list *v1.PartialObjectMetadata: the server could not find the requested resource
W0708 06:51:39.803036       1 reflector.go:547] k8s.io/client-go/metadata/metadatainformer/informer.go:138: failed to list *v1.PartialObjectMetadata: the server could not find the requested resource
E0708 06:51:39.803055       1 reflector.go:150] k8s.io/client-go/metadata/metadatainformer/informer.go:138: Failed to watch *v1.PartialObjectMetadata: failed to list *v1.PartialObjectMetadata: the server could not find the requested resource
W0708 06:51:50.500003       1 reflector.go:547] k8s.io/client-go/metadata/metadatainformer/informer.go:138: failed to list *v1.PartialObjectMetadata: the server could not find the requested resource
E0708 06:51:50.500023       1 reflector.go:150] k8s.io/client-go/metadata/metadatainformer/informer.go:138: Failed to watch *v1.PartialObjectMetadata: failed to list *v1.PartialObjectMetadata: the server could not find the requested resource
W0708 06:51:50.802399       1 reflector.go:547] k8s.io/client-go/metadata/metadatainformer/informer.go:138: failed to list *v1.PartialObjectMetadata: the server could not find the requested resource
E0708 06:51:50.802421       1 reflector.go:150] k8s.io/client-go/metadata/metadatainformer/informer.go:138: Failed to watch *v1.PartialObjectMetadata: failed to list *v1.PartialObjectMetadata: the server could not find the requested resource
W0708 06:51:51.660576       1 reflector.go:547] k8s.io/client-go/metadata/metadatainformer/informer.go:138: failed to list *v1.PartialObjectMetadata: the server could not find the requested resource
E0708 06:51:51.660601       1 reflector.go:150] k8s.io/client-go/metadata/metadatainformer/informer.go:138: Failed to watch *v1.PartialObjectMetadata: failed to list *v1.PartialObjectMetadata: the server could not find the requested resource
W0708 06:51:51.956482       1 reflector.go:547] k8s.io/client-go/metadata/metadatainformer/informer.go:138: failed to list *v1.PartialObjectMetadata: the server could not find the requested resource
E0708 06:51:51.956500       1 reflector.go:150] k8s.io/client-go/metadata/metadatainformer/informer.go:138: Failed to watch *v1.PartialObjectMetadata: failed to list *v1.PartialObjectMetadata: the server could not find the requested resource
W0708 06:51:54.125539       1 reflector.go:547] k8s.io/client-go/metadata/metadatainformer/informer.go:138: failed to list *v1.PartialObjectMetadata: the server could not find the requested resource
E0708 06:51:54.125561       1 reflector.go:150] k8s.io/client-go/metadata/metadatainformer/informer.go:138: Failed to watch *v1.PartialObjectMetadata: failed to list *v1.PartialObjectMetadata: the server could not find the requested resource
W0708 06:51:54.509535       1 reflector.go:547] k8s.io/client-go/metadata/metadatainformer/informer.go:138: failed to list *v1.PartialObjectMetadata: the server could not find the requested resource
E0708 06:51:54.509553       1 reflector.go:150] k8s.io/client-go/metadata/metadatainformer/informer.go:138: Failed to watch *v1.PartialObjectMetadata: failed to list *v1.PartialObjectMetadata: the server could not find the requested resource
W0708 06:51:56.683508       1 reflector.go:547] k8s.io/client-go/metadata/metadatainformer/informer.go:138: failed to list *v1.PartialObjectMetadata: the server could not find the requested resource
E0708 06:51:56.683533       1 reflector.go:150] k8s.io/client-go/metadata/metadatainformer/informer.go:138: Failed to watch *v1.PartialObjectMetadata: failed to list *v1.PartialObjectMetadata: the server could not find the requested resource
W0708 06:52:29.722111       1 reflector.go:547] k8s.io/client-go/metadata/metadatainformer/informer.go:138: failed to list *v1.PartialObjectMetadata: the server could not find the requested resource
E0708 06:52:29.722132       1 reflector.go:150] k8s.io/client-go/metadata/metadatainformer/informer.go:138: Failed to watch *v1.PartialObjectMetadata: failed to list *v1.PartialObjectMetadata: the server could not find the requested resource
W0708 06:52:32.301837       1 reflector.go:547] k8s.io/client-go/metadata/metadatainformer/informer.go:138: failed to list *v1.PartialObjectMetadata: the server could not find the requested resource
E0708 06:52:32.301857       1 reflector.go:150] k8s.io/client-go/metadata/metadatainformer/informer.go:138: Failed to watch *v1.PartialObjectMetadata: failed to list *v1.PartialObjectMetadata: the server could not find the requested resource
W0708 06:52:32.691041       1 reflector.go:547] k8s.io/client-go/metadata/metadatainformer/informer.go:138: failed to list *v1.PartialObjectMetadata: the server could not find the requested resource
E0708 06:52:32.691060       1 reflector.go:150] k8s.io/client-go/metadata/metadatainformer/informer.go:138: Failed to watch *v1.PartialObjectMetadata: failed to list *v1.PartialObjectMetadata: the server could not find the requested resource
W0708 06:52:34.146331       1 reflector.go:547] k8s.io/client-go/metadata/metadatainformer/informer.go:138: failed to list *v1.PartialObjectMetadata: the server could not find the requested resource
E0708 06:52:34.146355       1 reflector.go:150] k8s.io/client-go/metadata/metadatainformer/informer.go:138: Failed to watch *v1.PartialObjectMetadata: failed to list *v1.PartialObjectMetadata: the server could not find the requested resource


==> kube-proxy [e474f8be7678] <==
I0708 06:49:09.683907       1 server_linux.go:69] "Using iptables proxy"
I0708 06:49:09.692835       1 server.go:1062] "Successfully retrieved node IP(s)" IPs=["192.168.58.2"]
I0708 06:49:09.705538       1 server.go:659] "kube-proxy running in dual-stack mode" primary ipFamily="IPv4"
I0708 06:49:09.705572       1 server_linux.go:165] "Using iptables Proxier"
I0708 06:49:09.706966       1 server_linux.go:511] "Detect-local-mode set to ClusterCIDR, but no cluster CIDR for family" ipFamily="IPv6"
I0708 06:49:09.706977       1 server_linux.go:528] "Defaulting to no-op detect-local"
I0708 06:49:09.707275       1 proxier.go:243] "Setting route_localnet=1 to allow node-ports on localhost; to change this either disable iptables.localhostNodePorts (--iptables-localhost-nodeports) or set nodePortAddresses (--nodeport-addresses) to filter loopback addresses"
I0708 06:49:09.707400       1 server.go:872] "Version info" version="v1.30.0"
I0708 06:49:09.707414       1 server.go:874] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0708 06:49:09.708695       1 config.go:192] "Starting service config controller"
I0708 06:49:09.708700       1 config.go:101] "Starting endpoint slice config controller"
I0708 06:49:09.708820       1 config.go:319] "Starting node config controller"
I0708 06:49:09.708866       1 shared_informer.go:313] Waiting for caches to sync for service config
I0708 06:49:09.708865       1 shared_informer.go:313] Waiting for caches to sync for endpoint slice config
I0708 06:49:09.708870       1 shared_informer.go:313] Waiting for caches to sync for node config
I0708 06:49:09.809747       1 shared_informer.go:320] Caches are synced for node config
I0708 06:49:09.809749       1 shared_informer.go:320] Caches are synced for service config
I0708 06:49:09.809750       1 shared_informer.go:320] Caches are synced for endpoint slice config


==> kube-scheduler [416e9dc66185] <==
I0705 07:05:54.909855       1 serving.go:380] Generated self-signed cert in-memory
W0705 07:05:55.939875       1 requestheader_controller.go:193] Unable to get configmap/extension-apiserver-authentication in kube-system.  Usually fixed by 'kubectl create rolebinding -n kube-system ROLEBINDING_NAME --role=extension-apiserver-authentication-reader --serviceaccount=YOUR_NS:YOUR_SA'
W0705 07:05:55.939899       1 authentication.go:368] Error looking up in-cluster authentication configuration: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot get resource "configmaps" in API group "" in the namespace "kube-system"
W0705 07:05:55.939909       1 authentication.go:369] Continuing without authentication configuration. This may treat all requests as anonymous.
W0705 07:05:55.940463       1 authentication.go:370] To require authentication configuration lookup to succeed, set --authentication-tolerate-lookup-failure=false
I0705 07:05:55.954834       1 server.go:154] "Starting Kubernetes Scheduler" version="v1.30.0"
I0705 07:05:55.954858       1 server.go:156] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0705 07:05:55.956782       1 secure_serving.go:213] Serving securely on 127.0.0.1:10259
I0705 07:05:55.956955       1 tlsconfig.go:240] "Starting DynamicServingCertificateController"
I0705 07:05:55.956972       1 configmap_cafile_content.go:202] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I0705 07:05:55.957640       1 shared_informer.go:313] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
I0705 07:05:56.058210       1 shared_informer.go:320] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::client-ca-file


==> kube-scheduler [b24dd1a5dfbc] <==
I0705 06:16:14.400027       1 serving.go:380] Generated self-signed cert in-memory
W0705 06:16:15.663785       1 requestheader_controller.go:193] Unable to get configmap/extension-apiserver-authentication in kube-system.  Usually fixed by 'kubectl create rolebinding -n kube-system ROLEBINDING_NAME --role=extension-apiserver-authentication-reader --serviceaccount=YOUR_NS:YOUR_SA'
W0705 06:16:15.663809       1 authentication.go:368] Error looking up in-cluster authentication configuration: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot get resource "configmaps" in API group "" in the namespace "kube-system"
W0705 06:16:15.663819       1 authentication.go:369] Continuing without authentication configuration. This may treat all requests as anonymous.
W0705 06:16:15.663826       1 authentication.go:370] To require authentication configuration lookup to succeed, set --authentication-tolerate-lookup-failure=false
I0705 06:16:15.672957       1 server.go:154] "Starting Kubernetes Scheduler" version="v1.30.0"
I0705 06:16:15.672975       1 server.go:156] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0705 06:16:15.674896       1 configmap_cafile_content.go:202] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I0705 06:16:15.674913       1 shared_informer.go:313] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
I0705 06:16:15.675066       1 secure_serving.go:213] Serving securely on 127.0.0.1:10259
I0705 06:16:15.675977       1 tlsconfig.go:240] "Starting DynamicServingCertificateController"
I0705 06:16:15.775486       1 shared_informer.go:320] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
I0705 06:32:57.844375       1 tlsconfig.go:255] "Shutting down DynamicServingCertificateController"
I0705 06:32:57.844514       1 secure_serving.go:258] Stopped listening on 127.0.0.1:10259
I0705 06:32:57.844865       1 configmap_cafile_content.go:223] "Shutting down controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
E0705 06:32:57.845172       1 run.go:74] "command failed" err="finished without leader elect"


==> kubelet <==
Jul 08 03:28:11 minikube kubelet[1428]: E0708 03:28:11.096316    1428 kuberuntime_manager.go:1256] container &Container{Name:prometheus,Image:prom/prometheus:v2.2.1,Command:[/bin/prometheus],Args:[--config.file=/etc/prometheus/prometheus.yml],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:9090,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:prometheus-config,ReadOnly:false,MountPath:/etc/prometheus,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},VolumeMount{Name:kube-api-access-ln5vk,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod prometheus-7f5475d84c-fh7wp_prometheus(e05260d1-3f74-4821-8fe2-e0429487b988): ErrImagePull: [DEPRECATION NOTICE] Docker Image Format v1 and Docker Image manifest version 2, schema 1 support is disabled by default and will be removed in an upcoming release. Suggest the author of docker.io/prom/prometheus:v2.2.1 to upgrade the image to the OCI Format or Docker Image manifest v2, schema 2. More information at https://docs.docker.com/go/deprecated-image-specs/
Jul 08 03:28:11 minikube kubelet[1428]: E0708 03:28:11.096336    1428 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"prometheus\" with ErrImagePull: \"[DEPRECATION NOTICE] Docker Image Format v1 and Docker Image manifest version 2, schema 1 support is disabled by default and will be removed in an upcoming release. Suggest the author of docker.io/prom/prometheus:v2.2.1 to upgrade the image to the OCI Format or Docker Image manifest v2, schema 2. More information at https://docs.docker.com/go/deprecated-image-specs/\"" pod="prometheus/prometheus-7f5475d84c-fh7wp" podUID="e05260d1-3f74-4821-8fe2-e0429487b988"
Jul 08 03:28:11 minikube kubelet[1428]: I0708 03:28:11.926256    1428 topology_manager.go:215] "Topology Admit Handler" podUID="c30b4a83-d69d-4e6b-8d83-1ab3dd995953" podNamespace="prometheus" podName="prometheus-cdbc74675-qk5x9"
Jul 08 03:28:11 minikube kubelet[1428]: I0708 03:28:11.926330    1428 memory_manager.go:354] "RemoveStaleState removing state" podUID="ac316b98-af0c-43fd-9145-7fa0a52b22cf" containerName="cert-manager-controller"
Jul 08 03:28:12 minikube kubelet[1428]: I0708 03:28:12.056873    1428 reconciler_common.go:247] "operationExecutor.VerifyControllerAttachedVolume started for volume \"prometheus-config\" (UniqueName: \"kubernetes.io/configmap/c30b4a83-d69d-4e6b-8d83-1ab3dd995953-prometheus-config\") pod \"prometheus-cdbc74675-qk5x9\" (UID: \"c30b4a83-d69d-4e6b-8d83-1ab3dd995953\") " pod="prometheus/prometheus-cdbc74675-qk5x9"
Jul 08 03:28:12 minikube kubelet[1428]: I0708 03:28:12.056908    1428 reconciler_common.go:247] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-5s9mq\" (UniqueName: \"kubernetes.io/projected/c30b4a83-d69d-4e6b-8d83-1ab3dd995953-kube-api-access-5s9mq\") pod \"prometheus-cdbc74675-qk5x9\" (UID: \"c30b4a83-d69d-4e6b-8d83-1ab3dd995953\") " pod="prometheus/prometheus-cdbc74675-qk5x9"
Jul 08 03:28:25 minikube kubelet[1428]: E0708 03:28:25.058429    1428 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"prometheus\" with ImagePullBackOff: \"Back-off pulling image \\\"prom/prometheus:v2.2.1\\\"\"" pod="prometheus/prometheus-7f5475d84c-fh7wp" podUID="e05260d1-3f74-4821-8fe2-e0429487b988"
Jul 08 03:28:37 minikube kubelet[1428]: E0708 03:28:37.058914    1428 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"prometheus\" with ImagePullBackOff: \"Back-off pulling image \\\"prom/prometheus:v2.2.1\\\"\"" pod="prometheus/prometheus-7f5475d84c-fh7wp" podUID="e05260d1-3f74-4821-8fe2-e0429487b988"
Jul 08 03:28:49 minikube kubelet[1428]: E0708 03:28:49.059098    1428 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"prometheus\" with ImagePullBackOff: \"Back-off pulling image \\\"prom/prometheus:v2.2.1\\\"\"" pod="prometheus/prometheus-7f5475d84c-fh7wp" podUID="e05260d1-3f74-4821-8fe2-e0429487b988"
Jul 08 03:29:02 minikube kubelet[1428]: E0708 03:29:02.058817    1428 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"prometheus\" with ImagePullBackOff: \"Back-off pulling image \\\"prom/prometheus:v2.2.1\\\"\"" pod="prometheus/prometheus-7f5475d84c-fh7wp" podUID="e05260d1-3f74-4821-8fe2-e0429487b988"
Jul 08 03:29:15 minikube kubelet[1428]: E0708 03:29:15.058745    1428 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"prometheus\" with ImagePullBackOff: \"Back-off pulling image \\\"prom/prometheus:v2.2.1\\\"\"" pod="prometheus/prometheus-7f5475d84c-fh7wp" podUID="e05260d1-3f74-4821-8fe2-e0429487b988"
Jul 08 03:29:27 minikube kubelet[1428]: E0708 03:29:27.058919    1428 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"prometheus\" with ImagePullBackOff: \"Back-off pulling image \\\"prom/prometheus:v2.2.1\\\"\"" pod="prometheus/prometheus-7f5475d84c-fh7wp" podUID="e05260d1-3f74-4821-8fe2-e0429487b988"
Jul 08 03:32:57 minikube kubelet[1428]: I0708 03:32:57.431728    1428 pod_startup_latency_tracker.go:104] "Observed pod startup duration" pod="prometheus/prometheus-cdbc74675-qk5x9" podStartSLOduration=2.623426748 podStartE2EDuration="4m46.431709965s" podCreationTimestamp="2024-07-08 03:28:11 +0000 UTC" firstStartedPulling="2024-07-08 03:28:12.544221259 +0000 UTC m=+22523.538806585" lastFinishedPulling="2024-07-08 03:32:56.352504475 +0000 UTC m=+22807.347089802" observedRunningTime="2024-07-08 03:32:57.431418032 +0000 UTC m=+22808.426003362" watchObservedRunningTime="2024-07-08 03:32:57.431709965 +0000 UTC m=+22808.426295292"
Jul 08 03:32:59 minikube kubelet[1428]: E0708 03:32:59.382382    1428 remote_image.go:180] "PullImage from image service failed" err="rpc error: code = Unknown desc = [DEPRECATION NOTICE] Docker Image Format v1 and Docker Image manifest version 2, schema 1 support is disabled by default and will be removed in an upcoming release. Suggest the author of docker.io/prom/prometheus:v2.2.1 to upgrade the image to the OCI Format or Docker Image manifest v2, schema 2. More information at https://docs.docker.com/go/deprecated-image-specs/" image="prom/prometheus:v2.2.1"
Jul 08 03:32:59 minikube kubelet[1428]: E0708 03:32:59.382419    1428 kuberuntime_image.go:55] "Failed to pull image" err="[DEPRECATION NOTICE] Docker Image Format v1 and Docker Image manifest version 2, schema 1 support is disabled by default and will be removed in an upcoming release. Suggest the author of docker.io/prom/prometheus:v2.2.1 to upgrade the image to the OCI Format or Docker Image manifest v2, schema 2. More information at https://docs.docker.com/go/deprecated-image-specs/" image="prom/prometheus:v2.2.1"
Jul 08 03:32:59 minikube kubelet[1428]: E0708 03:32:59.382491    1428 kuberuntime_manager.go:1256] container &Container{Name:prometheus,Image:prom/prometheus:v2.2.1,Command:[/bin/prometheus],Args:[--config.file=/etc/prometheus/prometheus.yml],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:9090,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:prometheus-config,ReadOnly:false,MountPath:/etc/prometheus,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},VolumeMount{Name:kube-api-access-ln5vk,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod prometheus-7f5475d84c-fh7wp_prometheus(e05260d1-3f74-4821-8fe2-e0429487b988): ErrImagePull: [DEPRECATION NOTICE] Docker Image Format v1 and Docker Image manifest version 2, schema 1 support is disabled by default and will be removed in an upcoming release. Suggest the author of docker.io/prom/prometheus:v2.2.1 to upgrade the image to the OCI Format or Docker Image manifest v2, schema 2. More information at https://docs.docker.com/go/deprecated-image-specs/
Jul 08 03:32:59 minikube kubelet[1428]: E0708 03:32:59.382514    1428 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"prometheus\" with ErrImagePull: \"[DEPRECATION NOTICE] Docker Image Format v1 and Docker Image manifest version 2, schema 1 support is disabled by default and will be removed in an upcoming release. Suggest the author of docker.io/prom/prometheus:v2.2.1 to upgrade the image to the OCI Format or Docker Image manifest v2, schema 2. More information at https://docs.docker.com/go/deprecated-image-specs/\"" pod="prometheus/prometheus-7f5475d84c-fh7wp" podUID="e05260d1-3f74-4821-8fe2-e0429487b988"
Jul 08 03:32:59 minikube kubelet[1428]: I0708 03:32:59.738389    1428 reconciler_common.go:161] "operationExecutor.UnmountVolume started for volume \"prometheus-config\" (UniqueName: \"kubernetes.io/configmap/e05260d1-3f74-4821-8fe2-e0429487b988-prometheus-config\") pod \"e05260d1-3f74-4821-8fe2-e0429487b988\" (UID: \"e05260d1-3f74-4821-8fe2-e0429487b988\") "
Jul 08 03:32:59 minikube kubelet[1428]: I0708 03:32:59.738423    1428 reconciler_common.go:161] "operationExecutor.UnmountVolume started for volume \"kube-api-access-ln5vk\" (UniqueName: \"kubernetes.io/projected/e05260d1-3f74-4821-8fe2-e0429487b988-kube-api-access-ln5vk\") pod \"e05260d1-3f74-4821-8fe2-e0429487b988\" (UID: \"e05260d1-3f74-4821-8fe2-e0429487b988\") "
Jul 08 03:32:59 minikube kubelet[1428]: I0708 03:32:59.738660    1428 operation_generator.go:887] UnmountVolume.TearDown succeeded for volume "kubernetes.io/configmap/e05260d1-3f74-4821-8fe2-e0429487b988-prometheus-config" (OuterVolumeSpecName: "prometheus-config") pod "e05260d1-3f74-4821-8fe2-e0429487b988" (UID: "e05260d1-3f74-4821-8fe2-e0429487b988"). InnerVolumeSpecName "prometheus-config". PluginName "kubernetes.io/configmap", VolumeGidValue ""
Jul 08 03:32:59 minikube kubelet[1428]: I0708 03:32:59.739932    1428 operation_generator.go:887] UnmountVolume.TearDown succeeded for volume "kubernetes.io/projected/e05260d1-3f74-4821-8fe2-e0429487b988-kube-api-access-ln5vk" (OuterVolumeSpecName: "kube-api-access-ln5vk") pod "e05260d1-3f74-4821-8fe2-e0429487b988" (UID: "e05260d1-3f74-4821-8fe2-e0429487b988"). InnerVolumeSpecName "kube-api-access-ln5vk". PluginName "kubernetes.io/projected", VolumeGidValue ""
Jul 08 03:32:59 minikube kubelet[1428]: I0708 03:32:59.838587    1428 reconciler_common.go:289] "Volume detached for volume \"prometheus-config\" (UniqueName: \"kubernetes.io/configmap/e05260d1-3f74-4821-8fe2-e0429487b988-prometheus-config\") on node \"minikube\" DevicePath \"\""
Jul 08 03:32:59 minikube kubelet[1428]: I0708 03:32:59.838606    1428 reconciler_common.go:289] "Volume detached for volume \"kube-api-access-ln5vk\" (UniqueName: \"kubernetes.io/projected/e05260d1-3f74-4821-8fe2-e0429487b988-kube-api-access-ln5vk\") on node \"minikube\" DevicePath \"\""
Jul 08 03:33:01 minikube kubelet[1428]: I0708 03:33:01.062728    1428 kubelet_volumes.go:163] "Cleaned up orphaned pod volumes dir" podUID="e05260d1-3f74-4821-8fe2-e0429487b988" path="/var/lib/kubelet/pods/e05260d1-3f74-4821-8fe2-e0429487b988/volumes"
Jul 08 03:59:29 minikube kubelet[1428]: E0708 03:59:29.199596    1428 upgradeaware.go:427] Error proxying data from client to backend: readfrom tcp [::1]:44008->[::1]:43133: write tcp [::1]:44008->[::1]:43133: write: broken pipe
Jul 08 04:08:39 minikube kubelet[1428]: E0708 04:08:39.048419    1428 upgradeaware.go:427] Error proxying data from client to backend: readfrom tcp [::1]:43108->[::1]:43133: write tcp [::1]:43108->[::1]:43133: write: broken pipe
Jul 08 06:10:42 minikube kubelet[1428]: E0708 06:10:42.557671    1428 upgradeaware.go:427] Error proxying data from client to backend: readfrom tcp [::1]:47570->[::1]:43133: write tcp [::1]:47570->[::1]:43133: write: broken pipe
Jul 08 06:49:08 minikube kubelet[1428]: I0708 06:49:08.985146    1428 scope.go:117] "RemoveContainer" containerID="5f701fef7925ad6c139f45592c637ff2b98df468660bd17c3408d7b21f9b0f08"
Jul 08 06:49:09 minikube kubelet[1428]: I0708 06:49:09.002861    1428 scope.go:117] "RemoveContainer" containerID="b7615fc7a1b9495c88508f12983858fa9ae9d16113c602daad95ad5dea12839e"
Jul 08 06:49:09 minikube kubelet[1428]: I0708 06:49:09.018652    1428 topology_manager.go:215] "Topology Admit Handler" podUID="ae14030d-5a45-408b-ada4-52b270121030" podNamespace="kube-system" podName="kube-proxy-r82ck"
Jul 08 06:49:09 minikube kubelet[1428]: E0708 06:49:09.018723    1428 cpu_manager.go:395] "RemoveStaleState: removing container" podUID="84bf6f89-6056-433e-8d73-a7b1bc0179d7" containerName="kube-proxy"
Jul 08 06:49:09 minikube kubelet[1428]: E0708 06:49:09.018735    1428 cpu_manager.go:395] "RemoveStaleState: removing container" podUID="84bf6f89-6056-433e-8d73-a7b1bc0179d7" containerName="kube-proxy"
Jul 08 06:49:09 minikube kubelet[1428]: I0708 06:49:09.018771    1428 memory_manager.go:354] "RemoveStaleState removing state" podUID="84bf6f89-6056-433e-8d73-a7b1bc0179d7" containerName="kube-proxy"
Jul 08 06:49:09 minikube kubelet[1428]: I0708 06:49:09.018783    1428 memory_manager.go:354] "RemoveStaleState removing state" podUID="84bf6f89-6056-433e-8d73-a7b1bc0179d7" containerName="kube-proxy"
Jul 08 06:49:09 minikube kubelet[1428]: I0708 06:49:09.018791    1428 memory_manager.go:354] "RemoveStaleState removing state" podUID="84bf6f89-6056-433e-8d73-a7b1bc0179d7" containerName="kube-proxy"
Jul 08 06:49:09 minikube kubelet[1428]: I0708 06:49:09.027654    1428 scope.go:117] "RemoveContainer" containerID="5f701fef7925ad6c139f45592c637ff2b98df468660bd17c3408d7b21f9b0f08"
Jul 08 06:49:09 minikube kubelet[1428]: E0708 06:49:09.028104    1428 remote_runtime.go:432] "ContainerStatus from runtime service failed" err="rpc error: code = Unknown desc = Error response from daemon: No such container: 5f701fef7925ad6c139f45592c637ff2b98df468660bd17c3408d7b21f9b0f08" containerID="5f701fef7925ad6c139f45592c637ff2b98df468660bd17c3408d7b21f9b0f08"
Jul 08 06:49:09 minikube kubelet[1428]: I0708 06:49:09.028126    1428 pod_container_deletor.go:53] "DeleteContainer returned error" containerID={"Type":"docker","ID":"5f701fef7925ad6c139f45592c637ff2b98df468660bd17c3408d7b21f9b0f08"} err="failed to get container status \"5f701fef7925ad6c139f45592c637ff2b98df468660bd17c3408d7b21f9b0f08\": rpc error: code = Unknown desc = Error response from daemon: No such container: 5f701fef7925ad6c139f45592c637ff2b98df468660bd17c3408d7b21f9b0f08"
Jul 08 06:49:09 minikube kubelet[1428]: I0708 06:49:09.028139    1428 scope.go:117] "RemoveContainer" containerID="b7615fc7a1b9495c88508f12983858fa9ae9d16113c602daad95ad5dea12839e"
Jul 08 06:49:09 minikube kubelet[1428]: E0708 06:49:09.028438    1428 remote_runtime.go:432] "ContainerStatus from runtime service failed" err="rpc error: code = Unknown desc = Error response from daemon: No such container: b7615fc7a1b9495c88508f12983858fa9ae9d16113c602daad95ad5dea12839e" containerID="b7615fc7a1b9495c88508f12983858fa9ae9d16113c602daad95ad5dea12839e"
Jul 08 06:49:09 minikube kubelet[1428]: I0708 06:49:09.028459    1428 pod_container_deletor.go:53] "DeleteContainer returned error" containerID={"Type":"docker","ID":"b7615fc7a1b9495c88508f12983858fa9ae9d16113c602daad95ad5dea12839e"} err="failed to get container status \"b7615fc7a1b9495c88508f12983858fa9ae9d16113c602daad95ad5dea12839e\": rpc error: code = Unknown desc = Error response from daemon: No such container: b7615fc7a1b9495c88508f12983858fa9ae9d16113c602daad95ad5dea12839e"
Jul 08 06:49:09 minikube kubelet[1428]: I0708 06:49:09.109884    1428 reconciler_common.go:161] "operationExecutor.UnmountVolume started for volume \"lib-modules\" (UniqueName: \"kubernetes.io/host-path/84bf6f89-6056-433e-8d73-a7b1bc0179d7-lib-modules\") pod \"84bf6f89-6056-433e-8d73-a7b1bc0179d7\" (UID: \"84bf6f89-6056-433e-8d73-a7b1bc0179d7\") "
Jul 08 06:49:09 minikube kubelet[1428]: I0708 06:49:09.110003    1428 reconciler_common.go:161] "operationExecutor.UnmountVolume started for volume \"kube-proxy\" (UniqueName: \"kubernetes.io/configmap/84bf6f89-6056-433e-8d73-a7b1bc0179d7-kube-proxy\") pod \"84bf6f89-6056-433e-8d73-a7b1bc0179d7\" (UID: \"84bf6f89-6056-433e-8d73-a7b1bc0179d7\") "
Jul 08 06:49:09 minikube kubelet[1428]: I0708 06:49:09.110031    1428 reconciler_common.go:161] "operationExecutor.UnmountVolume started for volume \"kube-api-access-6ds25\" (UniqueName: \"kubernetes.io/projected/84bf6f89-6056-433e-8d73-a7b1bc0179d7-kube-api-access-6ds25\") pod \"84bf6f89-6056-433e-8d73-a7b1bc0179d7\" (UID: \"84bf6f89-6056-433e-8d73-a7b1bc0179d7\") "
Jul 08 06:49:09 minikube kubelet[1428]: I0708 06:49:09.110043    1428 reconciler_common.go:161] "operationExecutor.UnmountVolume started for volume \"xtables-lock\" (UniqueName: \"kubernetes.io/host-path/84bf6f89-6056-433e-8d73-a7b1bc0179d7-xtables-lock\") pod \"84bf6f89-6056-433e-8d73-a7b1bc0179d7\" (UID: \"84bf6f89-6056-433e-8d73-a7b1bc0179d7\") "
Jul 08 06:49:09 minikube kubelet[1428]: I0708 06:49:09.110089    1428 reconciler_common.go:247] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-proxy\" (UniqueName: \"kubernetes.io/configmap/ae14030d-5a45-408b-ada4-52b270121030-kube-proxy\") pod \"kube-proxy-r82ck\" (UID: \"ae14030d-5a45-408b-ada4-52b270121030\") " pod="kube-system/kube-proxy-r82ck"
Jul 08 06:49:09 minikube kubelet[1428]: I0708 06:49:09.110105    1428 reconciler_common.go:247] "operationExecutor.VerifyControllerAttachedVolume started for volume \"xtables-lock\" (UniqueName: \"kubernetes.io/host-path/ae14030d-5a45-408b-ada4-52b270121030-xtables-lock\") pod \"kube-proxy-r82ck\" (UID: \"ae14030d-5a45-408b-ada4-52b270121030\") " pod="kube-system/kube-proxy-r82ck"
Jul 08 06:49:09 minikube kubelet[1428]: I0708 06:49:09.110116    1428 reconciler_common.go:247] "operationExecutor.VerifyControllerAttachedVolume started for volume \"lib-modules\" (UniqueName: \"kubernetes.io/host-path/ae14030d-5a45-408b-ada4-52b270121030-lib-modules\") pod \"kube-proxy-r82ck\" (UID: \"ae14030d-5a45-408b-ada4-52b270121030\") " pod="kube-system/kube-proxy-r82ck"
Jul 08 06:49:09 minikube kubelet[1428]: I0708 06:49:09.110129    1428 reconciler_common.go:247] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-qm76k\" (UniqueName: \"kubernetes.io/projected/ae14030d-5a45-408b-ada4-52b270121030-kube-api-access-qm76k\") pod \"kube-proxy-r82ck\" (UID: \"ae14030d-5a45-408b-ada4-52b270121030\") " pod="kube-system/kube-proxy-r82ck"
Jul 08 06:49:09 minikube kubelet[1428]: I0708 06:49:09.110220    1428 operation_generator.go:887] UnmountVolume.TearDown succeeded for volume "kubernetes.io/host-path/84bf6f89-6056-433e-8d73-a7b1bc0179d7-xtables-lock" (OuterVolumeSpecName: "xtables-lock") pod "84bf6f89-6056-433e-8d73-a7b1bc0179d7" (UID: "84bf6f89-6056-433e-8d73-a7b1bc0179d7"). InnerVolumeSpecName "xtables-lock". PluginName "kubernetes.io/host-path", VolumeGidValue ""
Jul 08 06:49:09 minikube kubelet[1428]: I0708 06:49:09.110224    1428 operation_generator.go:887] UnmountVolume.TearDown succeeded for volume "kubernetes.io/host-path/84bf6f89-6056-433e-8d73-a7b1bc0179d7-lib-modules" (OuterVolumeSpecName: "lib-modules") pod "84bf6f89-6056-433e-8d73-a7b1bc0179d7" (UID: "84bf6f89-6056-433e-8d73-a7b1bc0179d7"). InnerVolumeSpecName "lib-modules". PluginName "kubernetes.io/host-path", VolumeGidValue ""
Jul 08 06:49:09 minikube kubelet[1428]: I0708 06:49:09.110832    1428 operation_generator.go:887] UnmountVolume.TearDown succeeded for volume "kubernetes.io/configmap/84bf6f89-6056-433e-8d73-a7b1bc0179d7-kube-proxy" (OuterVolumeSpecName: "kube-proxy") pod "84bf6f89-6056-433e-8d73-a7b1bc0179d7" (UID: "84bf6f89-6056-433e-8d73-a7b1bc0179d7"). InnerVolumeSpecName "kube-proxy". PluginName "kubernetes.io/configmap", VolumeGidValue ""
Jul 08 06:49:09 minikube kubelet[1428]: I0708 06:49:09.111889    1428 operation_generator.go:887] UnmountVolume.TearDown succeeded for volume "kubernetes.io/projected/84bf6f89-6056-433e-8d73-a7b1bc0179d7-kube-api-access-6ds25" (OuterVolumeSpecName: "kube-api-access-6ds25") pod "84bf6f89-6056-433e-8d73-a7b1bc0179d7" (UID: "84bf6f89-6056-433e-8d73-a7b1bc0179d7"). InnerVolumeSpecName "kube-api-access-6ds25". PluginName "kubernetes.io/projected", VolumeGidValue ""
Jul 08 06:49:09 minikube kubelet[1428]: I0708 06:49:09.211108    1428 reconciler_common.go:289] "Volume detached for volume \"kube-api-access-6ds25\" (UniqueName: \"kubernetes.io/projected/84bf6f89-6056-433e-8d73-a7b1bc0179d7-kube-api-access-6ds25\") on node \"minikube\" DevicePath \"\""
Jul 08 06:49:09 minikube kubelet[1428]: I0708 06:49:09.211129    1428 reconciler_common.go:289] "Volume detached for volume \"xtables-lock\" (UniqueName: \"kubernetes.io/host-path/84bf6f89-6056-433e-8d73-a7b1bc0179d7-xtables-lock\") on node \"minikube\" DevicePath \"\""
Jul 08 06:49:09 minikube kubelet[1428]: I0708 06:49:09.211139    1428 reconciler_common.go:289] "Volume detached for volume \"lib-modules\" (UniqueName: \"kubernetes.io/host-path/84bf6f89-6056-433e-8d73-a7b1bc0179d7-lib-modules\") on node \"minikube\" DevicePath \"\""
Jul 08 06:49:09 minikube kubelet[1428]: I0708 06:49:09.211149    1428 reconciler_common.go:289] "Volume detached for volume \"kube-proxy\" (UniqueName: \"kubernetes.io/configmap/84bf6f89-6056-433e-8d73-a7b1bc0179d7-kube-proxy\") on node \"minikube\" DevicePath \"\""
Jul 08 06:49:09 minikube kubelet[1428]: I0708 06:49:09.963089    1428 kubelet_volumes.go:163] "Cleaned up orphaned pod volumes dir" podUID="84bf6f89-6056-433e-8d73-a7b1bc0179d7" path="/var/lib/kubelet/pods/84bf6f89-6056-433e-8d73-a7b1bc0179d7/volumes"
Jul 08 06:49:10 minikube kubelet[1428]: I0708 06:49:10.003638    1428 pod_startup_latency_tracker.go:104] "Observed pod startup duration" pod="kube-system/kube-proxy-r82ck" podStartSLOduration=2.003625202 podStartE2EDuration="2.003625202s" podCreationTimestamp="2024-07-08 06:49:08 +0000 UTC" firstStartedPulling="0001-01-01 00:00:00 +0000 UTC" lastFinishedPulling="0001-01-01 00:00:00 +0000 UTC" observedRunningTime="2024-07-08 06:49:10.003598768 +0000 UTC m=+30788.097972310" watchObservedRunningTime="2024-07-08 06:49:10.003625202 +0000 UTC m=+30788.097998741"
Jul 08 06:50:29 minikube kubelet[1428]: E0708 06:50:29.659492    1428 upgradeaware.go:427] Error proxying data from client to backend: readfrom tcp [::1]:54618->[::1]:43133: write tcp [::1]:54618->[::1]:43133: write: broken pipe


==> storage-provisioner [5c627330e574] <==
I0705 07:05:57.636736       1 storage_provisioner.go:116] Initializing the minikube storage provisioner...
F0705 07:06:27.638324       1 main.go:39] error getting server version: Get "https://10.96.0.1:443/version?timeout=32s": dial tcp 10.96.0.1:443: i/o timeout


==> storage-provisioner [7946fadd3dfe] <==
I0705 07:06:43.886002       1 storage_provisioner.go:116] Initializing the minikube storage provisioner...
I0705 07:06:43.890825       1 storage_provisioner.go:141] Storage provisioner initialized, now starting service!
I0705 07:06:43.890855       1 leaderelection.go:243] attempting to acquire leader lease kube-system/k8s.io-minikube-hostpath...
I0705 07:07:01.279203       1 leaderelection.go:253] successfully acquired lease kube-system/k8s.io-minikube-hostpath
I0705 07:07:01.279305       1 event.go:282] Event(v1.ObjectReference{Kind:"Endpoints", Namespace:"kube-system", Name:"k8s.io-minikube-hostpath", UID:"c1841c4c-a535-4a36-a9e3-d5b4b223458d", APIVersion:"v1", ResourceVersion:"17395", FieldPath:""}): type: 'Normal' reason: 'LeaderElection' minikube_f8a82908-161e-416c-874f-0c71667662d2 became leader
I0705 07:07:01.279330       1 controller.go:835] Starting provisioner controller k8s.io/minikube-hostpath_minikube_f8a82908-161e-416c-874f-0c71667662d2!
I0705 07:07:01.380253       1 controller.go:884] Started provisioner controller k8s.io/minikube-hostpath_minikube_f8a82908-161e-416c-874f-0c71667662d2!
